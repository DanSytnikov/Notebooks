{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match _All_ The Things: Deduplication, Record Linkage, and Entity Resolution in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you work in the data business, you'll eventually find yourself trying to make an approximate match from one data source to another.  This comes in at least three flavors: _searching_, where a user-supplied name is matched against a database (for example, auto-complete); _joining_ or [_record linkage_](http://en.wikipedia.org/wiki/Record_linkage), where you're trying to join datasets like you would with a relational join except based on an approximate criteria; and _deduplication_ or _clustering_, where you're trying to group items from one source based on similarity.  The general problem of trying to determine if two records refer to the same entity is called _entity resolution_.\n",
    "\n",
    "These tasks are sometimes performed with a SQL database or some other information retrieval system (eg, Solr). But there are situations where we need something slightly different than the normal database management system, or we just want something simpler that doesn't require a separate deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "1. [The Matching Problem](#The-Matching-Problem)\n",
    "1. Exploring Your Data\n",
    "2. [String Similarity and Distance](#String-Similarity-and-Distance)\n",
    "  1. [Basic Python](#Basic-Python)\n",
    "  1. Difflib\n",
    "  1. Levenshtein\n",
    "  1. Jaro/Jaro-Winkler\n",
    "  1. Jaccard\n",
    "  2. Metrics\n",
    "  4. Token-Based Matching\n",
    "    1. [N-Grams/Shingling](#N-Grams/Shingling)\n",
    "    1. TF/IDF\n",
    "    1. SoftTf-Idf\n",
    "    2. Gayle-Shapley\n",
    "  4. Matching Non-Text Data\n",
    "4. Matching Sets of Data\n",
    "  1. Probablistic Matching and Fellegi-Sunter\n",
    "    1. The EM Algorithm\n",
    "  1. Blocking\n",
    "    5. Locality-Sensitive Hashing\n",
    "      1. Minhash\n",
    "6. Semantic Matching\n",
    "  1. Human Names\n",
    "6. Nearest-Neighbor\n",
    "  1. kd-Trees\n",
    "6. Pairwise alignment/Needleman-Wunsch\n",
    "6. Entropy, Mutual Information, Kulbach-Liebler\n",
    "6. Radix Tree/Patricia Trie\n",
    "6. Entity Recognition/CRF\n",
    "6. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Matching Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two sets of data items, A and B, each with no duplicates then \n",
    "\n",
    "$$\n",
    "M=\\{ (a,b)| a \\in A, b \\in B, a = b \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U=\\{ (a,b)| a \\in A, b \\in B, a \\ne b \\}\n",
    "$$\n",
    "\n",
    "By definition the union of the two sets M and U is the set of all pairs A and B, or:\n",
    "\n",
    "$$\n",
    "A \\times B = M \\cup U\n",
    "$$\n",
    "\n",
    "Some notes:\n",
    "- Formally the \"=\" indicates a true match.  In most real data sources, some non-matches will be spuriously classified as in the M set and some true matches will be put in the U set.\n",
    "- The vast majority of pairs in A x B will be in __U__.  If A and B both have 1000 members, A x B will have 1 million pairs, but __M__ can have at most 1000 elements.\n",
    "- Ideally if (a,b) and (b,c) are in __M__ then (a,c) should be also.  However, there are some methods that do not result in this _transitive closure_ and so extra logic might be need to find transitive associations.\n",
    "- In the deduplication case there is only one dataset (A) so you consider all of the pairs (a1,a2) in A x A except where a1 and a2 are the same item.  In this case the __M__ set contains the duplicates.  There may be numerous pairs that all refer to the same entity, so again extra work here is require to cluster the duplicates into groups and select the best attributes to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in matching is to identify fields that can be used to link sources.  These are usually elements of the identity of the item, such as name, address or SSN.  However, they can be other properties that change with time such as age, assets in an account, or number of children.  Beyond merely locating such identifiers we also need to see if they are used consistently.  If a field is often left blank it might not be useful for linking source, or if it was originally entered by hand it might have mistakes in value or format.  This type of analysis is an aspect of what's sometimes called _data profiling_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Examine a single source to find identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example, we'll use a file from the Department of Labor's website.  This is data from Schedule C of Form 5500, a form that must be filed each year by companies that sponsor defined contribution (ie 401k) plans for their employees.  Schedule C has the name of service providers that receive fees from the plan.  Here i'll load the file into a pandas Dataframe, and use various pandas tools to examine the data.  Our objective is to find identifiers for those service providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29147 entries, 0 to 29146\n",
      "Data columns (total 15 columns):\n",
      "ACK_ID                            29147 non-null object\n",
      "ROW_ORDER                         29147 non-null int64\n",
      "PROVIDER_ELIGIBLE_NAME            29135 non-null object\n",
      "PROVIDER_ELIGIBLE_EIN             18256 non-null float64\n",
      "PROVIDER_ELIGIBLE_US_ADDRESS1     12531 non-null object\n",
      "PROVIDER_ELIGIBLE_US_ADDRESS2     2550 non-null object\n",
      "PROVIDER_ELIGIBLE_US_CITY         12531 non-null object\n",
      "PROVIDER_ELIGIBLE_US_STATE        12531 non-null object\n",
      "PROVIDER_ELIGIBLE_US_ZIP          12531 non-null float64\n",
      "PROV_ELIGIBLE_FOREIGN_ADDRESS1    54 non-null object\n",
      "PROV_ELIGIBLE_FOREIGN_ADDRESS2    8 non-null object\n",
      "PROV_ELIGIBLE_FOREIGN_CITY        54 non-null object\n",
      "PROV_ELIGIBLE_FOREIGN_PROV_ST     25 non-null object\n",
      "PROV_ELIGIBLE_FOREIGN_CNTRY       54 non-null object\n",
      "PROV_ELIGIBLE_FOREIGN_POST_CD     37 non-null object\n",
      "dtypes: float64(2), int64(1), object(12)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('./data/F_SCH_C_PART1_ITEM1_2014_all.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe _info()_ method gives us both the names of the columns and the number of non-null values.  There are about 30k rows, but fewer than half have address information.  The PROVIDER\\_ELIGIBLE\\_EIN columns contains the Employer Identification Number (EIN) that the firm is assigned by the IRS, so that would seem to be a good matching candidate but it's only available for about 60% of the records.  The only identifier present for almost all records though is PROVIDER\\_ELIGIBLE\\_NAME.  Let't look at name individually using the pandas Series method _value_counts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIDELITY INVESTMENTS INSTITUTIONAL     2106\n",
       "PRINCIPAL LIFE INSURANCE COMPANY       1107\n",
       "AMERICAN FUNDS                          605\n",
       "ALLIANZ GLOBAL INVESTORS DISTRIBUTO     527\n",
       "T. ROWE PRICE                           485\n",
       "PIMCO                                   420\n",
       "THE VANGUARD GROUP, INC.                415\n",
       "VANGUARD                                400\n",
       "OPPENHEIMER FUNDS INC.                  368\n",
       "GOLDMAN SACHS                           368\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.PROVIDER_ELIGIBLE_NAME.value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us a couple of things.  First, some companies are referenced many times.  In this case, we probably knew that but in general this might reveal duplicates.  Second, this tells us that the same name is often used to reference this entity, which is usually good for cases of matching and de-duplication.\n",
    "\n",
    "It's usually not a good idea to assume that the name is an unambiguous identifier for an entity though, so let's move on to our second-most frequent identifier, the EIN.   The info display also gives us another data point, which is that the PROVIDER\\_ELIGIBLE\\_EIN column has the type _float64_, indicating that all non-null values are numeric.  EINs are sometimes entered with a &dash; between parts of the number, so this suggests that data has already been processed to some extent.  We also expect EINs to be a 9 digit number.  Here's a handy trick to find the distinct numeric patterns in fields that are all or partially numeric (ie, containing dashes or prefixes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EINpat\n",
       "99999999      4417\n",
       "999999999    13839\n",
       "NONE         10891\n",
       "Name: ACK_ID, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t={ord(x): y for (x, y) in zip('0123456789', '9999999999')}\n",
    "df['EINpat'] =  df.PROVIDER_ELIGIBLE_EIN.apply(lambda x: str(int(x)).translate(t) if pd.notnull(x) else 'NONE')\n",
    "df.groupby('EINpat').count()['ACK_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the records that do have an EIN, about a quarter are only 8 digits.  Although it might not be obvious, this is often a clue that you have a numeric field with occasional leading 0s.  To check, let's reload the dataframe, specifying that the EIN column should be a string, and then reapply the pattern generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EINpat\n",
       "999999999    18256\n",
       "NONE         10891\n",
       "Name: ACK_ID, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./data/F_SCH_C_PART1_ITEM1_2014_all.csv', dtype={'PROVIDER_ELIGIBLE_EIN': 'object'})\n",
    "\n",
    "df['EINpat'] =  df.PROVIDER_ELIGIBLE_EIN.apply(lambda x: x.translate(t) if pd.notnull(x) else 'NONE')\n",
    "df.groupby('EINpat').count()['ACK_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we process them as strings, we do see that the non-null values are consistently a sequence of 9 digits.  That's a good sign that this will be a high-quality identifier in the cases where it's present.  Often in manually entered data there are alternative formats, or data entered into the wrong fields.  Of course, this doesn't prove that the EINs are correct, just that the format is consistent. Let's look at EIN values associated with the names:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROVIDER_ELIGIBLE_NAME              PROVIDER_ELIGIBLE_EIN\n",
       "FIDELITY INVESTMENTS INSTITUTIONAL  042647786                2101\n",
       "PRINCIPAL LIFE INSURANCE COMPANY    420127290                1106\n",
       "AMERICAN FUNDS                      951411037                 374\n",
       "OPPENHEIMER FUNDS INC.              132527171                 368\n",
       "INVESCO DISTRIBUTORS, INC.          741894784                 344\n",
       "TIAA                                131624203                 336\n",
       "BLACKROCK INVESTMENTS, INC.         232784752                 300\n",
       "THE VANGUARD GROUP, INC.            231945930                 258\n",
       "PRUDENTIAL INVESTMENTS LLC          223468527                 236\n",
       "VANGUARD                            231945930                 231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['PROVIDER_ELIGIBLE_NAME', 'PROVIDER_ELIGIBLE_EIN']).size().sort(ascending=False, inplace=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that the EIN seems to be fairly consistent for the cases where it is used (eg 2101/2106 cases of FIDELITY INVESTMENTS INSTITUTIONAL use the EIN 042647786).  The bad news is that some of the common names aren't associated with EINs at all.  Let's check if the Fidelity case uses other EINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROVIDER_ELIGIBLE_EIN\n",
       "042644786       1\n",
       "042647786    2101\n",
       "Name: ACK_ID, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[pd.notnull(df.PROVIDER_ELIGIBLE_NAME) & df.PROVIDER_ELIGIBLE_NAME.str.contains('FIDELITY INVESTMENTS INSTITUTIONAL')]\\\n",
    "                    .groupby(['PROVIDER_ELIGIBLE_EIN'])\\\n",
    "                    .count()['ACK_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 5 cases that don't have the regular EIN, four are blank and the other has an incorrect digit.  Now let's do the converse and see if our EIN is used for other names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIDELITY INVESTMENTS INSTITUTIONAL     2101\n",
       "FID INV INST OPS CO                      25\n",
       "FIDELITY INVESTMENTS                     13\n",
       "FIDELITY INVESTMENTS INST OPERATION       6\n",
       "FIDELITY INVESTMENTS INST OPER CO         5\n",
       "FIDELITY INVESTMENT INSTITUTIONAL         4\n",
       "FIDELITY INV. INST. OPS. CO. INC.         3\n",
       "FID INV INSTL OPS CO                      3\n",
       "FID.INV.INST.OPS.CO.                      3\n",
       "FIDELITY INVESTMENTS INST. OPS. CO.       3\n",
       "FIDELITY INV. INST. OPS. CO., INC.        3\n",
       "FIDELITY INVST INSTNL OPRTN CO, INC       3\n",
       "FID GROWTH COMPANY                        3\n",
       "FIDELITY INVESTMENTS INST. OP. CO.        2\n",
       "FIDELITY INVESTMENT INST COMP             2\n",
       "TMPL GLOBAL BOND ADV                      2\n",
       "FIDELITY INVESTMENTS INST. OPER. CO       2\n",
       "FIDELITY INV INST OPERS CO INC            2\n",
       "NB SOCIALLY RESP I                        2\n",
       "TRP EQUITY INCOME                         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.PROVIDER_ELIGIBLE_EIN=='042647786'].PROVIDER_ELIGIBLE_NAME.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this EIN is also attached to several variations of the __Fidelity Investments Institutional__ name, and some names that don't seem to have much to do with Fidelity (these are probably the names of mutual funds in plans where Fidelity is the record-keeper).\n",
    "\n",
    "Although qualitative, this type of exploratory analysis is a good place to start.  There are two types of errors that we can make in data matching.  The first is that we treat two items as a match when they are in fact a mis-match.  The other is that we treat two items as a mismatch when they are a true match.  Which of these two types of errors matters more will be application dependent, but as a general rule of data quality we hope to minimize both.  If in the above case we decided to require both name and EIN to make a match, we'd miss a lot matches.  On the other hand if we use only name or only EIN, we'd make some incorrect matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Two Sources with Potential Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i'm using the the two datasets from [this](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/) article on the enigma.io blog ([On Wages and Hygiene: Surfacing Bad Management in Public Data](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/).  In the article, they found matches using business names and geo-coded addresses, but for now i'm going to retain the individual address components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53576 16655\n"
     ]
    }
   ],
   "source": [
    "# Florida restaurant inspection data.  Drop everything but unique name and addresses\n",
    "df_fl = pd.read_csv('./data/fl.restaurant-inspections.csv')\n",
    "df_fl_a = df_fl[['dba', 'location_address', 'location_city', 'location_zip_code']].drop_duplicates()\n",
    "\n",
    "# Wage data, drop everything but unique name and addresses, and filter to only Florida.\n",
    "df_oge = pd.read_csv('./data/ogesdw.whd.whisard.fl.csv')\n",
    "df_oge_a_fl = df_oge[['trade_nm', 'legal_name', 'street_addr_1_txt', 'cty_nm', 'st_cd', 'zip_cd']].drop_duplicates()\n",
    "\n",
    "print(len(df_fl_a), len(df_oge_a_fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with two sets of about 54k and 17k rows each.  The size of A x B would be almost 900 million.\n",
    "\n",
    "You might think that we can pick off some low-hanging fruit by matching on identical names.  In most cases of string matching, we'll assume that case does not matter and sometimes certain types of punctuation will be removed or substituted.  For now we'll just lowerize the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_nm</th>\n",
       "      <th>legal_name</th>\n",
       "      <th>street_addr_1_txt</th>\n",
       "      <th>cty_nm</th>\n",
       "      <th>st_cd</th>\n",
       "      <th>zip_cd</th>\n",
       "      <th>nname</th>\n",
       "      <th>dba</th>\n",
       "      <th>location_address</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Embarq</td>\n",
       "      <td>Embarq Management Company</td>\n",
       "      <td>555 Lake Border Drive</td>\n",
       "      <td>Apopka</td>\n",
       "      <td>FL</td>\n",
       "      <td>32703</td>\n",
       "      <td>embarq</td>\n",
       "      <td>EMBARQ</td>\n",
       "      <td>425 3 ST</td>\n",
       "      <td>LEESBURG</td>\n",
       "      <td>34748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Embarq</td>\n",
       "      <td>Embarq Management Company</td>\n",
       "      <td>555 Lake Border Drive</td>\n",
       "      <td>Apopka</td>\n",
       "      <td>FL</td>\n",
       "      <td>32703</td>\n",
       "      <td>embarq</td>\n",
       "      <td>EMBARQ</td>\n",
       "      <td>425 3RD STREET</td>\n",
       "      <td>LEESBURG</td>\n",
       "      <td>34748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Paris Enterprises, LLC</td>\n",
       "      <td>9260 NW 39th Ave</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>FL</td>\n",
       "      <td>32605</td>\n",
       "      <td>mcdonald's</td>\n",
       "      <td>MCDONALD'S</td>\n",
       "      <td>15096 NW 7 AVE</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>33168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Paris Enterprises, LLC</td>\n",
       "      <td>9260 NW 39th Ave</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>FL</td>\n",
       "      <td>32605</td>\n",
       "      <td>mcdonald's</td>\n",
       "      <td>MCDONALD'S</td>\n",
       "      <td>5700 NW 79 AVE</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>331663534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Paris Enterprises, LLC</td>\n",
       "      <td>9260 NW 39th Ave</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>FL</td>\n",
       "      <td>32605</td>\n",
       "      <td>mcdonald's</td>\n",
       "      <td>MCDONALD'S</td>\n",
       "      <td>2300 SW 87 AVE</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>331652011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trade_nm                 legal_name      street_addr_1_txt       cty_nm  \\\n",
       "0      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
       "1      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
       "2  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
       "3  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
       "4  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
       "\n",
       "  st_cd  zip_cd       nname         dba location_address location_city  \\\n",
       "0    FL   32703      embarq      EMBARQ         425 3 ST      LEESBURG   \n",
       "1    FL   32703      embarq      EMBARQ   425 3RD STREET      LEESBURG   \n",
       "2    FL   32605  mcdonald's  MCDONALD'S   15096 NW 7 AVE         MIAMI   \n",
       "3    FL   32605  mcdonald's  MCDONALD'S   5700 NW 79 AVE         MIAMI   \n",
       "4    FL   32605  mcdonald's  MCDONALD'S   2300 SW 87 AVE         MIAMI   \n",
       "\n",
       "  location_zip_code  \n",
       "0             34748  \n",
       "1             34748  \n",
       "2             33168  \n",
       "3         331663534  \n",
       "4         331652011  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl_a['nname'] = df_fl_a.dba.apply(lambda x:x.lower().translate({None:\"'.,\"}) if pd.notnull(x) else '')\n",
    "df_oge_a_fl['nname'] = df_oge_a_fl.trade_nm.apply(lambda x:x.lower().translate({None:\"'.,\"}) if pd.notnull(x) else '')\n",
    "dfm = df_oge_a_fl.merge(df_fl_a, on='nname')\n",
    "dfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see though that there are a lot of duplicate names.  For example, every McDonald's franchise has the same name so the cross product has a lot of mismatches.  So how about if we add some other attribute to the merge.  With a bit of profiling, we can check to see if zip code is a good candidate field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pzip\n",
      "99999         47778\n",
      "99999-9999     1611\n",
      "999999999      4179\n",
      "Name: location_zip_code, dtype: int64\n",
      "pzip\n",
      "99999.9    16655\n",
      "Name: zip_cd, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_fl_a['pzip'] =  df_fl_a.location_zip_code.apply(lambda x: str(x).translate(t) if pd.notnull(x) else x)\n",
    "print(df_fl_a.groupby('pzip').count()['location_zip_code'])\n",
    "df_oge_a_fl['pzip'] =  df_oge_a_fl.zip_cd.apply(lambda x: str(x).translate(t) if pd.notnull(x) else x)\n",
    "print(df_oge_a_fl.groupby('pzip').count()['zip_cd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every zip in the Florida restaurant data has at least 5 digits.  The wage data does as well, although it appears that pandas interpreted this as a numeric field since it only contains digits.  Again we can normalized the field in each dataframe so that it's more likely to match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nname</th>\n",
       "      <th>nzip</th>\n",
       "      <th>location_address</th>\n",
       "      <th>street_addr_1_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jimmy's place</td>\n",
       "      <td>33161</td>\n",
       "      <td>510 NE 125 ST</td>\n",
       "      <td>510 NE 125 St.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pascal's on ponce</td>\n",
       "      <td>33134</td>\n",
       "      <td>2611 PONCE DE LEON BLVD</td>\n",
       "      <td>2611 Ponce De Leon Blvd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>palomilla grill</td>\n",
       "      <td>33144</td>\n",
       "      <td>6890 W FLAGLER ST</td>\n",
       "      <td>6890  W. Flagler Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meson ria de vigo</td>\n",
       "      <td>33145</td>\n",
       "      <td>1363 - 65 CORAL WAY</td>\n",
       "      <td>1363 SW 22nd St.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sf catering n events</td>\n",
       "      <td>33016</td>\n",
       "      <td>9605 NW 79 AVE BAY 17 &amp; 18</td>\n",
       "      <td>9605 NW 79 Avenue 17/18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nname   nzip            location_address  \\\n",
       "0         jimmy's place  33161               510 NE 125 ST   \n",
       "1     pascal's on ponce  33134     2611 PONCE DE LEON BLVD   \n",
       "2       palomilla grill  33144           6890 W FLAGLER ST   \n",
       "3     meson ria de vigo  33145         1363 - 65 CORAL WAY   \n",
       "4  sf catering n events  33016  9605 NW 79 AVE BAY 17 & 18   \n",
       "\n",
       "          street_addr_1_txt  \n",
       "0            510 NE 125 St.  \n",
       "1  2611 Ponce De Leon Blvd.  \n",
       "2   6890  W. Flagler Street  \n",
       "3          1363 SW 22nd St.  \n",
       "4   9605 NW 79 Avenue 17/18  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl_a['nzip'] = df_fl_a.location_zip_code.apply(lambda x:x[0:5] if pd.notnull(x) else x)\n",
    "df_oge_a_fl['nzip'] = df_oge_a_fl.zip_cd.apply(lambda x: str(int(x)))\n",
    "dfmz = df_fl_a.merge(df_oge_a_fl, on=['nname', 'nzip'])\n",
    "dfmz[['nname','nzip', 'location_address', 'street_addr_1_txt']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a bit better at first, but it's still easy to find incorrect and duplicate matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nname</th>\n",
       "      <th>nzip</th>\n",
       "      <th>location_address</th>\n",
       "      <th>street_addr_1_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>il bolognese</td>\n",
       "      <td>33139</td>\n",
       "      <td>626 - 630 LINCOLN RD</td>\n",
       "      <td>1052 Ocean Drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>il bolognese</td>\n",
       "      <td>33139</td>\n",
       "      <td>626 - 630 LINCOLN RD</td>\n",
       "      <td>626 Lincoln Rd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>il bolognese</td>\n",
       "      <td>33139</td>\n",
       "      <td>1400 OCEAN DRIVE</td>\n",
       "      <td>1052 Ocean Drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>il bolognese</td>\n",
       "      <td>33139</td>\n",
       "      <td>1400 OCEAN DRIVE</td>\n",
       "      <td>626 Lincoln Rd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wajiro's restaurant</td>\n",
       "      <td>33184</td>\n",
       "      <td>12670 SW 8 ST</td>\n",
       "      <td>12670 SW 8th Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wajiro's restaurant</td>\n",
       "      <td>33184</td>\n",
       "      <td>12670 SW 8 ST</td>\n",
       "      <td>12670 S.W. 8 Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hosteria romana</td>\n",
       "      <td>33139</td>\n",
       "      <td>429 ESPANOLA WAY</td>\n",
       "      <td>429 Espaola Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hosteria romana</td>\n",
       "      <td>33139</td>\n",
       "      <td>429 ESPANOLA WAY</td>\n",
       "      <td>435 Espanola Way #B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>johnny rockets</td>\n",
       "      <td>33172</td>\n",
       "      <td>11401 NW 12 ST #E-508</td>\n",
       "      <td>11401 N W 12th Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>johnny rockets</td>\n",
       "      <td>33172</td>\n",
       "      <td>1455 NW 107 AVE #454 A</td>\n",
       "      <td>11401 N W 12th Street</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nname   nzip        location_address      street_addr_1_txt\n",
       "17         il bolognese  33139    626 - 630 LINCOLN RD       1052 Ocean Drive\n",
       "18         il bolognese  33139    626 - 630 LINCOLN RD        626 Lincoln Rd.\n",
       "19         il bolognese  33139        1400 OCEAN DRIVE       1052 Ocean Drive\n",
       "20         il bolognese  33139        1400 OCEAN DRIVE        626 Lincoln Rd.\n",
       "23  wajiro's restaurant  33184           12670 SW 8 ST    12670 SW 8th Street\n",
       "24  wajiro's restaurant  33184           12670 SW 8 ST    12670 S.W. 8 Street\n",
       "29      hosteria romana  33139        429 ESPANOLA WAY        429 Espaola Way\n",
       "30      hosteria romana  33139        429 ESPANOLA WAY    435 Espanola Way #B\n",
       "38       johnny rockets  33172   11401 NW 12 ST #E-508  11401 N W 12th Street\n",
       "39       johnny rockets  33172  1455 NW 107 AVE #454 A  11401 N W 12th Street"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmz.groupby(['nname','nzip']).filter(lambda x: len(x['cty_nm']) > 1)[['nname',\n",
    "                                                                      'nzip', \n",
    "                                                                      'location_address', \n",
    "                                                                      'street_addr_1_txt']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential source of duplicate matches is that the original data sets have duplicates.  Again, that's pretty easy to find with some aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dba</th>\n",
       "      <th>location_address</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_zip_code</th>\n",
       "      <th>nname</th>\n",
       "      <th>pzip</th>\n",
       "      <th>nzip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87993</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>3111 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BEACH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87994</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2225 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87996</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2411 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87997</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>3125 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87999</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>400 N ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88045</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2209 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88061</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2125 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BCH</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88291</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2209 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BEACH SHORES</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88474</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>1615 S ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88493</th>\n",
       "      <td>ATLANTIC VENDORS INC</td>\n",
       "      <td>2500 N ATLANTIC AVE</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>32118</td>\n",
       "      <td>atlantic vendors inc</td>\n",
       "      <td>99999</td>\n",
       "      <td>32118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        dba     location_address         location_city  \\\n",
       "87993  ATLANTIC VENDORS INC  3111 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
       "87994  ATLANTIC VENDORS INC  2225 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
       "87996  ATLANTIC VENDORS INC  2411 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
       "87997  ATLANTIC VENDORS INC  3125 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
       "87999  ATLANTIC VENDORS INC   400 N ATLANTIC AVE           DAYTONA BCH   \n",
       "88045  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
       "88061  ATLANTIC VENDORS INC  2125 S ATLANTIC AVE           DAYTONA BCH   \n",
       "88291  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
       "88474  ATLANTIC VENDORS INC  1615 S ATLANTIC AVE         DAYTONA BEACH   \n",
       "88493  ATLANTIC VENDORS INC  2500 N ATLANTIC AVE         DAYTONA BEACH   \n",
       "\n",
       "      location_zip_code                 nname   pzip   nzip  \n",
       "87993             32118  atlantic vendors inc  99999  32118  \n",
       "87994             32118  atlantic vendors inc  99999  32118  \n",
       "87996             32118  atlantic vendors inc  99999  32118  \n",
       "87997             32118  atlantic vendors inc  99999  32118  \n",
       "87999             32118  atlantic vendors inc  99999  32118  \n",
       "88045             32118  atlantic vendors inc  99999  32118  \n",
       "88061             32118  atlantic vendors inc  99999  32118  \n",
       "88291             32118  atlantic vendors inc  99999  32118  \n",
       "88474             32118  atlantic vendors inc  99999  32118  \n",
       "88493             32118  atlantic vendors inc  99999  32118  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl_a.groupby(['nname','nzip']).filter(lambda x: len(x['dba']) > 2).sort('dba').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting case because there's one pair that's a clear duplicate (at 2209 S Atlantic), and another triple that all look suspiciously close (2225 S, 2125 S, and 3125 S).\n",
    "\n",
    "There are several things we can learn from the preceding analysis of the two data sets that are common to all data matching problems:\n",
    "\n",
    "- We usually need to preprocess fields (lowerize, normalize, convert types, geocode, split, combine, etc.)\n",
    "- The same data will often be presented in various ways, and to varying degrees of quality in different sources, so it helps to profile.\n",
    "- Might need to dedupe individual sources before trying to link multiple sources.\n",
    "- Different data sources will have variations of name/address, necessitating approximate matching.\n",
    "- Need to assume there will be cases where attributes match but the entities are not a true match.\n",
    "- Need to assume that identifiers (street numbers, zip codes, etc.) will be *wrong* in some percentage of cases (ie, there are true matches where attributes are not equal).\n",
    "\n",
    "Probably the main difficulty in matching problems is determining the degree of similarity between individual attributes, especially string attributes, which will be the subject of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Similarity and Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similarity between strings is a subset of the field of natural language processing, so not surprisingly there are many string metrics in the well-known Python [Natural Language Toolkit (NLTK)](http://www.nltk.org/_modules/nltk/metrics/distance.html).  The [jellyfish](https://pypi.python.org/pypi/jellyfish) package is another Python library that has numerous string metrics. There are in fact a huge number of different metrics for measuring the similarity of, or distance between, two text strings.  This [Wikipedia article](http://en.wikipedia.org/wiki/String_metric) lists no fewer than 18. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, don't forget that you can do many string matching operations with basic Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "13\n",
      "13\n",
      "['Acme Traffic Signal Company', 'Acme Traffic Signal Co.', 'Acme Rocket Powered Products']\n",
      "['Acme Traffic Signal Company', 'Acme Traffic Signal Co.', 'ACME TRAFFIC SIGNAL COMPANY']\n"
     ]
    }
   ],
   "source": [
    "string1 = u'Acme Traffic Signal Company'\n",
    "string2 = u'Acme Traffic Signal Co.'\n",
    "string3 = u'ACME TRAFFIC SIGNAL COMPANY'\n",
    "string4 = u'Acme Rocket Powered Products'\n",
    "strings = [string1, string2, string3, string4]\n",
    "\n",
    "print(string1 == string2)\n",
    "print(string1[:20] == string2[:20])\n",
    "print(string1.lower() == string3.lower())\n",
    "print(string2.find('Signal'))\n",
    "print(string2.index('Signal'))\n",
    "print([s for s in strings if s.startswith('Acme')])\n",
    "print([s for s in strings if 'traffic' in s.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do \"wildcard\" matches using the Python regular expression module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acme Traffic Signal Company', 'Acme Traffic Signal Co.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "[s for s in strings if re.search('Acme.*Signal', s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another under-appreciated Python module is difflib.  Although its primary use is generating diff-like comparisons of documents, it also has some useful and interesting matching functionality.  The _get_close_matches_ function is a quick way to find near matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Acme Rocket Powered Products']\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "print(difflib.get_close_matches('Acme Rocket-Powered Inc.', strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the difflib SequenceMatcher can generate a score that indicates how well two sequences match (note that the sequences don't necessarily have to be strings).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acme Traffic Signal Company 0.27450980392156865\n",
      "Acme Traffic Signal Co. 0.2978723404255319\n",
      "ACME TRAFFIC SIGNAL COMPANY 0.1568627450980392\n",
      "Acme Rocket Powered Products 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "sm = difflib.SequenceMatcher()\n",
    "sm.set_seq1('Acme Rocket-Powered Inc.')\n",
    "for s in strings:\n",
    "    sm.set_seq2(s)\n",
    "    print(s, sm.ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the best know string similarity metric is the Levenshtein distance, sometimes just called the edit distance, which is simply a count of the 1-character edits you'd have to make to turn one string into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acme Traffic Signal Company Acme Traffic Signal Co.\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "\n",
    "print(string1, string2)\n",
    "print(jellyfish.levenshtein_distance(string1, string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some theorize that Levenshtein was inspired to invent this metric by the fact that everybody mispelled his name.  As you can see above this means that the value of the Levenshtein distance is a non-negative integer.  There are variants of this metric that allow simple transpositions to be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro and Jaro-Winkler Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acme Traffic Signal Company Acme Traffic Signal Co.\n",
      "0.9237788513150832\n",
      "0.9542673107890499\n"
     ]
    }
   ],
   "source": [
    "print(string1, string2)\n",
    "print(jellyfish.jaro_distance(string1, string2))\n",
    "print(jellyfish.jaro_winkler(string1, string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaro and Jaro-Winkler similarity metrics were developed specifically for record-linking applications.  Both are described well on the [Jaro Distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) Wikipedia page.  The Jaro-Winkler version gives more emphasis to strings that have an initial matching prefix, which is routinely the case when matching human or business names.  Empirically these metrics seem to give the best results for basic string-level matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity is simply defined as the length of the intersection of two sets divided by the length of the union of the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    x = set(a)\n",
    "    y = set(b)\n",
    "    return len(x & y) / len(x | y)\n",
    "\n",
    "jaccard_similarity(string1, string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the Jaccard similarity gives it interesting properties that aren't necessarily true of the string similarity measures we seen so far.  First, the _order_ of the characters in the two strings does not matter since they are treated as sets, and second extra copies of the same elements in one of the sets have no effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0\n",
      "1.0 0.611111111111111 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_similarity('abcdef', 'abcdef'), \n",
    "      jaccard_similarity('abcdef', 'cbfaed'), \n",
    "      jaccard_similarity('abcdef', 'cbfaedcbfaed') )\n",
    "print(jellyfish.jaro_distance(u'abcdef', u'abcdef'), \n",
    "      jellyfish.jaro_distance(u'abcdef', u'cbfaed'), \n",
    "      jellyfish.jaro_distance(u'abcdef', u'cbfaedcbfaed') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, Jaccard similarity can be easily applied to non-character data since it operates on sets.  This will turn out to be handy later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_similarity([2,3,4,5], [3,5]))\n",
    "print(jaccard_similarity([\"bag\",\"of\",\"words\"], [\"words\",\"bag\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term _distance_ or _metric_ is sometimes used informally for any value that measures the similarity of strings, but there's a formal definition for a distance metric\n",
    "\n",
    "- d(a, a) = 0\n",
    "- d(a, b) ≥ 0\n",
    "- d(a, b) = d(b, a)\n",
    "- d(a, c) ≤ d(a, b) + d(b, c)  #  The \"triangle inequality\n",
    "\n",
    "The Levenshtein distance does follow these rules.  However, the Jaro distance doesn't, mainly because the Jaro distance as implemented here is actually a similarity metric, that is, two identical strings have the maximum score (here 1.0) rather than 0.0.  However, even if you define a distance metric as 1.0 - Jaro, it's still not a metric.  It's a little difficult to prove that something _is_ a metric, however it's easy to find counter-examples showing that it's not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(abc-bca)=1.0  d(abc-acb) + d(acb-bca)=0.8888888888888891\n",
      "d(abc-cab)=1.0  d(abc-acb) + d(acb-cab)=0.8888888888888891\n",
      "d(abc-bca)=1.0  d(abc-bac) + d(bac-bca)=0.8888888888888891\n",
      "d(abc-cab)=1.0  d(abc-bac) + d(bac-cab)=0.8888888888888891\n",
      "d(abc-bca)=1.0  d(abc-cba) + d(cba-bca)=0.8888888888888891\n",
      "d(abc-cab)=1.0  d(abc-cba) + d(cba-cab)=0.8888888888888891\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def check_triangle(f):\n",
    "    t0 = u'abc'\n",
    "    p = [''.join(p) for p in itertools.permutations(t0)]\n",
    "\n",
    "    for t1 in p:\n",
    "        for t2 in p:\n",
    "            if(f(t0, t2) > (f(t0, t1) + f(t1,t2))):\n",
    "                print(\"d({t0}-{t2})={d02}  d({t0}-{t1}) + d({t1}-{t2})={d01_12}\".format(t0=t0,t1=t1,t2=t2,d02=f(t0, t2), d01_12=f(t0, t1) + f(t1,t2)))\n",
    "\n",
    "check_triangle(lambda x,y : 1 - jellyfish.jaro_winkler(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity can also be turned into a proper distance metric just by subtracting it from 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1578947368421053\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def jaccard_distance(a, b): return 1 - jaccard_similarity(a, b)\n",
    "print(jaccard_distance(string1, string2))\n",
    "print(jaccard_distance('abcdef', 'cbfaed'))\n",
    "check_triangle(jaccard_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of a distance metric are primarily useful in clustering applications, where we need a consistent idea of how far apart are the items we're trying to cluster.  A true metric creates what's called a _metric space_, the most common instance of which is a Euclidean space like we're used to in geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best metric to use depends on the application.  Some string similarity metrics like the edit distance are based on the idea that small differences are introduced into entity names, probably because of human error.  These metrics are good at finding similar strings with transpositions, misspellings, added or deleted characters, or abbreviations.  The Jaro distance and the Jaro-Winkler variation were developed for record linkage applications.  Something like Jaccard similarity might be better where the sequence is less important than the content or topic.\n",
    "\n",
    "Below is a case where the three metrics are applied to four strings that are in fact names for the same company (a mutual fund company taken from SEC data-- the first name is the current name and the other three are former names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>Levenshtein</th>\n",
       "      <th>Jaro</th>\n",
       "      <th>Jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.794935</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816399</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.765007</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.794935</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.845314</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816399</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911616</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>10</td>\n",
       "      <td>0.843852</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.765007</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
       "      <td>14</td>\n",
       "      <td>0.845314</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
       "      <td>10</td>\n",
       "      <td>0.843852</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  n1  \\\n",
       "0                 FRANKLIN LTD DURATION INCOME TRUST   \n",
       "1                 FRANKLIN LTD DURATION INCOME TRUST   \n",
       "2                 FRANKLIN LTD DURATION INCOME TRUST   \n",
       "3                 FRANKLIN LTD DURATION INCOME TRUST   \n",
       "4   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
       "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
       "6   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
       "7   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
       "8       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
       "9       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
       "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
       "11      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
       "12        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
       "13        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
       "14        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
       "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
       "\n",
       "                                                  n2  Levenshtein      Jaro  \\\n",
       "0                 FRANKLIN LTD DURATION INCOME TRUST            0  1.000000   \n",
       "1   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.794935   \n",
       "2       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.816399   \n",
       "3         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.765007   \n",
       "4                 FRANKLIN LTD DURATION INCOME TRUST           14  0.794935   \n",
       "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            0  1.000000   \n",
       "6       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            4  0.911616   \n",
       "7         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.845314   \n",
       "8                 FRANKLIN LTD DURATION INCOME TRUST           10  0.816399   \n",
       "9   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            4  0.911616   \n",
       "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            0  1.000000   \n",
       "11        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            10  0.843852   \n",
       "12                FRANKLIN LTD DURATION INCOME TRUST           14  0.765007   \n",
       "13  FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.845314   \n",
       "14      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.843852   \n",
       "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST             0  1.000000   \n",
       "\n",
       "     Jaccard  \n",
       "0   1.000000  \n",
       "1   0.941176  \n",
       "2   0.941176  \n",
       "3   0.833333  \n",
       "4   0.941176  \n",
       "5   1.000000  \n",
       "6   1.000000  \n",
       "7   0.888889  \n",
       "8   0.941176  \n",
       "9   1.000000  \n",
       "10  1.000000  \n",
       "11  0.888889  \n",
       "12  0.833333  \n",
       "13  0.888889  \n",
       "14  0.888889  \n",
       "15  1.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [u'FRANKLIN LTD DURATION INCOME TRUST',\n",
    "         u'FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST',\n",
    "         u'FRANKLIN TEMPLETON LTD DURATION INCOME TRUST',\n",
    "         u'FRANKLIN TEMPLETON STRATEGIC INCOME TRUST ']\n",
    "\n",
    "similarities = [(n1,n2, jellyfish.levenshtein_distance(n1,n2), jellyfish.jaro_distance(n1,n2), jaccard_similarity(n1,n2))\n",
    "              for n1 in names for n2 in names]\n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(similarities)\n",
    "df.columns = ['n1', 'n2', 'Levenshtein', 'Jaro', 'Jaccard']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this application, the Jaro distance seems like the most selective.  It ranks the similarity in the most intuitive way.  Note that the Jaccard similarity sometimes gives the highest score (1.0) to strings that aren't actually identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the techniques we've looked at already could be applied to tokens (or really any sequence) but things like Levenshtein distance and Jaro distance are usually applied to strings.  Normally by _token_ we'd mean some delimited piece of a larger strings or expression, but here we're going to use a fairly loose definition of tokens here as basically any subsequence of a string.  That might mean words, but it also might various other things as we'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Grams/Shingling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages have structure in the sense that some letters are more likely to follow other letters, and some words are more likely to follow others.  The term n-gram sometimes refers to the overlapping sequences of characters in a string, and sometimes to overlapping sequences of tokens.  These are also called _shingles_ (because they overlap, get it?).  As an example, below are the character bigrams for two of the names above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FR', 'RA', 'AN', 'NK', 'KL', 'LI', 'IN', 'N ', ' L', 'LT', 'TD', 'D ', ' D', 'DU', 'UR', 'RA', 'AT', 'TI', 'IO', 'ON', 'N ', ' I', 'IN', 'NC', 'CO', 'OM', 'ME', 'E ', ' T', 'TR', 'RU', 'US', 'ST']\n",
      "['FR', 'RA', 'AN', 'NK', 'KL', 'LI', 'IN', 'N ', ' T', 'TE', 'EM', 'MP', 'PL', 'LE', 'ET', 'TO', 'ON', 'N ', ' L', 'LI', 'IM', 'MI', 'IT', 'TE', 'ED', 'D ', ' D', 'DU', 'UR', 'RA', 'AT', 'TI', 'IO', 'ON', 'N ', ' I', 'IN', 'NC', 'CO', 'OM', 'ME', 'E ', ' T', 'TR', 'RU', 'US', 'ST']\n",
      "0.9411764705882353 0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "# N-gram function courtesy of Peter Norvig\n",
    "def ngrams(seq, n):\n",
    "    \"List all the (overlapping) ngrams in a sequence.\"\n",
    "    return [seq[i:i+n] for i in range(1+len(seq)-n)]\n",
    "\n",
    "print(ngrams(names[0], 2))\n",
    "print(ngrams(names[1], 2))\n",
    "print(jaccard_similarity(names[0], names[1]), jaccard_similarity(ngrams(names[0], 2), ngrams(names[1], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Jaccard similarity of the original strings is much higher than the Jaccard similarity of the bigram sets.  We can calculate a Jaccard score for the tokens in the strings as well (after we tweak the lists of bigrams slightly since our ngrams function returns unhashable list elements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FRANKLIN', 'LTD'], ['LTD', 'DURATION'], ['DURATION', 'INCOME'], ['INCOME', 'TRUST']]\n",
      "[['FRANKLIN', 'TEMPLETON'], ['TEMPLETON', 'LIMITED'], ['LIMITED', 'DURATION'], ['DURATION', 'INCOME'], ['INCOME', 'TRUST']]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(ngrams(names[0].split(), 2))\n",
    "print(ngrams(names[1].split(), 2))\n",
    "print(jaccard_similarity((tuple(x) for x in ngrams(names[0].split(), 2)), \n",
    "                         (tuple(x) for x in ngrams(names[2].split(), 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency/Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic similarity metric is _cosine similarity_, which measures how close two vectors are in an n-dimensional space by computing the cosine of the angle between the two vectors.  As you might recall from basic linear algebra, the cosine between to vectors X and Y can be computed as:\n",
    "\n",
    "$$\n",
    "cos(\\theta) = \\frac{ X \\cdot Y }{\\| X |\\ \\| Y \\|}\n",
    "$$\n",
    "\n",
    "That's cool, if you happen to have vectors.  The trick is to make a string of tokens look like a vector.  One way to do this is to analyze a whole set of items to match and calculate properties of each item with respect to the whole set (in information retrieval (IR) a token is a _term_, the item is a _document_ and the set is a _corpus_).  The most common way to do this is to calculate _term frequency_ and _inverse document frequency_, or TdIdf.\n",
    "\n",
    "The term frequency is just the count of a particular token in an item.  The inverse document frequency is a measure of how rare a word is.  Common tokens will occur in most items (documents) so their inverse frequency will be low, and vice versa.  Inverse document frequency is usually defined like this:\n",
    "\n",
    "$$\n",
    "idf(t,D) = log \\frac{|D|}{| d \\in D, t \\in d |}\n",
    "$$\n",
    "\n",
    "The log just magnifies the value of rare terms.\n",
    "\n",
    "In many entity resolution (record linkage, deduplication) cases the documents are small, so term frequencies > 1 are rare, but they do happen.  For example, lets look at the Florida restaurant names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((114117, 'tiki'), 3), ((13056, '03'), 3), ((12943, '01'), 3), ((12973, '05'), 3), ((12957, '07'), 3), ((23315, 'village'), 3), ((12937, '03'), 3), ((12952, '03'), 3), ((25078, 'subs'), 3), ((47065, 'new'), 2)]\n",
      "dba                  VILLAGE GRILLE VILLAGE PUMP VILLAGE SUSHI\n",
      "location_address                                4404 EL MAR DR\n",
      "location_city                             LAUDERDALE-BY-THE-SE\n",
      "location_zip_code                                        33308\n",
      "nname                village grille village pump village sushi\n",
      "pzip                                                     99999\n",
      "nzip                                                     33308\n",
      "Name: 23315, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "doc_terms = [(idx, t)for idx, name in df_fl_a.nname.iteritems() for t in re.split('\\W+', name) ]\n",
    "term_counts = Counter(doc_terms)\n",
    "print(term_counts.most_common(10))\n",
    "print(df_fl_a.ix[23315])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse document frequency will still be fairly interesting in the matching case, since there are many documents even though they are small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'south': 4.893591630007751, 'chinese': 4.019603277824285, 'sea': 5.469613453330283, 'restaurant': 1.6921449200851}\n",
      "95 169 4152\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(itertools.chain(*(re.split('\\W+',n) for n in df_fl_a.nname)))\n",
    "\n",
    "idf = lambda c,t: math.log(len(c) / c[t])\n",
    "n1 = 'south sea chinese restaurant'\n",
    "name_tokens = re.split('\\W+', n1)\n",
    "n1_idf = {t: idf(counts, t) for t in name_tokens}\n",
    "print(n1_idf)\n",
    "print(counts['sea'], counts['south'], counts['restaurant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we can make a TfIdf score for each token, we can make a vector for each name and compare them with cosine similarity.  The vector space that we're dealing with is n-dimensional, where n is the total number of terms in the set of items, so most terms in the vector would be zero (in this case we'd have n=22548).  In practice to calculate similarity you just find the set of tokens that match in each string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2960934870717506\n",
      "0.4151156193183266\n"
     ]
    }
   ],
   "source": [
    "dot = lambda a,b : sum(x*y for x,y in zip(a,b))\n",
    "\n",
    "n2 = 'atlantic chinese restaurant'\n",
    "n3 = 'south beach cafeteria & restaurant'\n",
    "\n",
    "w_12 = [idf(counts, 'chinese'), idf(counts, 'restaurant')]\n",
    "w_11 = [idf(counts, t) for t in name_tokens]\n",
    "w_22 = [idf(counts, t) for t in re.split('\\W+', n2)]\n",
    "\n",
    "w_13 = [idf(counts, 'south'), idf(counts, 'restaurant')]\n",
    "w_33 = [idf(counts, t) for t in re.split('\\W+', n3)]\n",
    "\n",
    "print( dot(w_12, w_12)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_22, w_22))) )\n",
    "print( dot(w_13, w_13)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_33, w_33))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though both strings share two tokens, (n1,n3) scores slightly higher because the token _south_ is more infrequent than _chinese_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftTfIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the Tf-Idf approach is that only identical tokens will contribute to the numerator of the cosine similarity.  That's less of an issue with larger documents where the objective is to find documents that match some query.  Here though we're trying to find items with the greatest degree of similarity.  One method that works well is called _SoftTfIdf_, where instead of identical tokens, the numerator includes every pair of tokens that's similar enough with respect to some string similarity metric, like Jaro-Winkler.\n",
    "\n",
    "This technique is helpful in many applications because there are often frequently encountered patterns in business or product names.  For example in our test data there are is a case where the name 'ole mexican restaurant' in one data source is misspelled as 'ol mexican restaurant' in the other source.  The Jaro-Winkler score for this would indicate similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9243867243867243"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler(u'ole mexican restaurant', u'ol mexican restaurant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the normal Tf-Idf approach would not regard these as similar because the only terms that they share are the relatively common tokens 'mexican' and 'restaurant'.  The SoftTf-Idf approach however uses all tokens with a given level of similarity:\n",
    "\n",
    "$$\n",
    "\\{ sim(w,v) > \\theta | w \\in X, v \\in Y \\} \n",
    "$$\n",
    "\n",
    "where X and Y are two names being compared and &theta; is a threshold value for a second similarity function, such as Jaro-Winkler.  Assuming that the token pair ('ole','ol') meets the threshold for a given _sim_ then SoftTdf-Idf _would_ regard these as similar because it would include the pair in its comparison.  \n",
    "\n",
    "On the other hand, Jaro-Winkle would _also_ regard these pairs as fairly similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365079365079364"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler(u'df mexican restaurant', u'ol mexican restaurant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SoftTf-Idf approach works much better here because it not give as much value to the more common tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gayle-Shapley Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the trickier parts of the SoftTf-Idf algorithm is picking a combination of similarity function and threshold so that you get a good set of tokens to match .  It's possible a token in the first string might be similar to two tokens in the second string.  It's also possible that token _t1_ in the first string has it's best match with token _t2_ in the candidate string, but token _t3_ in the first string _also_ has its best match with _t2_. [NB: SoftTf-Idf is not usually implemented this way-- i got the idea from [Richard Minerich's blog](http://richardminerich.com/2011/09/record-linkage-in-f-token-matching-stable-marriages-and-the-gale-shapley-algorithm/)]\n",
    "\n",
    "Fortunately there's a well-known and understood algorithm for making the best set of matches.  The Gayle-Shapley algorithm, also sometimes called the _stable marriage_ algorithm, takes the preferences of each of two sets and finds the match that best meets all of the preferences.  In this case, the preferences are the similarity between tokens.  Below is an implemenation that's adapted from the algorithm's Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pair_tokens(r1, r2):\n",
    "    ''' Find the optimal pairing of tokens.  This is basically a version of the Gayle-Shapley\n",
    "    algorithm '''\n",
    "    xmatched = {}\n",
    "    unmatched_tokens = list(r1.keys())\n",
    "    while unmatched_tokens:\n",
    "        token = unmatched_tokens.pop(0)\n",
    "        match_token = r1[token].pop(0)\n",
    "        if match_token not in xmatched:\n",
    "            xmatched[match_token] = token\n",
    "        else:\n",
    "            curr_match = xmatched[match_token]\n",
    "            if r2[match_token].index(token) < r2[match_token].index(curr_match):\n",
    "                xmatched[match_token] = token\n",
    "                if r1[curr_match]:  # pragma: no cover\n",
    "                    unmatched_tokens.append(curr_match)\n",
    "            else:\n",
    "                if r1[token]:\n",
    "                    unmatched_tokens.append(token)\n",
    "\n",
    "    return {v: k for k, v in xmatched.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example in the two following strings, the two tokens _bakery_ and _bistro_ in the first string both rank _bistro_ as their best token match, but the second string also ranks bistro as it's best match for bistro, so bakery ends up with its second choice, _california_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bakery': 'california', 'bistro': 'bistro', 'frescos': 'fresco'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = u'frescos bistro and bakery'\n",
    "n2 = u'fresco california bistro'\n",
    "\n",
    "r1 = {u'and': [u'california', u'fresco', u'bistro'],\n",
    " u'bakery': [u'bistro', u'california', u'fresco'],\n",
    " u'bistro': [u'bistro', u'fresco', u'california'],\n",
    " u'frescos': [u'fresco', u'bistro', u'california']}\n",
    "r2 = {u'bistro': [u'bistro', u'bakery', u'frescos', u'and'],\n",
    " u'california': [u'bakery', u'bistro', u'and', u'frescos'],\n",
    " u'fresco': [u'frescos', u'bistro', u'bakery', u'and']}\n",
    "pair_tokens(r1,r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Non-Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numbers (L1, L2 norm, relative differences)\n",
    "- Addresses - geocoding, lat./long.\n",
    "- NAICS codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Sets of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have numerous tools to match approximately on attributes, so we'll return to the problem of matching items in one set of data to another set.  Remember that in principle we have to check every single pair in the set A x B, which for our test data is about 900M pairs.  Although that's not a huge problem by today's standards, it's still significant and we know that most pairs are not matches, so we seek some way to reduce the number of pairs to test.  Usually this is accomplished by some sort of _indexing_, or in the language of entity resolution _blocking_.  We will discuss blocking later, but usually it involves clustering the candidate items in some way so that each item in set A only needs to be checked against a corresponding cluster in set B.\n",
    "\n",
    "For the sake of argument, let's say that we can use our normalized zip code as an index, and we'll make a dataframe that has all of the remaining potential matches.  Once we have the zip-indexed dataframe we'll compare the names of the remaining pairs using the Jaro-Winkler similarity, after removing the cases where the names already match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nname_x</th>\n",
       "      <th>nname_y</th>\n",
       "      <th>nzip</th>\n",
       "      <th>snum_x</th>\n",
       "      <th>location_city</th>\n",
       "      <th>snum_y</th>\n",
       "      <th>cty_nm</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190603</th>\n",
       "      <td>la pequena columbia restaurant</td>\n",
       "      <td>la pequena colombia espress</td>\n",
       "      <td>33604</td>\n",
       "      <td>6312</td>\n",
       "      <td>TAMPA</td>\n",
       "      <td>6204</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>0.900539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288526</th>\n",
       "      <td>fairfield inn &amp; suites by marriott miami airp</td>\n",
       "      <td>fairfield inn &amp; suites miami south</td>\n",
       "      <td>33126</td>\n",
       "      <td>4101</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>4101</td>\n",
       "      <td>Miami</td>\n",
       "      <td>0.900776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248225</th>\n",
       "      <td>ciro's speakeasy and restaurant</td>\n",
       "      <td>ciro's speakeasy and supper club</td>\n",
       "      <td>33606</td>\n",
       "      <td>2109</td>\n",
       "      <td>TAMPA</td>\n",
       "      <td>2109</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>0.901540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711593</th>\n",
       "      <td>hooks fish &amp; chicken (miami gardens)</td>\n",
       "      <td>hook's fish &amp; chicken</td>\n",
       "      <td>33054</td>\n",
       "      <td>15850</td>\n",
       "      <td>OPA-LOCKA</td>\n",
       "      <td>15850</td>\n",
       "      <td>Miami Gardens</td>\n",
       "      <td>0.901587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704319</th>\n",
       "      <td>katherine's  island  house  cafe</td>\n",
       "      <td>katherine's island house inc</td>\n",
       "      <td>34134</td>\n",
       "      <td>3801</td>\n",
       "      <td>BONITA SPRINGS</td>\n",
       "      <td>3801</td>\n",
       "      <td>Bonita Springs</td>\n",
       "      <td>0.902060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503336</th>\n",
       "      <td>mcdonalds store # 15535</td>\n",
       "      <td>mcdonald's snack bar #15535</td>\n",
       "      <td>32223</td>\n",
       "      <td>10991-1</td>\n",
       "      <td>JACKSONVILLE</td>\n",
       "      <td>10991</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>0.902061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300441</th>\n",
       "      <td>hunter marine</td>\n",
       "      <td>hunter winter</td>\n",
       "      <td>32615</td>\n",
       "      <td>14700</td>\n",
       "      <td>ALACHUA</td>\n",
       "      <td>15202</td>\n",
       "      <td>Alachua</td>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197086</th>\n",
       "      <td>mikes dockside restaurant, llc</td>\n",
       "      <td>mike's dockside waterfront grille</td>\n",
       "      <td>34667</td>\n",
       "      <td>14333</td>\n",
       "      <td>HUDSON</td>\n",
       "      <td>14333</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>0.902182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313182</th>\n",
       "      <td>subway v</td>\n",
       "      <td>subway 3174</td>\n",
       "      <td>32514</td>\n",
       "      <td>1325</td>\n",
       "      <td>PENSACOLA</td>\n",
       "      <td>8102</td>\n",
       "      <td>Pensacola</td>\n",
       "      <td>0.902273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755271</th>\n",
       "      <td>innisbrook resort and golf club</td>\n",
       "      <td>innisbrook resort &amp; golf club osprey clubhouse</td>\n",
       "      <td>34684</td>\n",
       "      <td>36750</td>\n",
       "      <td>PALM HARBOR</td>\n",
       "      <td>36750</td>\n",
       "      <td>Palm Harbor</td>\n",
       "      <td>0.902384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               nname_x  \\\n",
       "190603                  la pequena columbia restaurant   \n",
       "288526   fairfield inn & suites by marriott miami airp   \n",
       "248225                 ciro's speakeasy and restaurant   \n",
       "711593            hooks fish & chicken (miami gardens)   \n",
       "704319                katherine's  island  house  cafe   \n",
       "1503336                        mcdonalds store # 15535   \n",
       "1300441                                  hunter marine   \n",
       "197086                  mikes dockside restaurant, llc   \n",
       "313182                                        subway v   \n",
       "755271                 innisbrook resort and golf club   \n",
       "\n",
       "                                                nname_y   nzip   snum_x  \\\n",
       "190603                      la pequena colombia espress  33604     6312   \n",
       "288526               fairfield inn & suites miami south  33126     4101   \n",
       "248225                 ciro's speakeasy and supper club  33606     2109   \n",
       "711593                            hook's fish & chicken  33054    15850   \n",
       "704319                     katherine's island house inc  34134     3801   \n",
       "1503336                     mcdonald's snack bar #15535  32223  10991-1   \n",
       "1300441                                   hunter winter  32615    14700   \n",
       "197086                mike's dockside waterfront grille  34667    14333   \n",
       "313182                                      subway 3174  32514     1325   \n",
       "755271   innisbrook resort & golf club osprey clubhouse  34684    36750   \n",
       "\n",
       "          location_city snum_y          cty_nm     score  \n",
       "190603            TAMPA   6204           Tampa  0.900539  \n",
       "288526            MIAMI   4101           Miami  0.900776  \n",
       "248225            TAMPA   2109           Tampa  0.901540  \n",
       "711593        OPA-LOCKA  15850   Miami Gardens  0.901587  \n",
       "704319   BONITA SPRINGS   3801  Bonita Springs  0.902060  \n",
       "1503336    JACKSONVILLE  10991    Jacksonville  0.902061  \n",
       "1300441         ALACHUA  15202         Alachua  0.902098  \n",
       "197086           HUDSON  14333          Hudson  0.902182  \n",
       "313182        PENSACOLA   8102       Pensacola  0.902273  \n",
       "755271      PALM HARBOR  36750     Palm Harbor  0.902384  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl_a['snum'] = df_fl_a.location_address.str.split(' ',1).apply(lambda x:x[0])\n",
    "df_oge_a_fl['snum'] = df_oge_a_fl.street_addr_1_txt.str.split(' ',1).apply(lambda x:x[0])\n",
    "\n",
    "dfz = df_oge_a_fl.merge(df_fl_a, on='nzip')\n",
    "dfz_nnm = dfz[dfz.nname_x != dfz.nname_y]\n",
    "dfz_nm = dfz[dfz.nname_x == dfz.nname_y]\n",
    "dfz_nnm['score'] = dfz_nnm[['nname_x','nname_y']].apply(lambda p: jellyfish.jaro_winkler(p[0], p[1]), axis=1)\n",
    "\n",
    "dfz_nnm[dfz_nnm.score>0.9][['nname_x', \n",
    "                       'nname_y', \n",
    "                       'nzip', \n",
    "                       'snum_x',\n",
    "                       'location_city',\n",
    "                       'snum_y',\n",
    "                       'cty_nm',\n",
    "                       'score']].sort('score',ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are many businesses that are highly probable matches but don't have identical names (maybe 6 out of 10 in this group of the first 10 with scores > 0.9).   There are two things we can learn from this: one is that we will miss some matches if we rely on exact matches, and the other is that you will rarely find a threshold for any approximate match such that everything is a match.  One of the key problems in matching is finding the most true matches while introducing the least false positives.\n",
    "\n",
    "The first method that's usually tried is to devise some heuristic score and then treat anything above some value of that score as a match, and anything below some value as a miss.  So for example, we could have a score for this data set that is [(Jaro-Winkler similarity of name) + (1 if zips match) + (1 if street number matches)].  We could say that anything > 2.9 is a match, anything < 2.0 is a non-match, and everything between 2.9 and 2.0 is uncertain (ie, might need a human to look at it).\n",
    "\n",
    "That heuristic might work adequately in this case (remember, we're ignoring the fact that we could geocode the addresses), but it's difficult to assess the quality of the results since the threshold numbers are essentially guesses.  In other words we know that we'll have false positive matches and false negative misses, but we can't estimate beforehand how likely those errors will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Matching and Fellegi-Sunter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first rigorous method for finding an optimal set of matches is the Fellegi-Sunter method, which goes back to the 1960s, although it's still in use today.  It was motivated by a need to link records from various data sources collected by the Census Bureau.  Like the heuristic method above, the goals here is to find two threshold levels, T_u and T_l, such that anything above T_u is a positive link, anything below T_l is a positive non_link, and anything between is uncertain.  However, the goal with this method is to be able to set the threshold values such that they meet known (and presumably acceptable) error rates.\n",
    "\n",
    "In general any two data sets will have some subset of attributes, say &alpha;1 to &alpha;n, on which they could match.  With n attributes there are 2^n patterns of matching.  For example, suppose that in our test data we have only business name, zip code, and street number on which to match.  The patterns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>zip</th>\n",
       "      <th>street num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>match</td>\n",
       "      <td>match</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match</td>\n",
       "      <td>match</td>\n",
       "      <td>unmatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>match</td>\n",
       "      <td>unmatch</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>match</td>\n",
       "      <td>unmatch</td>\n",
       "      <td>unmatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unmatch</td>\n",
       "      <td>match</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unmatch</td>\n",
       "      <td>match</td>\n",
       "      <td>unmatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unmatch</td>\n",
       "      <td>unmatch</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unmatch</td>\n",
       "      <td>unmatch</td>\n",
       "      <td>unmatch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name      zip street num\n",
       "0    match    match      match\n",
       "1    match    match    unmatch\n",
       "2    match  unmatch      match\n",
       "3    match  unmatch    unmatch\n",
       "4  unmatch    match      match\n",
       "5  unmatch    match    unmatch\n",
       "6  unmatch  unmatch      match\n",
       "7  unmatch  unmatch    unmatch"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=list(itertools.product(['match','unmatch'], repeat=3)),\n",
    "             columns=['name', 'zip', 'street num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each pattern as a separate comparison on a record from A and a record from B.  So the vector of 8 comparison makes a function on A X B:\n",
    "\n",
    "$$\n",
    "\\gamma(r_{A}, r_{B}) = [ \\gamma_{mmm}(r_{A}, r_{B}), \\gamma_{mmu}(r_{A}, r_{B}), \\dots, \\gamma_{uuu}(r_{A}, r_{B})]\n",
    "$$\n",
    "\n",
    "If you pick records at random from A and B, you'll end up with a bunch of vectors like those below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def gamma(ra, rb):\n",
    "    gg = lambda x: 1 if x else 0\n",
    "    return [gg(((ra[0] == rb[0]) == i) and ((ra[1] == rb[1]) == j) and ((ra[2] == rb[2]) == k)) \n",
    "     for i,j,k in list(itertools.product([True,False], repeat=3))]\n",
    "        \n",
    "print(gamma(('tropicana restaurant & lounge', '33012', '1950'), ('pizza hut', '33566', '2316')))\n",
    "print(gamma(('granger and sons bar-b-que', '33809', '8121'), ('body parts of america', '32534', '8121')))\n",
    "print(gamma((\"antonio's pizzeria restaurant\", '33023', '6890'), ('horizon dental care', '33023', '6890')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the vector patterns will be more common for records that don't match, but some will be more common for records that do match.  You can think of &gamma; as a random variable and the following conditional probabilities can be defined:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in M] \\\\\n",
    "= \\sum_{a,b \\in M} P[\\gamma(r_{A},r_{B})]P[(r_{A}, r_{B}) \\in M]\n",
    "\\end{align}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{align}\n",
    "u(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in U] \\\\\n",
    "= \\sum_{a,b \\in U} P[\\gamma(r_{A},r_{B}]P[(r_{A}, r_{B}) \\in U]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The best patterns for matching will be those that have the largest ratio of m/u.  Suppose that we have two files with 100 records, giving us 100x100=10000 pairs.  Say that all 100 records in each file match.  In 20 cases, the name, zip, and street number are identical.  In 75 cases the zip and street number are the same.  However, in only 2 cases do all attributes match where the records are not a match, but there are 50 cases where the zip and street number match but the record do not match.  Of the remaining 5 matches 4 have the same zip, but 200 non-matches have the same zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.00020202020202020202 990.0\n",
      "0.75 0.005050505050505051 148.5\n",
      "0.04 0.020202020202020204 1.98\n",
      "0.01 1.0048484848484849 0.009951749095295536\n"
     ]
    }
   ],
   "source": [
    "m_111 = (20/100)\n",
    "u_111 = (2/9900)\n",
    "m_011 = (75/100)\n",
    "u_011 = (50/9900)\n",
    "m_010 = (4/100)\n",
    "u_010 = (200/9900)\n",
    "m_000 = (1/100)\n",
    "u_000 = (9948/9900)\n",
    "print(m_111, u_111, m_111/u_111)\n",
    "print(m_011, u_011, m_011/u_011)\n",
    "print(m_010, u_010, m_010/u_010)\n",
    "print(m_000, u_000, m_000/u_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can establish what's called a _linkage rule_ by setting the two values &mu; and &lambda;.  In the Fellegi-Sunter method the values are interpreted as:\n",
    "\n",
    "$$\n",
    "\\mu = P(\\mbox{positive link}|U)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = P(\\mbox{positive non-link}|M)\n",
    "$$\n",
    "\n",
    "In other words, we set &mu; to the level of false positive we're willing to tolerate, and we set &lambda; to the level of false negatives we will accept.  To relate these values to the m and u values in the Fellegi-Sunter method we first rank the m/u ratios in descending order.  For &mu;, we include patterns in the automatic match part of the rule until the sum of the u values exceeds &mu;.  For &lambda; we choose as many patterns from the end of the list \n",
    "\n",
    "For example suppose that we set &mu; = 0.01 indicating that we want at most 1% of false positives in our automatic match set.  If we include the first two of the above patterns, we'll end up with 52 matches for items that are actually in U (sum of u values = 0.0052) but if we include the third pattern we get 252 (sum of u values = 0.0252).  To get a similar level for &lambda; we could only use the final rule to establish positive non-links.  To establish an optimal linkage rule, we can set the upper and lower threshold for matches and non-matches to the m/u ratio for the patterns that meet our &mu; and &lambda; values.  The linkage rule looks like this:\n",
    "\n",
    "$$\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\frac{m}{u} > T_{\\mu}  & \\mbox{match}  \\\\\n",
    "        T_{\\mu} < \\frac{m}{u} < T_{\\lambda}  & \\mbox{potential match}  \\\\\n",
    "\t\t\\frac{m}{u} < T_{\\lambda}  & \\mbox{non-match}  \\\\\n",
    "\t\\end{array}\n",
    "$$\n",
    "\n",
    "And in this particular case, we can set the threshold values to:\n",
    "\n",
    "$$\n",
    "T_{\\mu} = m_{011}/u_{011} \\\\\n",
    "T_{\\lambda} = m_{000}/u_{000}\n",
    "$$\n",
    "\n",
    "\n",
    "In practice, _conditional independence_ will be assumed between the attributes so that we can write\n",
    "\n",
    "$$\n",
    "Pr(\\texttt{name matches}, \\texttt{zip matches}, \\texttt{street matches}|M)\n",
    "$$\n",
    "\n",
    "as the product of probabilities on the individual properties\n",
    "\n",
    "$$\n",
    "Pr(\\texttt{name matches}|M) Pr(\\texttt{zip matches}|M) Pr(\\texttt{street matches}|M)\n",
    "$$\n",
    "\n",
    "The m and u probabilities can then be written in terms of the individual properties, eg:\n",
    "\n",
    "$$\n",
    "m(\\texttt{name matches}) = \\frac{Pr(\\texttt{name matches}|M)}{Pr(\\texttt{name matches}|U)}\n",
    "$$\n",
    "\n",
    "Now the overall m value for a pair of records (a,b) can be written as:\n",
    "\n",
    "$$\n",
    "m = m(\\texttt{name matches}) * m(\\texttt{zip matches}) * m(\\texttt{street matches})\n",
    "$$\n",
    "\n",
    "The m/u ratios are usually converted to weights in the following fashion:\n",
    "\n",
    "$$\n",
    "w_{i} = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tlog_{2}(\\frac{m_{i}}{u_{i}})  & \\mbox{if agreement on field i}  \\\\\n",
    "\t\tlog_{2}(\\frac{1-m_{i}}{1-u_{i}}) & \\mbox{if otherwise}\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_u = [('name', 0.5, 0.0000005),\n",
    "      ('zip', 0.9, 0.001),\n",
    "      ('snum', 0.9, 0.0003)]\n",
    "df_mu = pd.DataFrame(data=m_u, columns=['Field', 'm_i', 'u_i'])\n",
    "df_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def field_w(a, b, m, u):\n",
    "    return math.log(m/u, 2) if a == b else math.log((1.0 - m)/(1.0 - u))\n",
    "\n",
    "def fs_weights(ra, rb, m_u):\n",
    "    return sum( field_w(ra[i], rb[i], m_u[i][1], m_u[i][2]) for i in range(len(ra)) )\n",
    "\n",
    "\n",
    "print(fs_weights(('sea siam', '33156', '12735'), ('sea siam', '33156', '12735'), m_u))\n",
    "print(fs_weights((\"antonio's pizzeria restaurant\", '33023', '6890'), ('horizon dental care', '33023', '6890'), m_u))\n",
    "print(fs_weights(('granger and sons bar-b-que', '33809', '8121'), ('body parts of america', '32534', '8121'), m_u))\n",
    "print(fs_weights(('tropicana restaurant & lounge', '33012', '1950'), ('pizza hut', '33566', '2316'), m_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the weight decreases rapidly as fewer fields match in each record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, by now you'll be thinking that we don't actually know the m and u values because we don't know if a particular item is in the M set or the U set.  The usual solution to this problem for the Fellegi-Sunter method is the _expectation-maximization_, or __EM__ algorithm.  You can think of the EM algorithm as a sort of unsupervised classification method that's trying, in our case, to place each record pair in either the M class or the U class.\n",
    "\n",
    "To start we assume that the distribution of matching patterns (our gammas above) is actually a finite mixture of two distributions, defined by the following parameters:\n",
    "\n",
    "$$\n",
    "\\Phi = ((m_{1}\\dots m_{k}), (u_{1}\\dots u_{k}), p)\n",
    "$$\n",
    "\n",
    "where _m_ and _u_ are vectors of the m and u values, and p is the mixing probability, which here is the probability of a matching record.  [This is similar to the approach taken in _scikit-learn_ Gaussian mixture models, except that these aren't gaussian distributions.] What we want is the _maximum likehood estimate_ of these parameters given the data that we have.  In other words, given the matching patterns that we know (the &gamma;) we want to find the parameters (&phi;) that maximize the likelihood function:\n",
    "\n",
    "$$\n",
    "f(\\gamma; \\Phi)\n",
    "$$\n",
    "\n",
    "This is generally an intractable problem though.  For the EM algorithm we also presume the existence of a _latent_ variable _g_ that determines if the record is in the M set or the U set.  Then the likelihood function becomes:\n",
    "\n",
    "$$\n",
    "f(g,\\gamma; \\Phi) = \\prod_{j=1}({pP(\\gamma_{j}|r_{j} \\in M))^g((1-p)P(\\gamma_{j}|r_{j} \\in U)})^{(1-g)}\n",
    "$$\n",
    "\n",
    "The expectation step in the EM algorithm calculates the expected value of the probability that g=1 given an initial set of parameters\n",
    "\n",
    "$$\n",
    "E(g|\\Phi,R_{ab}) = Pr(g=1|\\Phi,R_{ab})\n",
    "$$\n",
    "\n",
    "Then the maximization step find new values for the parameters given the calculated values of _g_. This process is repeated until the parameter values stop changing given some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matching.fellegi_sunter import fs_em\n",
    "\n",
    "df_fl_prep = pd.read_csv('./data/fl.restaurant-prep.csv', encoding='utf-8')\n",
    "df_oge_prep = pd.read_csv('./data/ogesdw.whd-prep.csv', encoding='utf-8')\n",
    "\n",
    "df1 = df_fl_prep[['nname', 'nzip', 'snum']].sample(frac=0.1)\n",
    "df2 = df_oge_prep[['nname', 'nzip', 'snum']].sample(frac=0.1)\n",
    "df1['k'] = 1\n",
    "df2['k'] = 1\n",
    "record_pairs = []\n",
    "\n",
    "for idx, r in df1.merge(df2, on='k').iterrows():\n",
    "    record_pairs.append(((r.nname_x, r.nzip_x, r.snum_x), (r.nname_y, r.nzip_y, r.snum_y)))\n",
    "\n",
    "mh, uh, ph = fs_em(record_pairs, df_mu.m_i.values, df_mu.u_i.values, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.08910887,  0.98090761,  0.88638131]), array([ 0.00019569,  0.00198455,  0.0003152 ]), 1.7981676907452704e-05)\n",
      "[  455.36697812   494.27189455  2812.14216799]\n",
      "[  8.83088587   8.94916106  11.45745382]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(mh,uh,ph)\n",
    "print(mh/uh)\n",
    "print(np.log2(mh/uh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dataset where some matches and non-matches have already been determined, you can use a standard classifier to develop a model for matching.  This sounds unlikely, but since matching in a commercial setting often involves a team to review matches manually it does happen.\n",
    "\n",
    "This will usually be a two-class problem (match and non-match), but it could also be a three-class problem (match, non-match, potential match).  As with any classification problem, there are a few things to be careful about.  First, you'll want to get a sample of data that's fairly representative of the full data.  However, if you simply sample some fraction of all possible record pairs, you might get too few match cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised and Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dedupe\n",
    "\n",
    "data_d = {}\n",
    "df_oge_a_fl[['nname','nzip','snum']]\n",
    "for idx, t in df_oge_a_fl[['nname','nzip','snum']].iterrows():\n",
    "    data_d[idx] = t.to_dict()\n",
    "\n",
    "fields = [\n",
    "        {'field' : 'nname', 'type': 'String'},\n",
    "        {'field' : 'nzip', 'type': 'Exact'},\n",
    "        {'field' : 'snum', 'type': 'Exact'},\n",
    "        ]\n",
    "\n",
    "deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "deduper.sample(data_d, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair = deduper.uncertainPairs()[0]\n",
    "pd.DataFrame(data=[pair[0].values() + pair[1].values()], columns=['Num1','Name1','Zip1', 'Num2', 'Name2', 'Zip2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deduper.markPairs( {'match':[\n",
    "                                ({'snum': '1620', 'nname': 'berry holding group - hp', 'nzip': '33527'},\n",
    "                                 {'snum': '1620', 'nname': 'berry holding group - hp', 'nzip': '33527'}),\n",
    "                                ({'snum': '936', 'nname': 'st george & the dragon restaurant', 'nzip': '34102'},\n",
    "                                 {'snum': '936', 'nname': 'st george & the dragon restaurant', 'nzip': '34102'}),\n",
    "                                ({'snum': '11801', 'nname': 'jnj growers inc', 'nzip': '33187'},\n",
    "                                 {'snum': '19801', 'nname': 'j n j growers inc', 'nzip': '33187'}))], \n",
    "                    'distinct':[\n",
    "                                ({'snum': 'P.O.', 'nname': 'cruz juan flc', 'nzip': '33862'}, \n",
    "                                 {'snum': '1510', 'nname': 'senior bridge', 'nzip': '32901'}),\n",
    "                                ({'snum': '171', 'nname': 'raul anzualda flc', 'nzip': '32131'}, \n",
    "                                 {'snum': '4461', 'nname': 'bee ridge florist', 'nzip': '34233'}),\n",
    "                                ({'snum': '4332', 'nname': 'advans enterprises', 'nzip': '33317'},\n",
    "                                 {'snum': '19501', 'nname': 'all day enterprises', 'nzip': '33917'}),\n",
    "                                ({'snum': '1920', 'nname': 'suggars restaurant and lounge', 'nzip': '33907'},\n",
    "                                 {'snum': '529', 'nname': 'flavors restaurant and lounge', 'nzip': '34983'}),]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deduper.train()\n",
    "dupes = deduper.match(data_d)\n",
    "dupes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using an attribute\n",
    "- Shingle-based blocking\n",
    "- Locality-Sensitive Hashing/Minhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [String Metric - Wikipedia](https://en.wikipedia.org/wiki/String_metric)\n",
    "- [\"On the Resemblance and Containment of Documents\" - Andrei Broder](http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf)\n",
    "- [_Entity Resolution and Information Quality_ - John R. Talburt](http://www.amazon.com/Entity-Resolution-Information-Quality-Talburt/dp/0123819725)\n",
    "- Christen, P., & SpringerLink (Online service). (2012). Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection. Berlin ; New York: Springer.\n",
    "- [Data Quality and Record Linkage Techniques](http://www.amazon.com/gp/product/B0016PZT7M)\n",
    "Thomas N. Herzog, Fritz J. Scheuren, William E. Winkler\n",
    "-  Fellegi, Ivan; Sunter, Alan (December 1969). \"A Theory for Record Linkage\" (PDF). Journal of the American Statistical Association 64 (328): pp. 1183–1210. doi:10.2307/2286061. JSTOR 2286061.\n",
    "- Gregg, Forest and Derek Eder. 2015. Dedupe. https://github.com/datamade/dedupe.\n",
    "- [Record Linkage in F# – Token Matching, Stable Marriages and the Gale-Shapley algorithm](http://richardminerich.com/2011/09/record-linkage-in-f-token-matching-stable-marriages-and-the-gale-shapley-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_fl_prep = pd.read_csv('./data/fl.restaurant-prep.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'dba', u'location_address', u'location_city', u'location_zip_code',\n",
       "       u'nname', u'nzip', u'snum', u'rating', u'lat', u'lon', u'stnum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl_prep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_fl_prep.nname[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oge_prep = pd.read_csv('./data/ogesdw.whd-prep.csv', encoding='utf-8')\n",
    "type(df_oge_prep.nname[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>zip</th>\n",
       "      <th>street num</th>\n",
       "      <th>m</th>\n",
       "      <th>u</th>\n",
       "      <th>mu</th>\n",
       "      <th>w</th>\n",
       "      <th>cu</th>\n",
       "      <th>cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077476</td>\n",
       "      <td>1.224100e-10</td>\n",
       "      <td>6.329257e+08</td>\n",
       "      <td>29.237461</td>\n",
       "      <td>1.224100e-10</td>\n",
       "      <td>0.077476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.791982</td>\n",
       "      <td>6.254078e-07</td>\n",
       "      <td>1.266345e+06</td>\n",
       "      <td>20.272239</td>\n",
       "      <td>6.255302e-07</td>\n",
       "      <td>0.869458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>3.882342e-07</td>\n",
       "      <td>2.558027e+04</td>\n",
       "      <td>14.642744</td>\n",
       "      <td>1.013764e-06</td>\n",
       "      <td>0.879389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>6.155908e-08</td>\n",
       "      <td>2.449682e+04</td>\n",
       "      <td>14.580307</td>\n",
       "      <td>1.075323e-06</td>\n",
       "      <td>0.880897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101518</td>\n",
       "      <td>1.983536e-03</td>\n",
       "      <td>5.118046e+01</td>\n",
       "      <td>5.677521</td>\n",
       "      <td>1.984612e-03</td>\n",
       "      <td>0.982416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>3.145129e-04</td>\n",
       "      <td>4.901273e+01</td>\n",
       "      <td>5.615085</td>\n",
       "      <td>2.299124e-03</td>\n",
       "      <td>0.997831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.952401e-04</td>\n",
       "      <td>9.900612e-01</td>\n",
       "      <td>-0.014410</td>\n",
       "      <td>2.494365e-03</td>\n",
       "      <td>0.998024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>9.975056e-01</td>\n",
       "      <td>1.980894e-03</td>\n",
       "      <td>-8.979633</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  zip  street num         m             u            mu          w  \\\n",
       "0     1    1           1  0.077476  1.224100e-10  6.329257e+08  29.237461   \n",
       "4     0    1           1  0.791982  6.254078e-07  1.266345e+06  20.272239   \n",
       "1     1    1           0  0.009931  3.882342e-07  2.558027e+04  14.642744   \n",
       "2     1    0           1  0.001508  6.155908e-08  2.449682e+04  14.580307   \n",
       "5     0    1           0  0.101518  1.983536e-03  5.118046e+01   5.677521   \n",
       "6     0    0           1  0.015415  3.145129e-04  4.901273e+01   5.615085   \n",
       "3     1    0           0  0.000193  1.952401e-04  9.900612e-01  -0.014410   \n",
       "7     0    0           0  0.001976  9.975056e-01  1.980894e-03  -8.979633   \n",
       "\n",
       "             cu        cm  \n",
       "0  1.224100e-10  0.077476  \n",
       "4  6.255302e-07  0.869458  \n",
       "1  1.013764e-06  0.879389  \n",
       "2  1.075323e-06  0.880897  \n",
       "5  1.984612e-03  0.982416  \n",
       "6  2.299124e-03  0.997831  \n",
       "3  2.494365e-03  0.998024  \n",
       "7  1.000000e+00  1.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def mp(gamma, mh, uh):\n",
    "    return reduce(operator.mul, (mh[i] ** g * (1 - mh[i]) ** (1 - g) for i,g in enumerate(gamma)))\n",
    "\n",
    "def up(gamma, mh, uh):\n",
    "    return reduce(operator.mul, (uh[i] ** g * (1 - uh[i]) ** (1 - g) for i,g in enumerate(gamma)))\n",
    "\n",
    "def log_score(gamma, mh, uh):\n",
    "    return sum([np.log2(mh[i]/uh[i]) if g==1 else np.log2((1-mh[i])/(1-uh[i])) for i,g in enumerate(gamma)])\n",
    "\n",
    "df_scores = pd.DataFrame(data=list(itertools.product([1,0], repeat=3)),\n",
    "             columns=['name', 'zip', 'street num'])\n",
    "df_scores['m'] = df_scores[['name', 'zip', 'street num']].apply(lambda p: mp(p,mh,uh), axis=1)\n",
    "df_scores['u'] = df_scores[['name', 'zip', 'street num']].apply(lambda p: up(p,mh,uh), axis=1)\n",
    "df_scores['mu'] = df_scores.m/df_scores.u\n",
    "df_scores['w'] = df_scores[['name', 'zip', 'street num']].apply(lambda p: log_score(p,mh,uh), axis=1)\n",
    "df_scores.sort('mu', ascending=False, inplace=True)\n",
    "df_scores['cu'] = df_scores.u.cumsum()\n",
    "df_scores['cm'] = df_scores.m.cumsum()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_fl_prep = pd.read_csv('./data/fl.restaurant-prep.csv')\n",
    "df_oge_prep = pd.read_csv('./data/ogesdw.whd-prep.csv')\n",
    "dfz = df_fl_prep.merge(df_oge_prep, on='nzip')\n",
    "mh = np.array([ 0.08910887,  0.98090761,  0.88638131])\n",
    "uh = np.array([ 0.00019569,  0.00198455,  0.0003152 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gamma_pattern(r_ab):\n",
    "    t1_0 = lambda x: 1 if x else 0\n",
    "#    return [1 if a_i == b_i else 0 for (a_i, b_i) in zip(r_ab[0], r_ab[1])]\n",
    "    return [t1_0(jellyfish.jaro_winkler(r_ab[0][0], r_ab[1][0]) > 0.9),\n",
    "            t1_0(r_ab[0][1] == r_ab[1][1]),\n",
    "            t1_0(r_ab[0][2] == r_ab[1][2])]\n",
    "\n",
    "def score(r):\n",
    "    return log_score(gamma_pattern((r[0:3], r[3:])), mh, uh)\n",
    "\n",
    "dfz['fss'] = dfz[['nname_x', 'nzip', 'snum_x', 'nname_y', 'nzip', 'snum_y']].apply(score, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dba</th>\n",
       "      <th>location_address</th>\n",
       "      <th>location_city</th>\n",
       "      <th>trade_nm</th>\n",
       "      <th>street_addr_1_txt</th>\n",
       "      <th>cty_nm</th>\n",
       "      <th>fss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>LATIN AMERICAN CAFETERIA</td>\n",
       "      <td>401 BISCAYNE BLVD</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>Latin American Cafeteria at Bayside</td>\n",
       "      <td>401 Biscayne Blvd.</td>\n",
       "      <td>Miami</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818921</th>\n",
       "      <td>MCDONALDS</td>\n",
       "      <td>100 S NOVA RD</td>\n",
       "      <td>ORMOND BCH</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>100 Nova Road</td>\n",
       "      <td>Ormond Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819010</th>\n",
       "      <td>ROCKIN RANCH</td>\n",
       "      <td>801 S NOVA RD</td>\n",
       "      <td>ORMOND BCH</td>\n",
       "      <td>Rockin' Ranch</td>\n",
       "      <td>801 S Nova Road</td>\n",
       "      <td>Ormond Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820428</th>\n",
       "      <td>PRESTWICK GOLF CLUB</td>\n",
       "      <td>1044 HAMPSTEAD LN</td>\n",
       "      <td>ORMOND BCH</td>\n",
       "      <td>Prestwick Golf Club at Plantation Bay</td>\n",
       "      <td>1044 Hampstead Land</td>\n",
       "      <td>Ormond Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822439</th>\n",
       "      <td>MARIA BONITA RESTAURANT</td>\n",
       "      <td>195 W GRANADA BLVD</td>\n",
       "      <td>ORMOND BEACH</td>\n",
       "      <td>Maria Bonita Mexican Restaurant</td>\n",
       "      <td>195 W. Granada Blvd</td>\n",
       "      <td>Ormond Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824722</th>\n",
       "      <td>PANTHEON PIZZA</td>\n",
       "      <td>505 WHITE ST</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>Pantheon Pizza</td>\n",
       "      <td>505 White Street</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825318</th>\n",
       "      <td>LARRYS GIANT SUBS</td>\n",
       "      <td>1500 BEVILLE RD</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>Larry's Giant Subs</td>\n",
       "      <td>1500 Beville Road, Suite 604</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826845</th>\n",
       "      <td>HILTON GARDEN INN RESTAURANT</td>\n",
       "      <td>189 MIDWAY AVE</td>\n",
       "      <td>DAYTONA BCH</td>\n",
       "      <td>Hilton Garden Inn</td>\n",
       "      <td>189 Midway Ave</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826971</th>\n",
       "      <td>CHURCH'S CHICKEN</td>\n",
       "      <td>988 ORANGE AVE</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>Church's Chicken</td>\n",
       "      <td>988 Orange Avenue</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827376</th>\n",
       "      <td>HALIFAX RIVER YACHT CLUB</td>\n",
       "      <td>331 SOUTH BEACH STREET</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>Halifax River Yacht Club</td>\n",
       "      <td>331 South Beach Street</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818719</th>\n",
       "      <td>TOMOKA OAKS GOLF CLUB</td>\n",
       "      <td>20 TOMOKA OAKS BLVD</td>\n",
       "      <td>ORMOND BEACH</td>\n",
       "      <td>Tomoka Oaks Golf and Country Club</td>\n",
       "      <td>20 Tomoka Oaks Blvd</td>\n",
       "      <td>Ormond Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830853</th>\n",
       "      <td>PELICAN BAY GOLF CLUB</td>\n",
       "      <td>350 PELICAN BAY DR</td>\n",
       "      <td>DAYTONA BEACH</td>\n",
       "      <td>Pelican Bay North Club</td>\n",
       "      <td>350 Pelican Bay Drive</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831362</th>\n",
       "      <td>FIREHOUSE SUBS</td>\n",
       "      <td>1844 S RIDGEWOOD AVE</td>\n",
       "      <td>SOUTH DAYTONA</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>1844 S. Ridgewood Ave.</td>\n",
       "      <td>South Daytona</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831667</th>\n",
       "      <td>DAYTONA INTERNATIONAL SKATEWAY</td>\n",
       "      <td>2400 S RIDGEWOOD AVE STE 63D</td>\n",
       "      <td>S DAYTONA BCH</td>\n",
       "      <td>Daytona International Skateway</td>\n",
       "      <td>2400 S. Ridgewood Avenue, Ste. 63D</td>\n",
       "      <td>Daytona Beach</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832513</th>\n",
       "      <td>INLET HARBOR MARINA &amp; REST</td>\n",
       "      <td>133 INLET HARBOR RD</td>\n",
       "      <td>PONCE INLET</td>\n",
       "      <td>Inlet Harbor Restaurant</td>\n",
       "      <td>133 Inlet Harbor Road</td>\n",
       "      <td>Port Orange</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832746</th>\n",
       "      <td>MONTEREY GRILL</td>\n",
       "      <td>1665 DUNLAWTON AVE UNIT 101</td>\n",
       "      <td>PORT ORANGE</td>\n",
       "      <td>Monterey Grill</td>\n",
       "      <td>1665 Dunlawton Avenue</td>\n",
       "      <td>Port Orange</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833213</th>\n",
       "      <td>MALIBU BEACH GRILL</td>\n",
       "      <td>5543 WILLIAMSON BLVD</td>\n",
       "      <td>PORT ORANGE</td>\n",
       "      <td>Malibu Beach Grill</td>\n",
       "      <td>5543 South Williamson Blvd #900</td>\n",
       "      <td>Port Orange</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833787</th>\n",
       "      <td>BURGER KING #0056</td>\n",
       "      <td>20 NW 16 AVE</td>\n",
       "      <td>GAINESVILLE</td>\n",
       "      <td>Burger King</td>\n",
       "      <td>20 NW 16th Ave</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834990</th>\n",
       "      <td>DRAGONFLY SUSHI &amp; SAKE CO</td>\n",
       "      <td>201 SE 2 AVE #104</td>\n",
       "      <td>GAINESVILLE</td>\n",
       "      <td>Dragonfly Sushi &amp; Sake Company, Inc.</td>\n",
       "      <td>201 SE 2nd Avenue</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836315</th>\n",
       "      <td>PERKINS RESTAURANT &amp; BAKERY #1131</td>\n",
       "      <td>6825 W NEWBERRY RD</td>\n",
       "      <td>GAINESVILLE</td>\n",
       "      <td>Perkins Restaurant</td>\n",
       "      <td>6825 NW 8th Ave</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>29.237461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      dba              location_address  \\\n",
       "484              LATIN AMERICAN CAFETERIA             401 BISCAYNE BLVD   \n",
       "818921                          MCDONALDS                 100 S NOVA RD   \n",
       "819010                       ROCKIN RANCH                 801 S NOVA RD   \n",
       "820428                PRESTWICK GOLF CLUB             1044 HAMPSTEAD LN   \n",
       "822439            MARIA BONITA RESTAURANT            195 W GRANADA BLVD   \n",
       "824722                     PANTHEON PIZZA                  505 WHITE ST   \n",
       "825318                  LARRYS GIANT SUBS               1500 BEVILLE RD   \n",
       "826845       HILTON GARDEN INN RESTAURANT                189 MIDWAY AVE   \n",
       "826971                   CHURCH'S CHICKEN                988 ORANGE AVE   \n",
       "827376           HALIFAX RIVER YACHT CLUB        331 SOUTH BEACH STREET   \n",
       "818719              TOMOKA OAKS GOLF CLUB           20 TOMOKA OAKS BLVD   \n",
       "830853              PELICAN BAY GOLF CLUB            350 PELICAN BAY DR   \n",
       "831362                     FIREHOUSE SUBS          1844 S RIDGEWOOD AVE   \n",
       "831667     DAYTONA INTERNATIONAL SKATEWAY  2400 S RIDGEWOOD AVE STE 63D   \n",
       "832513         INLET HARBOR MARINA & REST           133 INLET HARBOR RD   \n",
       "832746                     MONTEREY GRILL   1665 DUNLAWTON AVE UNIT 101   \n",
       "833213                 MALIBU BEACH GRILL          5543 WILLIAMSON BLVD   \n",
       "833787                  BURGER KING #0056                  20 NW 16 AVE   \n",
       "834990          DRAGONFLY SUSHI & SAKE CO             201 SE 2 AVE #104   \n",
       "836315  PERKINS RESTAURANT & BAKERY #1131            6825 W NEWBERRY RD   \n",
       "\n",
       "        location_city                               trade_nm  \\\n",
       "484             MIAMI    Latin American Cafeteria at Bayside   \n",
       "818921     ORMOND BCH                             McDonald's   \n",
       "819010     ORMOND BCH                          Rockin' Ranch   \n",
       "820428     ORMOND BCH  Prestwick Golf Club at Plantation Bay   \n",
       "822439   ORMOND BEACH        Maria Bonita Mexican Restaurant   \n",
       "824722  DAYTONA BEACH                         Pantheon Pizza   \n",
       "825318  DAYTONA BEACH                     Larry's Giant Subs   \n",
       "826845    DAYTONA BCH                      Hilton Garden Inn   \n",
       "826971  DAYTONA BEACH                       Church's Chicken   \n",
       "827376  DAYTONA BEACH               Halifax River Yacht Club   \n",
       "818719   ORMOND BEACH      Tomoka Oaks Golf and Country Club   \n",
       "830853  DAYTONA BEACH                 Pelican Bay North Club   \n",
       "831362  SOUTH DAYTONA                         Firehouse Subs   \n",
       "831667  S DAYTONA BCH         Daytona International Skateway   \n",
       "832513    PONCE INLET                Inlet Harbor Restaurant   \n",
       "832746    PORT ORANGE                         Monterey Grill   \n",
       "833213    PORT ORANGE                     Malibu Beach Grill   \n",
       "833787    GAINESVILLE                            Burger King   \n",
       "834990    GAINESVILLE   Dragonfly Sushi & Sake Company, Inc.   \n",
       "836315    GAINESVILLE                     Perkins Restaurant   \n",
       "\n",
       "                         street_addr_1_txt         cty_nm        fss  \n",
       "484                     401 Biscayne Blvd.          Miami  29.237461  \n",
       "818921                       100 Nova Road   Ormond Beach  29.237461  \n",
       "819010                     801 S Nova Road   Ormond Beach  29.237461  \n",
       "820428                 1044 Hampstead Land   Ormond Beach  29.237461  \n",
       "822439                 195 W. Granada Blvd   Ormond Beach  29.237461  \n",
       "824722                    505 White Street  Daytona Beach  29.237461  \n",
       "825318        1500 Beville Road, Suite 604  Daytona Beach  29.237461  \n",
       "826845                      189 Midway Ave  Daytona Beach  29.237461  \n",
       "826971                   988 Orange Avenue  Daytona Beach  29.237461  \n",
       "827376              331 South Beach Street  Daytona Beach  29.237461  \n",
       "818719                 20 Tomoka Oaks Blvd   Ormond Beach  29.237461  \n",
       "830853               350 Pelican Bay Drive  Daytona Beach  29.237461  \n",
       "831362              1844 S. Ridgewood Ave.  South Daytona  29.237461  \n",
       "831667  2400 S. Ridgewood Avenue, Ste. 63D  Daytona Beach  29.237461  \n",
       "832513               133 Inlet Harbor Road    Port Orange  29.237461  \n",
       "832746               1665 Dunlawton Avenue    Port Orange  29.237461  \n",
       "833213     5543 South Williamson Blvd #900    Port Orange  29.237461  \n",
       "833787                      20 NW 16th Ave    Gainesville  29.237461  \n",
       "834990                   201 SE 2nd Avenue    Gainesville  29.237461  \n",
       "836315                     6825 NW 8th Ave    Gainesville  29.237461  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfz[dfz.fss>21.0].sort('fss')[['dba','location_address', 'location_city', 'trade_nm', 'street_addr_1_txt', 'cty_nm', 'fss']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00019569,  0.00198455,  0.0003152 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2553016000000001e-07"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-uh[0])**0 * uh[1]**1 * uh[2]**1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfz['match'] = dfz.fss.apply(lambda x: 1 if x > 21.0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100114"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfz.to_csv('./data/matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
