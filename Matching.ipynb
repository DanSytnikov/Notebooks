{
 "metadata": {
  "name": "",
  "signature": "sha256:5f33c355764622211081d413df38b2bced74a4e61ed4a8df68639dd892ea9163"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Match _All_ The Things: Deduplication, Record Linkage, and Entity Resolution in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you work in the data business, you'll eventually find yourself trying to make an approximate match from one data source to another.  This comes in at least three flavors: _searching_, where a user-supplied name is matched against a database (for example, auto-complete); _joining_ or [_record linkage_](http://en.wikipedia.org/wiki/Record_linkage), where you're trying to join datasets like you would with a relational join except based on an approximate criteria; and _deduplication_ or _clustering_, where you're trying to group items from one source based on similarity.  The general problem of trying to determine if two records refer to the same entity is called _entity resolution_.\n",
      "\n",
      "These tasks are sometimes performed with a SQL database or some other information retrieval system (eg, Solr). But there are situations where we need something slightly different than the normal database management system, or we just want something simpler that doesn't require a separate deployment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Table of Contents\n",
      "\n",
      "1. The Matching Problem\n",
      "\n",
      "1. [Basic Python](#Basic-Python)\n",
      "2. [String Similarity and Distance](#String-Similarity-and-Distance)\n",
      "  1. Levenshtein\n",
      "  1. Jaro/Jaro-Winkler\n",
      "  1. Jaccard\n",
      "  2. Metrics\n",
      "4. Token-Based Matching\n",
      "  1. [N-Grams/Shingling](#N-Grams/Shingling)\n",
      "  1. TF/IDF\n",
      "  2. Gayle-Shapley\n",
      "5. Locality-Sensitive Hashing\n",
      "  1. Minhash\n",
      "6. Semantic Matching\n",
      "  1. Human Names\n",
      "6. Nearest-Neighbor\n",
      "  1. kd-Trees\n",
      "6. Pairwise alignment/Needleman-Wunsch\n",
      "6. Entropy, Mutual Information, Kulbach-Liebler\n",
      "6. Radix Tree/Patricia Trie\n",
      "6. Entity Recognition/CRF\n",
      "6. References"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Matching Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have two sets of data items, A and B, each with no duplicates then \n",
      "\n",
      "$$\n",
      "M=\\{ (a,b)| a \\in A, b \\in B, a = b \\}\n",
      "$$\n",
      "\n",
      "$$\n",
      "U=\\{ (a,b)| a \\in A, b \\in B, a \\ne b \\}\n",
      "$$\n",
      "\n",
      "By definition the union of the two sets M and U is the set of all pairs A and B, or:\n",
      "\n",
      "$$\n",
      "A \\times B = M \\cup U\n",
      "$$\n",
      "\n",
      "Some notes:\n",
      "- The \"=\" indicates a true match.  It is best to assume that in any real data sources, some non-matches will be spuriously classified as in the M set and some true matches will be put in the U set.\n",
      "- The vast majority of pairs in A x B will be in U.  If A and B both have 1000 members, A x B will have 1 million pairs, but M can have at most 1000 elements.\n",
      "- If there is only one dataset (D) but it is assumed to have duplicates, then you'd still potentially have to check all pairs in D x D\n",
      "\n",
      "\n",
      "As an example to use throughout this document, i'm using the the two datasets from [this](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/) article on the enigma.io blog ([On Wages and Hygiene: Surfacing Bad Management in Public Data](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/).  In the article, they found matches using business names and geo-coded addresses, but for now i'm going to retain the individual address components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Florida restaurant inspection data.  Drop everything but unique name and addresses\n",
      "df_fl = pd.read_csv('./data/fl.restaurant-inspections.csv')\n",
      "df_fl_a = df_fl[['dba', 'location_address', 'location_city', 'location_zip_code']].drop_duplicates()\n",
      "\n",
      "# Wage data, drop everything but unique name and addresses, and filter to only Florida.\n",
      "df_oge = pd.read_csv('./data/ogesdw.whd.whisard.fl.csv')\n",
      "df_oge_a_fl = df_oge[['trade_nm', 'legal_name', 'street_addr_1_txt', 'cty_nm', 'st_cd', 'zip_cd']].drop_duplicates()\n",
      "\n",
      "print(len(df_fl_a), len(df_oge_a_fl))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(53576, 16655)\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This leaves us with two sets of about 54k and 17k rows each.  The size of A x B would be almost 900 million.\n",
      "\n",
      "You might think that we can pick off some low-hanging fruit by matching on identical names.  In most cases of string matching, we'll assume that case does not matter and sometimes certain types of punctuation will be removed or substituted.  For now we'll just lowerize the names:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['nname'] = df_fl_a.dba.apply(lambda x:x.lower().translate(None,\"'.,\") if pd.notnull(x) else '')\n",
      "df_oge_a_fl['nname'] = df_oge_a_fl.trade_nm.apply(lambda x:x.lower().translate(None,\"'.,\") if pd.notnull(x) else '')\n",
      "dfm = df_oge_a_fl.merge(df_fl_a, on='nname')\n",
      "dfm.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>trade_nm</th>\n",
        "      <th>legal_name</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "      <th>cty_nm</th>\n",
        "      <th>st_cd</th>\n",
        "      <th>zip_cd</th>\n",
        "      <th>nname</th>\n",
        "      <th>dba</th>\n",
        "      <th>location_address</th>\n",
        "      <th>location_city</th>\n",
        "      <th>location_zip_code</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>Embarq</td>\n",
        "      <td>Embarq Management Company</td>\n",
        "      <td>555 Lake Border Drive</td>\n",
        "      <td>Apopka</td>\n",
        "      <td>FL</td>\n",
        "      <td>32703</td>\n",
        "      <td>embarq</td>\n",
        "      <td>EMBARQ</td>\n",
        "      <td>425 3 ST</td>\n",
        "      <td>LEESBURG</td>\n",
        "      <td>34748</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>Embarq</td>\n",
        "      <td>Embarq Management Company</td>\n",
        "      <td>555 Lake Border Drive</td>\n",
        "      <td>Apopka</td>\n",
        "      <td>FL</td>\n",
        "      <td>32703</td>\n",
        "      <td>embarq</td>\n",
        "      <td>EMBARQ</td>\n",
        "      <td>425 3RD STREET</td>\n",
        "      <td>LEESBURG</td>\n",
        "      <td>34748</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>McDonald's</td>\n",
        "      <td>Paris Enterprises, LLC</td>\n",
        "      <td>9260 NW 39th Ave</td>\n",
        "      <td>Gainesville</td>\n",
        "      <td>FL</td>\n",
        "      <td>32605</td>\n",
        "      <td>mcdonalds</td>\n",
        "      <td>MCDONALDS</td>\n",
        "      <td>460 HIALEAH DR</td>\n",
        "      <td>HIALEAH</td>\n",
        "      <td>330105347</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>McDonald's</td>\n",
        "      <td>Paris Enterprises, LLC</td>\n",
        "      <td>9260 NW 39th Ave</td>\n",
        "      <td>Gainesville</td>\n",
        "      <td>FL</td>\n",
        "      <td>32605</td>\n",
        "      <td>mcdonalds</td>\n",
        "      <td>MCDONALDS</td>\n",
        "      <td>9850 SW 8 ST</td>\n",
        "      <td>MIAMI</td>\n",
        "      <td>331742996</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>McDonald's</td>\n",
        "      <td>Paris Enterprises, LLC</td>\n",
        "      <td>9260 NW 39th Ave</td>\n",
        "      <td>Gainesville</td>\n",
        "      <td>FL</td>\n",
        "      <td>32605</td>\n",
        "      <td>mcdonalds</td>\n",
        "      <td>MCDONALD'S</td>\n",
        "      <td>15096 NW 7 AVE</td>\n",
        "      <td>MIAMI</td>\n",
        "      <td>33168</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "     trade_nm                 legal_name      street_addr_1_txt       cty_nm  \\\n",
        "0      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
        "1      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
        "2  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "3  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "4  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "\n",
        "  st_cd  zip_cd      nname         dba location_address location_city  \\\n",
        "0    FL   32703     embarq      EMBARQ         425 3 ST      LEESBURG   \n",
        "1    FL   32703     embarq      EMBARQ   425 3RD STREET      LEESBURG   \n",
        "2    FL   32605  mcdonalds   MCDONALDS   460 HIALEAH DR       HIALEAH   \n",
        "3    FL   32605  mcdonalds   MCDONALDS     9850 SW 8 ST         MIAMI   \n",
        "4    FL   32605  mcdonalds  MCDONALD'S   15096 NW 7 AVE         MIAMI   \n",
        "\n",
        "  location_zip_code  \n",
        "0             34748  \n",
        "1             34748  \n",
        "2         330105347  \n",
        "3         331742996  \n",
        "4             33168  "
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see though that there are a lot of duplicate names.  For example, every McDonald's franchise has the same name so the cross product has a lot of mismatches.  So how about if we add some other attribute to the merge.  With a bit of \"profiling\", we can check to see if zip code is a good candidate field"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "t = string.maketrans('0123456789', '9999999999')\n",
      "df_fl_a['pzip'] =  df_fl_a.location_zip_code.apply(lambda x: str(x).translate(t) if pd.notnull(x) else x)\n",
      "print(df_fl_a.groupby('pzip').count()['location_zip_code'])\n",
      "df_oge_a_fl['pzip'] =  df_oge_a_fl.zip_cd.apply(lambda x: str(x).translate(t) if pd.notnull(x) else x)\n",
      "print(df_oge_a_fl.groupby('pzip').count()['zip_cd'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pzip\n",
        "99999         47778\n",
        "99999-9999     1611\n",
        "999999999      4179\n",
        "Name: location_zip_code, dtype: int64\n",
        "pzip\n",
        "99999.9    16655\n",
        "Name: zip_cd, dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that every zip in the Florida restaurant data has at least 5 digits.  The wage data does as well, although it appears that pandas interpreted this as a numeric field since it is only contains digits.  Again we can normalized the field in each dataframe so that it's more likely to match:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['nzip'] = df_fl_a.location_zip_code.apply(lambda x:x[0:5] if pd.notnull(x) else x)\n",
      "df_oge_a_fl['nzip'] = df_oge_a_fl.zip_cd.apply(lambda x: str(int(x)))\n",
      "dfmz = df_fl_a.merge(df_oge_a_fl, on=['nname', 'nzip'])\n",
      "dfmz[['nname','nzip', 'location_address', 'street_addr_1_txt']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname</th>\n",
        "      <th>nzip</th>\n",
        "      <th>location_address</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>jimmys place</td>\n",
        "      <td>33161</td>\n",
        "      <td>510 NE 125 ST</td>\n",
        "      <td>510 NE 125 St.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>pascals on ponce</td>\n",
        "      <td>33134</td>\n",
        "      <td>2611 PONCE DE LEON BLVD</td>\n",
        "      <td>2611 Ponce De Leon Blvd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>palomilla grill</td>\n",
        "      <td>33144</td>\n",
        "      <td>6890 W FLAGLER ST</td>\n",
        "      <td>6890  W. Flagler Street</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>meson ria de vigo</td>\n",
        "      <td>33145</td>\n",
        "      <td>1363 - 65 CORAL WAY</td>\n",
        "      <td>1363 SW 22nd St.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>mcdonalds #6589</td>\n",
        "      <td>33142</td>\n",
        "      <td>2200 NW 36 ST</td>\n",
        "      <td>2200 N.W. 36 Street</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "               nname   nzip         location_address         street_addr_1_txt\n",
        "0       jimmys place  33161            510 NE 125 ST            510 NE 125 St.\n",
        "1   pascals on ponce  33134  2611 PONCE DE LEON BLVD  2611 Ponce De Leon Blvd.\n",
        "2    palomilla grill  33144        6890 W FLAGLER ST   6890  W. Flagler Street\n",
        "3  meson ria de vigo  33145      1363 - 65 CORAL WAY          1363 SW 22nd St.\n",
        "4    mcdonalds #6589  33142            2200 NW 36 ST       2200 N.W. 36 Street"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That looks a bit better at first, but it's still easy to find incorrect and duplicate matches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfmz.groupby(['nname','nzip']).filter(lambda x: len(x['cty_nm']) > 1)[['nname',\n",
      "                                                                      'nzip', \n",
      "                                                                      'location_address', \n",
      "                                                                      'street_addr_1_txt']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname</th>\n",
        "      <th>nzip</th>\n",
        "      <th>location_address</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>il bolognese</td>\n",
        "      <td>33139</td>\n",
        "      <td>626 - 630 LINCOLN RD</td>\n",
        "      <td>1052 Ocean Drive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>il bolognese</td>\n",
        "      <td>33139</td>\n",
        "      <td>626 - 630 LINCOLN RD</td>\n",
        "      <td>626 Lincoln Rd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>il bolognese</td>\n",
        "      <td>33139</td>\n",
        "      <td>1400 OCEAN DRIVE</td>\n",
        "      <td>1052 Ocean Drive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>il bolognese</td>\n",
        "      <td>33139</td>\n",
        "      <td>1400 OCEAN DRIVE</td>\n",
        "      <td>626 Lincoln Rd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>wajiros restaurant</td>\n",
        "      <td>33184</td>\n",
        "      <td>12670 SW 8 ST</td>\n",
        "      <td>12670 SW 8th Street</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>wajiros restaurant</td>\n",
        "      <td>33184</td>\n",
        "      <td>12670 SW 8 ST</td>\n",
        "      <td>12670 S.W. 8 Street</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td>hosteria romana</td>\n",
        "      <td>33139</td>\n",
        "      <td>429 ESPANOLA WAY</td>\n",
        "      <td>429 Espaola Way</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td>hosteria romana</td>\n",
        "      <td>33139</td>\n",
        "      <td>429 ESPANOLA WAY</td>\n",
        "      <td>435 Espanola Way #B</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td>johnny rockets</td>\n",
        "      <td>33172</td>\n",
        "      <td>11401 NW 12 ST #E-508</td>\n",
        "      <td>11401 N W 12th Street</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td>johnny rockets</td>\n",
        "      <td>33172</td>\n",
        "      <td>1455 NW 107 AVE #454 A</td>\n",
        "      <td>11401 N W 12th Street</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "                 nname   nzip        location_address      street_addr_1_txt\n",
        "19        il bolognese  33139    626 - 630 LINCOLN RD       1052 Ocean Drive\n",
        "20        il bolognese  33139    626 - 630 LINCOLN RD        626 Lincoln Rd.\n",
        "21        il bolognese  33139        1400 OCEAN DRIVE       1052 Ocean Drive\n",
        "22        il bolognese  33139        1400 OCEAN DRIVE        626 Lincoln Rd.\n",
        "25  wajiros restaurant  33184           12670 SW 8 ST    12670 SW 8th Street\n",
        "26  wajiros restaurant  33184           12670 SW 8 ST    12670 S.W. 8 Street\n",
        "31     hosteria romana  33139        429 ESPANOLA WAY        429 Espaola Way\n",
        "32     hosteria romana  33139        429 ESPANOLA WAY    435 Espanola Way #B\n",
        "41      johnny rockets  33172   11401 NW 12 ST #E-508  11401 N W 12th Street\n",
        "42      johnny rockets  33172  1455 NW 107 AVE #454 A  11401 N W 12th Street"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One potential source of duplicate matches is that the original data sets have duplicates.  Again, that's pretty easy to find with some aggregation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a.groupby(['nname','nzip']).filter(lambda x: len(x['dba']) > 2).sort('dba').head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>dba</th>\n",
        "      <th>location_address</th>\n",
        "      <th>location_city</th>\n",
        "      <th>location_zip_code</th>\n",
        "      <th>nname</th>\n",
        "      <th>pzip</th>\n",
        "      <th>nzip</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>87993</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>3111 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BEACH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87994</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2225 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87996</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2411 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87997</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>3125 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87999</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>400 N ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88045</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2209 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88061</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2125 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BCH</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88291</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2209 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BEACH SHORES</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88474</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>1615 S ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BEACH</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88493</th>\n",
        "      <td>ATLANTIC VENDORS INC</td>\n",
        "      <td>2500 N ATLANTIC AVE</td>\n",
        "      <td>DAYTONA BEACH</td>\n",
        "      <td>32118</td>\n",
        "      <td>atlantic vendors inc</td>\n",
        "      <td>99999</td>\n",
        "      <td>32118</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "                        dba     location_address         location_city  \\\n",
        "87993  ATLANTIC VENDORS INC  3111 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
        "87994  ATLANTIC VENDORS INC  2225 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87996  ATLANTIC VENDORS INC  2411 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87997  ATLANTIC VENDORS INC  3125 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87999  ATLANTIC VENDORS INC   400 N ATLANTIC AVE           DAYTONA BCH   \n",
        "88045  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "88061  ATLANTIC VENDORS INC  2125 S ATLANTIC AVE           DAYTONA BCH   \n",
        "88291  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
        "88474  ATLANTIC VENDORS INC  1615 S ATLANTIC AVE         DAYTONA BEACH   \n",
        "88493  ATLANTIC VENDORS INC  2500 N ATLANTIC AVE         DAYTONA BEACH   \n",
        "\n",
        "      location_zip_code                 nname   pzip   nzip  \n",
        "87993             32118  atlantic vendors inc  99999  32118  \n",
        "87994             32118  atlantic vendors inc  99999  32118  \n",
        "87996             32118  atlantic vendors inc  99999  32118  \n",
        "87997             32118  atlantic vendors inc  99999  32118  \n",
        "87999             32118  atlantic vendors inc  99999  32118  \n",
        "88045             32118  atlantic vendors inc  99999  32118  \n",
        "88061             32118  atlantic vendors inc  99999  32118  \n",
        "88291             32118  atlantic vendors inc  99999  32118  \n",
        "88474             32118  atlantic vendors inc  99999  32118  \n",
        "88493             32118  atlantic vendors inc  99999  32118  "
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is an interesting case because there's one pair that's a clear duplicate (at 2209 S Atlantic), and another triple that all look suspiciously close (2225 S, 2125 S, and 3125 S).\n",
      "\n",
      "There are several things we can learn from the preceding analysis of the two data sets that are common to all data matching problems:\n",
      "\n",
      "- We usually need to preprocess fields (lowerize, normalize, convert types, geocode, split, combine, etc.)\n",
      "- The same data will often be presented in various ways, and to varying degrees of quality in different sources, so it helps to profile.\n",
      "- Might need to dedupe individual sources before trying to link multiple sources.\n",
      "- Different data sources will have variations of name/address, necessitating approximate matching.\n",
      "- Need to assume there will be cases where attributes match but the entities are not a true match.\n",
      "- Need to assume that identifiers (street numbers, zip codes, etc.) will be *wrong* in some percentage of cases (ie, there are true matches where attributes are not equal).\n",
      "\n",
      "Probably the main difficulty in matching problems is determining the degree of similarity between individual attributes, especially string attributes, which will be the subject of the next section."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "String Similarity and Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finding similarity between strings is a subset of the field of natural language processing, so not surprisingly there are many string metrics in the well-known Python [Natural Language Toolkit (NLTK)](http://www.nltk.org/_modules/nltk/metrics/distance.html).  The [jellyfish](https://pypi.python.org/pypi/jellyfish) package is another Python library that has numerous string metrics. There are in fact a huge number of different metrics for measuring the similarity of, or distance between, two text strings.  This [Wikipedia article](http://en.wikipedia.org/wiki/String_metric) lists no fewer than 18. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, don't forget that you can do many string matching operations with basic Python functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string1 = u'Acme Traffic Signal Company'\n",
      "string2 = u'Acme Traffic Signal Co.'\n",
      "string3 = u'ACME TRAFFIC SIGNAL COMPANY'\n",
      "string4 = u'Acme Rocket Powered Products'\n",
      "strings = [string1, string2, string3, string4]\n",
      "\n",
      "print(string1 == string2)\n",
      "print(string1[:20] == string2[:20])\n",
      "print(string1.lower() == string3.lower())\n",
      "print(string2.find('Signal'))\n",
      "print(string2.index('Signal'))\n",
      "print [s for s in strings if s.startswith('Acme')]\n",
      "print [s for s in strings if 'traffic' in s.lower()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n",
        "True\n",
        "True\n",
        "13\n",
        "13\n",
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.', u'Acme Rocket Powered Products']\n",
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.', u'ACME TRAFFIC SIGNAL COMPANY']\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also do \"wildcard\" matches using the Python regular expression module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "[s for s in strings if re.search('Acme.*Signal', s)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.']"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Difflib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another under-appreciated Python module is difflib.  Although its primary use is generating diff-like comparisons of documents, it also has some useful and interesting matching functionality.  The _get_close_matches_ function is a quick way to find near matches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "\n",
      "print(difflib.get_close_matches('Acme Rocket-Powered Inc.', strings))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Acme Rocket Powered Products']\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the difflib SequenceMatcher can generate a score that indicates how well two sequences match (note that the sequences don't necessarily have to be strings).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sm = difflib.SequenceMatcher()\n",
      "sm.set_seq1('Acme Rocket-Powered Inc.')\n",
      "for s in strings:\n",
      "    sm.set_seq2(s)\n",
      "    print(s, sm.ratio())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', 0.27450980392156865)\n",
        "(u'Acme Traffic Signal Co.', 0.2978723404255319)\n",
        "(u'ACME TRAFFIC SIGNAL COMPANY', 0.1568627450980392)\n",
        "(u'Acme Rocket Powered Products', 0.7692307692307693)\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Levenshtein Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probably the best know string similarity metric is the Levenshtein distance, sometimes just called the edit distance, which is simply a count of the 1-character edits you'd have to make to turn one string into another."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import jellyfish\n",
      "\n",
      "print(string1, string2)\n",
      "print(jellyfish.levenshtein_distance(string1, string2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.')\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some theorize that Levenshtein was inspired to invent this metric by the fact that everybody mispelled his name.  As you can see above this means that the value of the Levenshtein distance is a non-negative integer.  There are variants of this metric that allow simple transpositions to be ignored."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jaro and Jaro-Winkler Similarity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(string1, string2)\n",
      "print jellyfish.jaro_distance(string1, string2)\n",
      "print jellyfish.jaro_winkler(string1, string2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.')\n",
        "0.923778851315\n",
        "0.954267310789\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Jaro and Jaro-Winkler similarity metrics were developed specifically for record-linking applications.  Both are described well on the [Jaro Distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) Wikipedia page.  The Jaro-Winkler version gives more emphasis to strings that have an initial matching prefix, which is routinely the case when matching human or business names.  Empirically these metrics seem to give the best results for basic string-level matching."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jaccard Similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jaccard similarity is simply defined as the length of the intersection of two sets divided by the length of the union of the two sets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "def jaccard_similarity(a, b):\n",
      "    x = set(a)\n",
      "    y = set(b)\n",
      "    return len(x & y) / len(x | y)\n",
      "\n",
      "jaccard_similarity(string1, string2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 101,
       "text": [
        "0.8421052631578947"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The definition of the Jaccard similarity gives it interesting properties that aren't necessarily true of the string similarity measures we seen so far.  First, the _order_ of the characters in the two strings does not matter since they are treated as sets, and second extra copies of the same elements in one of the sets have no effect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(jaccard_similarity('abcdef', 'abcdef'), \n",
      "      jaccard_similarity('abcdef', 'cbfaed'), \n",
      "      jaccard_similarity('abcdef', 'cbfaedcbfaed') )\n",
      "print(jellyfish.jaro_distance(u'abcdef', u'abcdef'), \n",
      "      jellyfish.jaro_distance(u'abcdef', u'cbfaed'), \n",
      "      jellyfish.jaro_distance(u'abcdef', u'cbfaedcbfaed') )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1.0, 1.0, 1.0)\n",
        "(1.0, 0.611111111111111, 0.7222222222222222)\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Third, Jaccard similarity can be easily applied to non-character data since it operates on sets.  This will turn out to be handy later:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(jaccard_similarity([2,3,4,5], [3,5]))\n",
      "print(jaccard_similarity([\"bag\",\"of\",\"words\"], [\"words\",\"bag\"]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.5\n",
        "0.666666666667\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The term _distance_ or _metric_ is sometimes used informally for any value that measures the similarity of strings, but there's a formal definition for a distance metric\n",
      "\n",
      "- d(a, a) = 0\n",
      "- d(a, b) \u2265 0\n",
      "- d(a, b) = d(b, a)\n",
      "- d(a, c) \u2264 d(a, b) + d(b, c)  #  The \"triangle inequality\n",
      "\n",
      "The Levenshtein distance does follow these rules.  However, the Jaro distance doesn't, mainly because the Jaro distance as implemented here is actually a similarity metric, that is, two identical strings have the maximum score (here 1.0) rather than 0.0.  However, even if you define a distance metric as 1.0 - Jaro, it's still not a metric.  It's a little difficult to prove that something _is_ a metric, however it's easy to find counter-examples showing that it's not:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "def check_triangle(f):\n",
      "    t0 = u'abc'\n",
      "    p = [''.join(p) for p in itertools.permutations(t0)]\n",
      "\n",
      "    for t1 in p:\n",
      "        for t2 in p:\n",
      "            if(f(t0, t2) > (f(t0, t1) + f(t1,t2))):\n",
      "                print(\"d({t0}-{t2})={d02}  d({t0}-{t1}) + d({t1}-{t2})={d01_12}\".format(t0=t0,t1=t1,t2=t2,d02=f(t0, t2), d01_12=f(t0, t1) + f(t1,t2)))\n",
      "\n",
      "check_triangle(lambda x,y : 1 - jellyfish.jaro_winkler(x, y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "d(abc-bca)=1.0  d(abc-acb) + d(acb-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-acb) + d(acb-cab)=0.888888888889\n",
        "d(abc-bca)=1.0  d(abc-bac) + d(bac-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-bac) + d(bac-cab)=0.888888888889\n",
        "d(abc-bca)=1.0  d(abc-cba) + d(cba-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-cba) + d(cba-cab)=0.888888888889\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jaccard similarity can also be turned into a proper distance metric just by subtracting it from 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard_distance(a, b): return 1 - jaccard_similarity(a, b)\n",
      "print(jaccard_distance(string1, string2))\n",
      "print(jaccard_distance('abcdef', 'cbfaed'))\n",
      "check_triangle(jaccard_distance)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.157894736842\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The properties of a distance metric are primarily useful in clustering applications, where we need a consistent idea of how far apart are the items we're trying to cluster.  A true metric creates what's called a _metric space_, the most common instance of which is a Euclidean space like we're used to in geometry."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best metric to use depends on the application.  Some string similarity metrics like the edit distance are based on the idea that small differences are introduced into entity names, probably because of human error.  These metrics are good at finding similar strings with transpositions, misspellings, added or deleted characters, or abbreviations.  The Jaro distance and the Jaro-Winkler variation were developed for record linkage applications.  Something like Jaccard similarity might be better where the sequence is less important than the content or topic.\n",
      "\n",
      "Below is a case where the three metrics are applied to four strings that are in fact names for the same company (a mutual fund company taken from SEC data-- the first name is the current name and the other three are former names)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = [u'FRANKLIN LTD DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON LTD DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON STRATEGIC INCOME TRUST ']\n",
      "\n",
      "similarities = [(n1,n2, jellyfish.levenshtein_distance(n1,n2), jellyfish.jaro_distance(n1,n2), jaccard_similarity(n1,n2))\n",
      "              for n1 in names for n2 in names]\n",
      "        \n",
      "import pandas as pd\n",
      "df = pd.DataFrame(similarities)\n",
      "df.columns = ['n1', 'n2', 'Levenshtein', 'Jaro', 'Jaccard']\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>n1</th>\n",
        "      <th>n2</th>\n",
        "      <th>Levenshtein</th>\n",
        "      <th>Jaro</th>\n",
        "      <th>Jaccard</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>0</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.794935</td>\n",
        "      <td>0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>10</td>\n",
        "      <td>0.816399</td>\n",
        "      <td>0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.765007</td>\n",
        "      <td>0.833333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.794935</td>\n",
        "      <td>0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>0</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>4</td>\n",
        "      <td>0.911616</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.845314</td>\n",
        "      <td>0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>10</td>\n",
        "      <td>0.816399</td>\n",
        "      <td>0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>4</td>\n",
        "      <td>0.911616</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>0</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>10</td>\n",
        "      <td>0.843852</td>\n",
        "      <td>0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.765007</td>\n",
        "      <td>0.833333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>14</td>\n",
        "      <td>0.845314</td>\n",
        "      <td>0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>10</td>\n",
        "      <td>0.843852</td>\n",
        "      <td>0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>FRANKLIN TEMPLETON STRATEGIC INCOME TRUST</td>\n",
        "      <td>0</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 106,
       "text": [
        "                                                  n1  \\\n",
        "0                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "1                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "2                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "3                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "4   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "6   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "7   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "8       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "9       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "11      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "12        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "13        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "14        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "\n",
        "                                                  n2  Levenshtein      Jaro  \\\n",
        "0                 FRANKLIN LTD DURATION INCOME TRUST            0  1.000000   \n",
        "1   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.794935   \n",
        "2       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.816399   \n",
        "3         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.765007   \n",
        "4                 FRANKLIN LTD DURATION INCOME TRUST           14  0.794935   \n",
        "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            0  1.000000   \n",
        "6       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            4  0.911616   \n",
        "7         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.845314   \n",
        "8                 FRANKLIN LTD DURATION INCOME TRUST           10  0.816399   \n",
        "9   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            4  0.911616   \n",
        "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            0  1.000000   \n",
        "11        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            10  0.843852   \n",
        "12                FRANKLIN LTD DURATION INCOME TRUST           14  0.765007   \n",
        "13  FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.845314   \n",
        "14      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.843852   \n",
        "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST             0  1.000000   \n",
        "\n",
        "     Jaccard  \n",
        "0   1.000000  \n",
        "1   0.941176  \n",
        "2   0.941176  \n",
        "3   0.833333  \n",
        "4   0.941176  \n",
        "5   1.000000  \n",
        "6   1.000000  \n",
        "7   0.888889  \n",
        "8   0.941176  \n",
        "9   1.000000  \n",
        "10  1.000000  \n",
        "11  0.888889  \n",
        "12  0.833333  \n",
        "13  0.888889  \n",
        "14  0.888889  \n",
        "15  1.000000  "
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this application, the Jaro distance seems like the most selective.  It ranks the similarity in the most intuitive way.  Note that the Jaccard similarity sometimes gives the highest score (1.0) to strings that aren't actually identical."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Token-Based Matching"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A lot of the techniques we've looked at already could be applied to tokens (or really any sequence) but things like Levenshtein distance and Jaro distance are usually applied to strings.  Normally by _token_ we'd mean some delimited piece of a larger strings or expression, but here we're going to use a fairly loose definition of tokens here as basically any subsequence of a string.  That might mean words, but it also might various other things as we'll see."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "N-Grams/Shingling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Languages have structure in the sense that some letters are more likely to follow other letters, and some words are more likely to follow others.  The term n-gram sometimes refers to the overlapping sequences of characters in a string, and sometimes to overlapping sequences of tokens.  These are also called _shingles_ (because they overlap, get it?).  As an example, below are the character bigrams for two of the names above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# N-gram function courtesy of Peter Norvig\n",
      "def ngrams(seq, n):\n",
      "    \"List all the (overlapping) ngrams in a sequence.\"\n",
      "    return [seq[i:i+n] for i in range(1+len(seq)-n)]\n",
      "\n",
      "print(ngrams(names[0], 2))\n",
      "print(ngrams(names[1], 2))\n",
      "print(jaccard_similarity(names[0], names[1]), jaccard_similarity(ngrams(names[0], 2), ngrams(names[1], 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'FR', u'RA', u'AN', u'NK', u'KL', u'LI', u'IN', u'N ', u' L', u'LT', u'TD', u'D ', u' D', u'DU', u'UR', u'RA', u'AT', u'TI', u'IO', u'ON', u'N ', u' I', u'IN', u'NC', u'CO', u'OM', u'ME', u'E ', u' T', u'TR', u'RU', u'US', u'ST']\n",
        "[u'FR', u'RA', u'AN', u'NK', u'KL', u'LI', u'IN', u'N ', u' T', u'TE', u'EM', u'MP', u'PL', u'LE', u'ET', u'TO', u'ON', u'N ', u' L', u'LI', u'IM', u'MI', u'IT', u'TE', u'ED', u'D ', u' D', u'DU', u'UR', u'RA', u'AT', u'TI', u'IO', u'ON', u'N ', u' I', u'IN', u'NC', u'CO', u'OM', u'ME', u'E ', u' T', u'TR', u'RU', u'US', u'ST']\n",
        "(0.9411764705882353, 0.6829268292682927)\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the Jaccard similarity of the original strings is much higher than the Jaccard similarity of the bigram sets.  We can calculate a Jaccard score for the tokens in the strings as well (after we tweak the lists of bigrams slightly since our ngrams function returns unhashable list elements):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(ngrams(names[0].split(), 2))\n",
      "print(ngrams(names[1].split(), 2))\n",
      "print(jaccard_similarity((tuple(x) for x in ngrams(names[0].split(), 2)), \n",
      "                         (tuple(x) for x in ngrams(names[2].split(), 2))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'FRANKLIN', u'LTD'], [u'LTD', u'DURATION'], [u'DURATION', u'INCOME'], [u'INCOME', u'TRUST']]\n",
        "[[u'FRANKLIN', u'TEMPLETON'], [u'TEMPLETON', u'LIMITED'], [u'LIMITED', u'DURATION'], [u'DURATION', u'INCOME'], [u'INCOME', u'TRUST']]\n",
        "0.5\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Term Frequency/Inverse Document Frequency"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One basic similarity metric is _cosine similarity_, which measures how close two vectors are in an n-dimensional space by computing the cosine of the angle between the two vectors.  As you might recall from basic linear algebra, the cosine between to vectors X and Y can be computed as:\n",
      "\n",
      "$$\n",
      "cos(\\theta) = \\frac{ X \\cdot Y }{\\| X |\\ \\| Y \\|}\n",
      "$$\n",
      "\n",
      "That's cool, if you happen to have vectors.  The trick is to make a string of tokens look like a vector.  One way to do this is to analyze a whole set of items to match and calculate properties of each item with respect to the whole set (in information retrieval (IR) a token is a _term_, the item is a _document_ and the set is a _corpus_).  The most common way to do this is to calculate _term frequency_ and _inverse document frequency_, or TdIdf.\n",
      "\n",
      "The term frequency is just the count of a particular token in an item.  The inverse document frequency is a measure of how rare a word is.  Common tokens will occur in most items (documents) so their inverse frequency will be low, and vice versa.  Inverse document frequency is usually defined like this:\n",
      "\n",
      "$$\n",
      "idf(t,D) = log \\frac{|D|}{| d \\in D, t \\in d |}\n",
      "$$\n",
      "\n",
      "The log just magnifies the value of rare terms.\n",
      "\n",
      "In many entity resolution (record linkage, deduplication) cases the documents are small, so term frequencies > 1 are rare, but they do happen.  For example, lets look at the Florida restaurant names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import itertools\n",
      "from collections import Counter\n",
      "\n",
      "doc_terms = [(idx, t)for idx, name in df_fl_a.nname.iteritems() for t in re.split('\\W+', name) ]\n",
      "term_counts = Counter(doc_terms)\n",
      "print(term_counts.most_common(10))\n",
      "print(df_fl_a.ix[23315])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[((25078, 'subs'), 3), ((23315, 'village'), 3), ((114117, 'tiki'), 3), ((38177, 'boca'), 2), ((95227, 'blue'), 2), ((126172, 's'), 2), ((35103, 'j'), 2), ((89781, 'd'), 2), ((96959, 'r'), 2), ((89992, 'the'), 2)]\n",
        "dba                  VILLAGE GRILLE VILLAGE PUMP VILLAGE SUSHI\n",
        "location_address                                4404 EL MAR DR\n",
        "location_city                             LAUDERDALE-BY-THE-SE\n",
        "location_zip_code                                        33308\n",
        "nname                village grille village pump village sushi\n",
        "pzip                                                     99999\n",
        "nzip                                                     33308\n",
        "Name: 23315, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inverse document frequency will still be fairly interesting in the matching case, since there are many documents even though they are small"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = Counter(itertools.chain(*(re.split('\\W+',n) for n in df_fl_a.nname)))\n",
      "\n",
      "idf = lambda c,t: math.log(len(c) / c[t])\n",
      "n1 = 'south sea chinese restaurant'\n",
      "name_tokens = re.split('\\W+', n1)\n",
      "n1_idf = {t: idf(counts, t) for t in name_tokens}\n",
      "print(n1_idf)\n",
      "print(counts['sea'], counts['south'], counts['restaurant'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'chinese': 4.035267096851123, 'sea': 5.485277272357121, 'south': 4.909255449034588, 'restaurant': 1.707808739111937}\n",
        "(95, 169, 4152)\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now that we can make a TfIdf score for each token, we can make a vector for each name and compare them with cosine similarity.  The vector space that we're dealing with is n-dimensional, where n is the total number of terms in the set of items, so most terms in the vector would be zero (in this case we'd have n=22548).  In practice to calculate similarity you just find the set of tokens that match in each string:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dot = lambda a,b : sum(x*y for x,y in zip(a,b))\n",
      "\n",
      "n2 = 'atlantic chinese restaurant'\n",
      "n3 = 'south beach cafeteria & restaurant'\n",
      "\n",
      "w_12 = [idf(counts, 'chinese'), idf(counts, 'restaurant')]\n",
      "w_11 = [idf(counts, t) for t in name_tokens]\n",
      "w_22 = [idf(counts, t) for t in re.split('\\W+', n2)]\n",
      "\n",
      "w_13 = [idf(counts, 'south'), idf(counts, 'restaurant')]\n",
      "w_33 = [idf(counts, t) for t in re.split('\\W+', n3)]\n",
      "\n",
      "print( dot(w_12, w_12)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_22, w_22))) )\n",
      "print( dot(w_13, w_13)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_33, w_33))) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.29688427984\n",
        "0.415154789268\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that even though both strings share two tokens, (n1,n3) scores slightly higher because the token _south_ is more infrequent than _chinese_."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SoftTfIdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One issue with the Tf-Idf approach is that only identical tokens will contribute to the numerator of the cosine similarity.  That's less of an issue with larger documents where the objective is to find documents that match some query.  Here though we're trying to find items with the greatest degree of similarity.  One method that works well is called _SoftTfIdf_, where instead of identical tokens, the numerator includes every pair of tokens that's similar enough with respect to some string similarity metric, like Jaro-Winkler.\n",
      "\n",
      "This technique is helpful in many applications because there are often frequently encountered patterns in business or product names.  For example in our test data there are is a case where the name 'ole mexican restaurant' in one data source is misspelled as 'ol mexican restaurant' in the other source.  The Jaro-Winkler score for this would indicate similarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jellyfish.jaro_winkler(u'ole mexican restaurant', u'ol mexican restaurant')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 112,
       "text": [
        "0.9243867243867243"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However the normal Tf-Idf approach would not regard these as similar because the only terms that they share are the relatively common tokens 'mexican' and 'restaurant'.  The SoftTf-Idf approach however uses all tokens with a given level of similarity:\n",
      "\n",
      "$$\n",
      "\\{ sim(w,v) > \\theta | w \\in X, v \\in Y \\} \n",
      "$$\n",
      "\n",
      "where X and Y are two names being compared and &theta; is a threshold value for a second similarity function, such as Jaro-Winkler.  Assuming that the token pair ('ole','ol') meets the threshold for a given _sim_ then SoftTdf-Idf _would_ regard these as similar because it would include the pair in its comparison.  \n",
      "\n",
      "On the other hand, Jaro-Winkle would _also_ regard these pairs as fairly similar:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jellyfish.jaro_winkler(u'df mexican restaurant', u'ol mexican restaurant')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "0.9365079365079364"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SoftTf-Idf approach works much better here because it not give as much value to the more common tokens."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gayle-Shapley Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the trickier parts of the SoftTf-Idf algorithm is picking a combination of similarity function and threshold so that you get a good set of tokens to match .  It's possible a token in the first string might be similar to two tokens in the second string.  It's also possible that token _t1_ in the first string has it's best match with token _t2_ in the candidate string, but token _t3_ in the first string _also_ has its best match with _t2_. [NB: SoftTf-Idf is not usually implemented this way-- i got the idea from [Richard Minerich's blog](http://richardminerich.com/2011/09/record-linkage-in-f-token-matching-stable-marriages-and-the-gale-shapley-algorithm/)]\n",
      "\n",
      "Fortunately there's a well-known and understood algorithm for making the best set of matches.  The Gayle-Shapley algorithm, also sometimes called the _stable marriage_ algorithm, takes the preferences of each of two sets and finds the match that best meets all of the preferences.  In this case, the preferences are the similarity between tokens.  Below is an implemenation that's adapted from the algorithm's Wikipedia page."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pair_tokens(r1, r2):\n",
      "    ''' Find the optimal pairing of tokens.  This is basically a version of the Gayle-Shapley\n",
      "    algorithm '''\n",
      "    xmatched = {}\n",
      "    unmatched_tokens = list(r1.keys())\n",
      "    while unmatched_tokens:\n",
      "        token = unmatched_tokens.pop(0)\n",
      "        match_token = r1[token].pop(0)\n",
      "        if match_token not in xmatched:\n",
      "            xmatched[match_token] = token\n",
      "        else:\n",
      "            curr_match = xmatched[match_token]\n",
      "            if r2[match_token].index(token) < r2[match_token].index(curr_match):\n",
      "                xmatched[match_token] = token\n",
      "                if r1[curr_match]:  # pragma: no cover\n",
      "                    unmatched_tokens.append(curr_match)\n",
      "            else:\n",
      "                if r1[token]:\n",
      "                    unmatched_tokens.append(token)\n",
      "\n",
      "    return {v: k for k, v in xmatched.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So for example in the two following strings, the two tokens _bakery_ and _bistro_ in the first string both rank _bistro_ as their best token match, but the second string also ranks bistro as it's best match for bistro, so bakery ends up with its second choice, _california_."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n1 = u'frescos bistro and bakery'\n",
      "n2 = u'fresco california bistro'\n",
      "\n",
      "r1 = {u'and': [u'california', u'fresco', u'bistro'],\n",
      " u'bakery': [u'bistro', u'california', u'fresco'],\n",
      " u'bistro': [u'bistro', u'fresco', u'california'],\n",
      " u'frescos': [u'fresco', u'bistro', u'california']}\n",
      "r2 = {u'bistro': [u'bistro', u'bakery', u'frescos', u'and'],\n",
      " u'california': [u'bakery', u'bistro', u'and', u'frescos'],\n",
      " u'fresco': [u'frescos', u'bistro', u'bakery', u'and']}\n",
      "pair_tokens(r1,r2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 115,
       "text": [
        "{u'bakery': u'california', u'bistro': u'bistro', u'frescos': u'fresco'}"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Matching Non-Text Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-Numbers (L1, L2 norm, relative differences)\n",
      "-Addresses - geocoding, lat./long.\n",
      "-NAICS codes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Matching Sets of Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have numerous tools to match approximately on attributes, so we'll return to the problem of matching items in one set of data to another set.  Remember that in principle we have to check every single pair in the set A x B, which for our test data is about 900M pairs.  Although that's not a huge problem by today's standards, it's still significant and we know that most pairs are not matches, so we seek some way to reduce the number of pairs to test.  Usually this is accomplished by some sort of _indexing_, or in the language of entity resolution _blocking_.  We will discuss blocking later, but usually it involves clustering the candidate items in some way so that each item in set A only needs to be checked against a corresponding cluster in set B.\n",
      "\n",
      "For the sake of argument, let's say that we can use our normalized zip code as an index, and we'll make a dataframe that has all of the remaining potential matches.  Once we have the zip-indexed dataframe we'll compare the names of the remaining pairs using the Jaro-Winkler similarity, after removing the cases where the names already match exactly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['snum'] = df_fl_a.location_address.str.split(' ',1).apply(lambda x:x[0])\n",
      "df_oge_a_fl['snum'] = df_oge_a_fl.street_addr_1_txt.str.split(' ',1).apply(lambda x:x[0])\n",
      "\n",
      "dfz = df_oge_a_fl.merge(df_fl_a, on='nzip')\n",
      "dfz_nnm = dfz[dfz.nname_x != dfz.nname_y]\n",
      "dfz_nm = dfz[dfz.nname_x == dfz.nname_y]\n",
      "dfz_nnm['score'] = dfz_nnm[['nname_x','nname_y']].apply(lambda p: jellyfish.jaro_winkler(unicode(p[0], 'unicode-escape'),\n",
      "                                                                                   unicode(p[1], 'unicode-escape')), axis=1)\n",
      "\n",
      "dfz_nnm[dfz_nnm.score>0.9][['nname_x', \n",
      "                       'nname_y', \n",
      "                       'nzip', \n",
      "                       'snum_x',\n",
      "                       'location_city',\n",
      "                       'snum_y',\n",
      "                       'cty_nm',\n",
      "                       'score']].sort('score',ascending=True).head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname_x</th>\n",
        "      <th>nname_y</th>\n",
        "      <th>nzip</th>\n",
        "      <th>snum_x</th>\n",
        "      <th>location_city</th>\n",
        "      <th>snum_y</th>\n",
        "      <th>cty_nm</th>\n",
        "      <th>score</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>190603</th>\n",
        "      <td>la pequena columbia restaurant</td>\n",
        "      <td>la pequena colombia espress</td>\n",
        "      <td>33604</td>\n",
        "      <td>6312</td>\n",
        "      <td>TAMPA</td>\n",
        "      <td>6204</td>\n",
        "      <td>Tampa</td>\n",
        "      <td>0.900539</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>288526</th>\n",
        "      <td>fairfield inn &amp; suites by marriott miami airp</td>\n",
        "      <td>fairfield inn &amp; suites miami south</td>\n",
        "      <td>33126</td>\n",
        "      <td>4101</td>\n",
        "      <td>MIAMI</td>\n",
        "      <td>4101</td>\n",
        "      <td>Miami</td>\n",
        "      <td>0.900776</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1217187</th>\n",
        "      <td>nauti-nancies inc</td>\n",
        "      <td>nauti-nancys cafe</td>\n",
        "      <td>33755</td>\n",
        "      <td>700</td>\n",
        "      <td>CLEARWATER</td>\n",
        "      <td>700</td>\n",
        "      <td>Clearwater</td>\n",
        "      <td>0.900840</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1515345</th>\n",
        "      <td>clancys irish sports pub &amp; grill</td>\n",
        "      <td>clancys sports pub &amp; grill</td>\n",
        "      <td>34210</td>\n",
        "      <td>6218</td>\n",
        "      <td>BRADENTON</td>\n",
        "      <td>6218</td>\n",
        "      <td>Bradenton</td>\n",
        "      <td>0.900962</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>379865</th>\n",
        "      <td>south beach atrium inc</td>\n",
        "      <td>south beach munchies</td>\n",
        "      <td>33139</td>\n",
        "      <td>1434</td>\n",
        "      <td>MIAMI BEACH</td>\n",
        "      <td>324</td>\n",
        "      <td>Miami Beach</td>\n",
        "      <td>0.901016</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>289563</th>\n",
        "      <td>homewood suites hilton miami airport</td>\n",
        "      <td>homewood suites miami airport blue lagoon</td>\n",
        "      <td>33126</td>\n",
        "      <td>5500</td>\n",
        "      <td>MIAMI</td>\n",
        "      <td>5500</td>\n",
        "      <td>Miami</td>\n",
        "      <td>0.901885</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1300441</th>\n",
        "      <td>hunter marine</td>\n",
        "      <td>hunter winter</td>\n",
        "      <td>32615</td>\n",
        "      <td>14700</td>\n",
        "      <td>ALACHUA</td>\n",
        "      <td>15202</td>\n",
        "      <td>Alachua</td>\n",
        "      <td>0.902098</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106027</th>\n",
        "      <td>frescos bistro and bakery</td>\n",
        "      <td>frescos bakery &amp; bistro inc</td>\n",
        "      <td>33801</td>\n",
        "      <td>132</td>\n",
        "      <td>LAKELAND</td>\n",
        "      <td>132</td>\n",
        "      <td>Lakeland</td>\n",
        "      <td>0.902196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>313182</th>\n",
        "      <td>subway v</td>\n",
        "      <td>subway 3174</td>\n",
        "      <td>32514</td>\n",
        "      <td>1325</td>\n",
        "      <td>PENSACOLA</td>\n",
        "      <td>8102</td>\n",
        "      <td>Pensacola</td>\n",
        "      <td>0.902273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>755271</th>\n",
        "      <td>innisbrook resort and golf club</td>\n",
        "      <td>innisbrook resort &amp; golf club osprey clubhouse</td>\n",
        "      <td>34684</td>\n",
        "      <td>36750</td>\n",
        "      <td>PALM HARBOR</td>\n",
        "      <td>36750</td>\n",
        "      <td>Palm Harbor</td>\n",
        "      <td>0.902384</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "                                               nname_x  \\\n",
        "190603                  la pequena columbia restaurant   \n",
        "288526   fairfield inn & suites by marriott miami airp   \n",
        "1217187                              nauti-nancies inc   \n",
        "1515345               clancys irish sports pub & grill   \n",
        "379865                          south beach atrium inc   \n",
        "289563            homewood suites hilton miami airport   \n",
        "1300441                                  hunter marine   \n",
        "106027                       frescos bistro and bakery   \n",
        "313182                                        subway v   \n",
        "755271                 innisbrook resort and golf club   \n",
        "\n",
        "                                                nname_y   nzip snum_x  \\\n",
        "190603                      la pequena colombia espress  33604   6312   \n",
        "288526               fairfield inn & suites miami south  33126   4101   \n",
        "1217187                               nauti-nancys cafe  33755    700   \n",
        "1515345                      clancys sports pub & grill  34210   6218   \n",
        "379865                             south beach munchies  33139   1434   \n",
        "289563        homewood suites miami airport blue lagoon  33126   5500   \n",
        "1300441                                   hunter winter  32615  14700   \n",
        "106027                      frescos bakery & bistro inc  33801    132   \n",
        "313182                                      subway 3174  32514   1325   \n",
        "755271   innisbrook resort & golf club osprey clubhouse  34684  36750   \n",
        "\n",
        "        location_city snum_y       cty_nm     score  \n",
        "190603          TAMPA   6204        Tampa  0.900539  \n",
        "288526          MIAMI   4101        Miami  0.900776  \n",
        "1217187    CLEARWATER    700   Clearwater  0.900840  \n",
        "1515345     BRADENTON   6218    Bradenton  0.900962  \n",
        "379865    MIAMI BEACH    324  Miami Beach  0.901016  \n",
        "289563          MIAMI   5500        Miami  0.901885  \n",
        "1300441       ALACHUA  15202      Alachua  0.902098  \n",
        "106027       LAKELAND    132     Lakeland  0.902196  \n",
        "313182      PENSACOLA   8102    Pensacola  0.902273  \n",
        "755271    PALM HARBOR  36750  Palm Harbor  0.902384  "
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that there are many businesses that are highly probable matches but don't have identical names (maybe 6 out of 10 in this group of the first 10 with scores > 0.9).   There are two things we can learn from this: one is that we will miss some matches if we rely on exact matches, and the other is that you will rarely find a threshold for any approximate match such that everything is a match.  One of the key problems in matching is finding the most true matches while introducing the least false positives.\n",
      "\n",
      "The first method that's usually tried is to devise some heuristic score and then treat anything above some value of that score as a match, and anything below some value as a miss.  So for example, we could have a score for this data set that is [(Jaro-Winkler similarity of name) + (1 if zips match) + (1 if street number matches)].  We could say that anything > 2.9 is a match, anything < 2.0 is a non-match, and everything between 2.9 and 2.0 is uncertain (ie, might need a human to look at it).\n",
      "\n",
      "That heuristic might work adequately in this case (remember, we're ignoring the fact that we could geocode the addresses), but it's difficult to assess the quality of the results since the threshold numbers are essentially guesses.  In other words we know that we'll have false positive matches and false negative misses, but we can't estimate beforehand how likely those errors will be."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Probabilistic Matching and Fellegi-Sunter"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first rigorous method for finding an optimal set of matches is the Fellegi-Sunter method, which goes back to the 1960s, although it's still in use today.  It was motivated by a need to link records from various data sources collected by the Census Bureau.  Like the heuristic method above, the goals here is to find two threshold levels, T_u and T_l, such that anything above T_u is a positive link, anything below T_l is a positive non_link, and anything between is uncertain.  However, the goal with this method is to be able to set the threshold values such that they meet known (and presumably acceptable) error rates.\n",
      "\n",
      "In general any two data sets will have some subset of attributes, say &alpha;1 to &alpha;n, on which they could match.  With n attributes there are 2^n patterns of matching.  For example, suppose that in our test data we have only business name, zip code, and street number on which to match.  The patterns are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(data=list(itertools.product(['match','unmatch'], repeat=3)),\n",
      "             columns=['name', 'zip', 'street num'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>name</th>\n",
        "      <th>zip</th>\n",
        "      <th>street num</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>match</td>\n",
        "      <td>match</td>\n",
        "      <td>match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>match</td>\n",
        "      <td>match</td>\n",
        "      <td>unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>match</td>\n",
        "      <td>unmatch</td>\n",
        "      <td>match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>match</td>\n",
        "      <td>unmatch</td>\n",
        "      <td>unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>unmatch</td>\n",
        "      <td>match</td>\n",
        "      <td>match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>unmatch</td>\n",
        "      <td>match</td>\n",
        "      <td>unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>unmatch</td>\n",
        "      <td>unmatch</td>\n",
        "      <td>match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>unmatch</td>\n",
        "      <td>unmatch</td>\n",
        "      <td>unmatch</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 117,
       "text": [
        "      name      zip street num\n",
        "0    match    match      match\n",
        "1    match    match    unmatch\n",
        "2    match  unmatch      match\n",
        "3    match  unmatch    unmatch\n",
        "4  unmatch    match      match\n",
        "5  unmatch    match    unmatch\n",
        "6  unmatch  unmatch      match\n",
        "7  unmatch  unmatch    unmatch"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can think of each pattern as a separate comparison on a record from A and a record from B.  So the vector of 8 comparison makes a function on A X B:\n",
      "\n",
      "$$\n",
      "\\gamma(r_{A}, r_{B}) = [ \\gamma_{mmm}(r_{A}, r_{B}), \\gamma_{mmu}(r_{A}, r_{B}), \\dots, \\gamma_{uuu}(r_{A}, r_{B})]\n",
      "$$\n",
      "\n",
      "If you pick records at random from A and B, you'll end up with a bunch of vectors like those below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gamma(ra, rb):\n",
      "    gg = lambda x: 1 if x else 0\n",
      "    return [gg(((ra[0] == rb[0]) == i) and ((ra[1] == rb[1]) == j) and ((ra[2] == rb[2]) == k)) \n",
      "     for i,j,k in list(itertools.product([True,False], repeat=3))]\n",
      "        \n",
      "print(gamma(('tropicana restaurant & lounge', '33012', '1950'), ('pizza hut', '33566', '2316')))\n",
      "print(gamma(('granger and sons bar-b-que', '33809', '8121'), ('body parts of america', '32534', '8121')))\n",
      "print(gamma((\"antonio's pizzeria restaurant\", '33023', '6890'), ('horizon dental care', '33023', '6890')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 0, 0, 0, 0, 0, 1]\n",
        "[0, 0, 0, 0, 0, 0, 1, 0]\n",
        "[0, 0, 0, 0, 1, 0, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most of the vector patterns will be more common for records that don't match, but some will be more common for records that do match.  You can think of &gamma; as a random variable and the following conditional probabilities can be defined:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "m(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in M] \\\\\n",
      "= \\sum_{a,b \\in M} P[\\gamma(r_{A},r_{B})]P[(r_{A}, r_{B}) \\in M]\n",
      "\\end{align}\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\begin{align}\n",
      "u(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in U] \\\\\n",
      "= \\sum_{a,b \\in U} P[\\gamma(r_{A},r_{B}]P[(r_{A}, r_{B}) \\in U]\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "The best patterns for matching will be those that have the largest ratio of m/u.  Suppose that we have two files with 100 records, giving us 100x100=10000 pairs.  Say that all 100 records in each file match.  In 20 cases, the name, zip, and street number are identical.  In 75 cases the zip and street number are the same.  However, in only 2 cases do all attributes match where the records are not a match, but there are 50 cases where the zip and street number match but the record do not match.  Of the remaining 5 matches 4 have the same zip, but 200 non-matches have the same zip."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_111 = (20/100)\n",
      "u_111 = (2/9900)\n",
      "m_011 = (75/100)\n",
      "u_011 = (50/9900)\n",
      "m_010 = (4/100)\n",
      "u_010 = (200/9900)\n",
      "m_000 = (1/100)\n",
      "u_000 = (9948/9900)\n",
      "print(m_111, u_111, m_111/u_111)\n",
      "print(m_011, u_011, m_011/u_011)\n",
      "print(m_010, u_010, m_010/u_010)\n",
      "print(m_000, u_000, m_000/u_000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.2, 0.00020202020202020202, 990.0)\n",
        "(0.75, 0.005050505050505051, 148.5)\n",
        "(0.04, 0.020202020202020204, 1.98)\n",
        "(0.01, 1.0048484848484849, 0.009951749095295536)\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can establish what's called a _linkage rule_ by setting the two values &mu; and &lambda;.  In the Fellegi-Sunter method the values are interpreted as:\n",
      "\n",
      "$$\n",
      "\\mu = P(\\mbox{positive link}|U)\n",
      "$$\n",
      "\n",
      "$$\n",
      "\\lambda = P(\\mbox{positive non-link}|M)\n",
      "$$\n",
      "\n",
      "In other words, we set &mu; to the level of false positive we're willing to tolerate, and we set &lambda; to the level of false negatives we will accept.  To relate these values to the m and u values in the Fellegi-Sunter method we first rank the m/u ratios in descending order.  For &mu;, we include patterns in the automatic match part of the rule until the sum of the u values exceeds &mu;.  For &lambda; we choose as many patterns from the end of the list \n",
      "\n",
      "For example suppose that we set &mu; = 0.01 indicating that we want at most 1% of false positives in our automatic match set.  If we include the first two of the above patterns, we'll end up with 52 matches for items that are actually in U (sum of u values = 0.0052) but if we include the third pattern we get 252 (sum of u values = 0.0252).  To get a similar level for &lambda; we could only use the final rule to establish positive non-links.  To establish an optimal linkage rule, we can set the upper and lower threshold for matches and non-matches to the m/u ratio for the patterns that meet our &mu; and &lambda; values.  The linkage rule looks like this:\n",
      "\n",
      "$$\n",
      "\t\\begin{array}{ll}\n",
      "\t\t\\frac{m}{u} > T_{\\mu}  & \\mbox{match}  \\\\\n",
      "        T_{\\mu} < \\frac{m}{u} < T_{\\lambda}  & \\mbox{potential match}  \\\\\n",
      "\t\t\\frac{m}{u} < T_{\\lambda}  & \\mbox{non-match}  \\\\\n",
      "\t\\end{array}\n",
      "$$\n",
      "\n",
      "And in this particular case, we can set the threshold values to:\n",
      "\n",
      "$$\n",
      "T_{\\mu} = m_{011}/u_{011} \\\\\n",
      "T_{\\lambda} = m_{000}/u_{000}\n",
      "$$\n",
      "\n",
      "\n",
      "In practice, _conditional independence_ will be assumed between the attributes so that we can write\n",
      "\n",
      "$$\n",
      "Pr(\\texttt{name matches}, \\texttt{zip matches}, \\texttt{street matches}|M)\n",
      "$$\n",
      "\n",
      "as the product of probabilities on the individual properties\n",
      "\n",
      "$$\n",
      "Pr(\\texttt{name matches}|M) Pr(\\texttt{zip matches}|M) Pr(\\texttt{street matches}|M)\n",
      "$$\n",
      "\n",
      "The m and u probabilities can then be written in terms of the individual properties, eg:\n",
      "\n",
      "$$\n",
      "m(\\texttt{name matches}) = \\frac{Pr(\\texttt{name matches}|M)}{Pr(\\texttt{name matches}|U)}\n",
      "$$\n",
      "\n",
      "Now the overall m value for a pair of records (a,b) can be written as:\n",
      "\n",
      "$$\n",
      "m = m(\\texttt{name matches}) * m(\\texttt{zip matches}) * m(\\texttt{street matches})\n",
      "$$\n",
      "\n",
      "The m/u ratios are usually converted to weights in the following fashion:\n",
      "\n",
      "$$\n",
      "w_{i} = \n",
      "\\left\\{\n",
      "\t\\begin{array}{ll}\n",
      "\t\tlog_{2}(\\frac{m_{i}}{u_{i}})  & \\mbox{if agreement on field i}  \\\\\n",
      "\t\tlog_{2}(\\frac{1-m_{i}}{1-u_{i}}) & \\mbox{if otherwise}\n",
      "\t\\end{array}\n",
      "\\right.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_u = [('name', 0.5, 0.0000005),\n",
      "      ('zip', 0.9, 0.001),\n",
      "      ('snum', 0.9, 0.0003)]\n",
      "df_mu = pd.DataFrame(data=m_u, columns=['Field', 'm_i', 'u_i'])\n",
      "df_mu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Field</th>\n",
        "      <th>m_i</th>\n",
        "      <th>u_i</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>name</td>\n",
        "      <td>0.5</td>\n",
        "      <td>5.000000e-07</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>zip</td>\n",
        "      <td>0.9</td>\n",
        "      <td>1.000000e-03</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>snum</td>\n",
        "      <td>0.9</td>\n",
        "      <td>3.000000e-04</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 120,
       "text": [
        "  Field  m_i           u_i\n",
        "0  name  0.5  5.000000e-07\n",
        "1   zip  0.9  1.000000e-03\n",
        "2  snum  0.9  3.000000e-04"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def field_w(a, b, m, u):\n",
      "    return math.log(m/u, 2) if a == b else math.log((1.0 - m)/(1.0 - u))\n",
      "\n",
      "def fs_weights(ra, rb, m_u):\n",
      "    return sum( field_w(ra[i], rb[i], m_u[i][1], m_u[i][2]) for i in range(len(ra)) )\n",
      "\n",
      "\n",
      "print(fs_weights(('sea siam', '33156', '12735'), ('sea siam', '33156', '12735'), m_u))\n",
      "print(fs_weights((\"antonio's pizzeria restaurant\", '33023', '6890'), ('horizon dental care', '33023', '6890'), m_u))\n",
      "print(fs_weights(('granger and sons bar-b-que', '33809', '8121'), ('body parts of america', '32534', '8121'), m_u))\n",
      "print(fs_weights(('tropicana restaurant & lounge', '33012', '1950'), ('pizza hut', '33566', '2316'), m_u))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41.2960965459\n",
        "20.671381296\n",
        "8.55601551216\n",
        "-5.29701632121\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see the weight decreases rapidly as fewer fields match in each record."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The EM Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, by now you'll be thinking that we don't actually know the m and u values because we don't know if a particular item is in the M set or the U set.  The solution to this problem that's usually used for the Fellegi-Sunter method is the _expectation maximization_, or __EM__ algorithm.  You can think of the EM algorithm as a sort of unsupervised classification method that's trying, in our case, to place each record pair in either the M class or the U class.\n",
      "\n",
      "To start we assume that the distribution of matching patterns (our gammas above) is actually a mixture of two distributions.\n",
      "\n",
      "This is similar to the approach taken in _scikit-learn_ Gaussian mixture models, except that these aren't gaussian distributions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matching.fellegi_sunter import fs_em\n",
      "\n",
      "import numpy as np\n",
      "df1 = df_fl_a[['nname', 'nzip', 'snum']].ix[np.random.choice(df_fl_a.index, 5400)]\n",
      "df2 = df_oge_a_fl[['nname', 'nzip', 'snum']].ix[np.random.choice(df_oge_a_fl.index, 1700)]\n",
      "df1['k'] = 1\n",
      "df2['k'] = 1\n",
      "record_pairs = []\n",
      "\n",
      "for idx, r in df1.merge(df2, on='k').iterrows():\n",
      "#for idx, r in dfz.iterrows():\n",
      "    record_pairs.append(((r.nname_x, r.nzip_x, r.snum_x), (r.nname_y, r.nzip_y, r.snum_y)))\n",
      "    #record_pairs.append(((r.nname_x, r.nzip, r.snum_x), (r.nname_y, r.nzip, r.snum_y)))\n",
      "\n",
      "mh, uh, ph = fs_em(record_pairs, df_mu.m_i.values, df_mu.u_i.values, 1e-6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-122-a40d4d409ef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#for idx, r in dfz.iterrows():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mrecord_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnname_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnzip_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnum_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnname_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnzip_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnum_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m#record_pairs.append(((r.nname_x, r.nzip, r.snum_x), (r.nname_y, r.nzip, r.snum_y)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/mikem/Envs/matching/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2148\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n",
        "\u001b[1;32m/home/mikem/Envs/matching/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_info_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_info_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matching.fellegi_sunter import gamma_pattern, expectation_step\n",
      "g = []\n",
      "for r_ab in record_pairs:\n",
      "    g.append(gamma_pattern(r_ab))\n",
      "    \n",
      "gm, gu = expectation_step(record_pairs, df_mu.m_i.values, df_mu.u_i.values, 1e-6, g)\n",
      "max(gm),min(gm)\n",
      "dfg=pd.DataFrame(data=g)\n",
      "dfg['gm'] = gm\n",
      "dfg['gu'] = gu\n",
      "dfg.columns = ['m1','m2','m3','gm','gu']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "for k,g in itertools.groupby(sorted(g, reverse=True)):\n",
      "    print(k, len(list(g)))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have a dataset where some matches and non-matches have already been determined, you can use a standard classifier to develop a model for matching.  This sounds unlikely, but since matching in a commercial setting often involves a team to review matches manually it does happen.\n",
      "\n",
      "This will usually be a two-class problem (match and non-match) since we assume they've been reviewed and uncertain items have been resolved.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unsupervised and Active Learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dedupe\n",
      "\n",
      "data_d = {}\n",
      "df_oge_a_fl[['nname','nzip','snum']]\n",
      "for idx, t in df_oge_a_fl[['nname','nzip','snum']].iterrows():\n",
      "    data_d[idx] = t.to_dict()\n",
      "\n",
      "fields = [\n",
      "        {'field' : 'nname', 'type': 'String'},\n",
      "        {'field' : 'nzip', 'type': 'Exact'},\n",
      "        {'field' : 'snum', 'type': 'Exact'},\n",
      "        ]\n",
      "\n",
      "deduper = dedupe.Dedupe(fields)\n",
      "\n",
      "deduper.sample(data_d, 5000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pair = deduper.uncertainPairs()[0]\n",
      "pd.DataFrame(data=[pair[0].values() + pair[1].values()], columns=['Num1','Name1','Zip1', 'Num2', 'Name2', 'Zip2'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deduper.markPairs( {'match':[\n",
      "                                ({'snum': '1620', 'nname': 'berry holding group - hp', 'nzip': '33527'},\n",
      "                                 {'snum': '1620', 'nname': 'berry holding group - hp', 'nzip': '33527'}),\n",
      "                                ({'snum': '936', 'nname': 'st george & the dragon restaurant', 'nzip': '34102'},\n",
      "                                 {'snum': '936', 'nname': 'st george & the dragon restaurant', 'nzip': '34102'}),\n",
      "                                ({'snum': '11801', 'nname': 'jnj growers inc', 'nzip': '33187'},\n",
      "                                 {'snum': '19801', 'nname': 'j n j growers inc', 'nzip': '33187'}))], \n",
      "                    'distinct':[\n",
      "                                ({'snum': 'P.O.', 'nname': 'cruz juan flc', 'nzip': '33862'}, \n",
      "                                 {'snum': '1510', 'nname': 'senior bridge', 'nzip': '32901'}),\n",
      "                                ({'snum': '171', 'nname': 'raul anzualda flc', 'nzip': '32131'}, \n",
      "                                 {'snum': '4461', 'nname': 'bee ridge florist', 'nzip': '34233'}),\n",
      "                                ({'snum': '4332', 'nname': 'advans enterprises', 'nzip': '33317'},\n",
      "                                 {'snum': '19501', 'nname': 'all day enterprises', 'nzip': '33917'}),\n",
      "                                ({'snum': '1920', 'nname': 'suggars restaurant and lounge', 'nzip': '33907'},\n",
      "                                 {'snum': '529', 'nname': 'flavors restaurant and lounge', 'nzip': '34983'}),]\n",
      "                    })"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "deduper.train()\n",
      "dupes = deduper.match(data_d)\n",
      "dupes[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Blocking"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [String Metric - Wikipedia](https://en.wikipedia.org/wiki/String_metric)\n",
      "- [\"On the Resemblance and Containment of Documents\" - Andrei Broder](http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf)\n",
      "- [_Entity Resolution and Information Quality_ - John R. Talburt](http://www.amazon.com/Entity-Resolution-Information-Quality-Talburt/dp/0123819725)\n",
      "- Christen, P., & SpringerLink (Online service). (2012). Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection. Berlin ; New York: Springer.\n",
      "- [Data Quality and Record Linkage Techniques](http://www.amazon.com/gp/product/B0016PZT7M)\n",
      "Thomas N. Herzog, Fritz J. Scheuren, William E. Winkler\n",
      "-  Fellegi, Ivan; Sunter, Alan (December 1969). \"A Theory for Record Linkage\" (PDF). Journal of the American Statistical Association 64 (328): pp. 1183\u20131210. doi:10.2307/2286061. JSTOR 2286061.\n",
      "- Gregg, Forest and Derek Eder. 2015. Dedupe. https://github.com/datamade/dedupe.\n",
      "- [Record Linkage in F# \u2013 Token Matching, Stable Marriages and the Gale-Shapley algorithm](http://richardminerich.com/2011/09/record-linkage-in-f-token-matching-stable-marriages-and-the-gale-shapley-algorithm/)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfz_nnm[(dfz_nnm.score>0.9) & (dfz_nnm.nname_x.str.contains('mexican rest'))][['nname_x', \n",
      "                       'nname_y',\n",
      "                       'score']].sort('score',ascending=True).values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import jellyfish\n",
      "scores = []\n",
      "from matching.similarity import term_counts, soft_idf\n",
      "c= term_counts(list(df_fl_a.nname) + list(df_oge_a_fl.nname))\n",
      "\n",
      "n1 = df_fl_a[df_fl_a.nname.str.contains('mexican')].nname.values\n",
      "n2 = df_oge_a_fl[df_oge_a_fl.nname.str.contains('mexican')].nname.values\n",
      "for name1 in n1:\n",
      "    for name2 in n2:\n",
      "        scores.append((name1, name2,\n",
      "                       soft_idf(unicode(name1, 'unicode-escape'), unicode(name2, 'unicode-escape'), \n",
      "                                c, sim_func=jellyfish.jaro_winkler),\n",
      "                       jellyfish.jaro_winkler(unicode(name1, 'unicode-escape'),\n",
      "                                              unicode(name2, 'unicode-escape'))))\n",
      "\n",
      "dfmx = pd.DataFrame(data=scores, columns=['n1','n2','sidf','jr']).sort('jr',ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}