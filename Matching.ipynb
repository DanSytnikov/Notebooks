{
 "metadata": {
  "name": "",
  "signature": "sha256:b394259a51435e5240a26b96c51cc82fb80cc5b0a034c21ee514f36d9062b82a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Match _All_ The Things: Entity Resolution in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you work in the data business, you'll eventually find yourself trying to make an approximate match from one data source to another.  This comes in at least three flavors: _searching_, where a user-supplied name is matched against a database (for example, auto-complete); _joining_ or [_record linkage_](http://en.wikipedia.org/wiki/Record_linkage), where you're trying to join datasets like you would with a relational join except based on an approximate criteria; and _clustering_, where you're trying to group items based on similarity.\n",
      "\n",
      "Of course these tasks are often performed with a SQL database or some other information retrieval system (eg, Solr). But there are situations where we need something slightly different than the normal database management system, or we just want something simpler that doesn't require a separate deployment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Table of Contents\n",
      "\n",
      "1. The Matching Problem\n",
      "\n",
      "1. [Basic Python](#Basic-Python)\n",
      "2. [String Similarity and Distance](#String-Similarity-and-Distance)\n",
      "  1. Levenshtein\n",
      "  1. Jaro/Jaro-Winkler\n",
      "  1. Jaccard\n",
      "  2. Metrics\n",
      "4. Token-Based Matching\n",
      "  1. [N-Grams/Shingling](#N-Grams/Shingling)\n",
      "  1. Bag of Words\n",
      "  1. TF/IDF\n",
      "  2. Gayle-Shapley\n",
      "5. Locality-Sensitive Hashing\n",
      "  1. Minhash\n",
      "6. Semantic Matching\n",
      "  1. Human Names\n",
      "6. Nearest-Neighbor\n",
      "  1. kd-Trees\n",
      "6. Pairwise alignment/Needleman-Wunsch\n",
      "6. Entropy, Mutual Information, Kulbach-Liebler\n",
      "6. Radix Tree/Patricia Trie\n",
      "6. Entity Recognition/CRF\n",
      "6. References"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Matching Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have two sets of data items, A and B, each with no duplicates then \n",
      "\n",
      "$$\n",
      "M=\\{ (a,b)| a \\in A, b \\in B, a = b \\}\n",
      "$$\n",
      "\n",
      "$$\n",
      "U=\\{ (a,b)| a \\in A, b \\in B, a \\ne b \\}\n",
      "$$\n",
      "\n",
      "By definition the union of the two sets M and U is the set of all pairs A and B, or:\n",
      "\n",
      "$$\n",
      "A \\times B = M \\cup U\n",
      "$$\n",
      "\n",
      "Some notes:\n",
      "- The \"=\" is supposed to indicate a true match, but with real-life data sources even agreement on every attribute value might not be a true match due to data errors.\n",
      "- The vast majority of pairs in A x B will be in U.  If A and B both have 1000 members, A x B will have 1 million pairs, but M can have at most 1000 elements.\n",
      "- If there is only one dataset (D) but it is assumed to have duplicates, then you'd still potentially have to check all pairs in D x D\n",
      "\n",
      "\n",
      "As an example to use throughout this document, i'm using the the two datasets from [this](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/) article on the enigma.io blog ([On Wages and Hygiene: Surfacing Bad Management in Public Data](http://blog.enigma.io/bad-management-radiates-a-look-at-hygiene-and-wages/).  In the article, they found matches using business names and geo-coded addresses, but for now i'm going to retain the individual address components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Florida restaurant inspection data.  Drop everything but unique name and addresses\n",
      "df_fl = pd.read_csv('./data/fl.restaurant-inspections.csv')\n",
      "df_fl_a = df_fl[['dba', 'location_address', 'location_city', 'location_zip_code']].drop_duplicates()\n",
      "\n",
      "# Wage data, drop everything but unique name and addresses, and filter to only Florida.\n",
      "df_oge = pd.read_csv('./data/ogesdw.whd.whisard.csv')\n",
      "df_oge_a_fl = df_oge[df_oge.st_cd=='FL'][['trade_nm', 'legal_name', 'street_addr_1_txt', 'cty_nm', 'st_cd', 'zip_cd']].drop_duplicates()\n",
      "\n",
      "print(len(df_fl_a), len(df_oge_a_fl))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(53576, 16655)\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This leaves us with two sets of about 54k and 17k rows each.  The size of A x B would be almost 900 million.\n",
      "\n",
      "You might think that we can pick off some low-hanging fruit by matching on identical names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['nname'] = df_fl_a.dba.apply(lambda x:x.lower() if pd.notnull(x) else '')\n",
      "df_oge_a_fl['nname'] = df_oge_a_fl.trade_nm.apply(lambda x:x.lower() if pd.notnull(x) else '')\n",
      "dfm = df_oge_a_fl.merge(df_fl_a, on='nname')\n",
      "dfm.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>trade_nm</th>\n",
        "      <th>legal_name</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "      <th>cty_nm</th>\n",
        "      <th>st_cd</th>\n",
        "      <th>zip_cd</th>\n",
        "      <th>nname</th>\n",
        "      <th>dba</th>\n",
        "      <th>location_address</th>\n",
        "      <th>location_city</th>\n",
        "      <th>location_zip_code</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>     Embarq</td>\n",
        "      <td> Embarq Management Company</td>\n",
        "      <td> 555 Lake Border Drive</td>\n",
        "      <td>      Apopka</td>\n",
        "      <td> FL</td>\n",
        "      <td> 32703</td>\n",
        "      <td>     embarq</td>\n",
        "      <td>     EMBARQ</td>\n",
        "      <td>       425 3 ST</td>\n",
        "      <td> LEESBURG</td>\n",
        "      <td>     34748</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>     Embarq</td>\n",
        "      <td> Embarq Management Company</td>\n",
        "      <td> 555 Lake Border Drive</td>\n",
        "      <td>      Apopka</td>\n",
        "      <td> FL</td>\n",
        "      <td> 32703</td>\n",
        "      <td>     embarq</td>\n",
        "      <td>     EMBARQ</td>\n",
        "      <td> 425 3RD STREET</td>\n",
        "      <td> LEESBURG</td>\n",
        "      <td>     34748</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> McDonald's</td>\n",
        "      <td>    Paris Enterprises, LLC</td>\n",
        "      <td>      9260 NW 39th Ave</td>\n",
        "      <td> Gainesville</td>\n",
        "      <td> FL</td>\n",
        "      <td> 32605</td>\n",
        "      <td> mcdonald's</td>\n",
        "      <td> MCDONALD'S</td>\n",
        "      <td> 15096 NW 7 AVE</td>\n",
        "      <td>    MIAMI</td>\n",
        "      <td>     33168</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> McDonald's</td>\n",
        "      <td>    Paris Enterprises, LLC</td>\n",
        "      <td>      9260 NW 39th Ave</td>\n",
        "      <td> Gainesville</td>\n",
        "      <td> FL</td>\n",
        "      <td> 32605</td>\n",
        "      <td> mcdonald's</td>\n",
        "      <td> MCDONALD'S</td>\n",
        "      <td> 5700 NW 79 AVE</td>\n",
        "      <td>    MIAMI</td>\n",
        "      <td> 331663534</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> McDonald's</td>\n",
        "      <td>    Paris Enterprises, LLC</td>\n",
        "      <td>      9260 NW 39th Ave</td>\n",
        "      <td> Gainesville</td>\n",
        "      <td> FL</td>\n",
        "      <td> 32605</td>\n",
        "      <td> mcdonald's</td>\n",
        "      <td> MCDONALD'S</td>\n",
        "      <td> 2300 SW 87 AVE</td>\n",
        "      <td>    MIAMI</td>\n",
        "      <td> 331652011</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 146,
       "text": [
        "     trade_nm                 legal_name      street_addr_1_txt       cty_nm  \\\n",
        "0      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
        "1      Embarq  Embarq Management Company  555 Lake Border Drive       Apopka   \n",
        "2  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "3  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "4  McDonald's     Paris Enterprises, LLC       9260 NW 39th Ave  Gainesville   \n",
        "\n",
        "  st_cd  zip_cd       nname         dba location_address location_city  \\\n",
        "0    FL   32703      embarq      EMBARQ         425 3 ST      LEESBURG   \n",
        "1    FL   32703      embarq      EMBARQ   425 3RD STREET      LEESBURG   \n",
        "2    FL   32605  mcdonald's  MCDONALD'S   15096 NW 7 AVE         MIAMI   \n",
        "3    FL   32605  mcdonald's  MCDONALD'S   5700 NW 79 AVE         MIAMI   \n",
        "4    FL   32605  mcdonald's  MCDONALD'S   2300 SW 87 AVE         MIAMI   \n",
        "\n",
        "  location_zip_code  \n",
        "0             34748  \n",
        "1             34748  \n",
        "2             33168  \n",
        "3         331663534  \n",
        "4         331652011  "
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see though that there are a lot of duplicate names.  For example, every McDonald's franchise has the same name so the cross product has a lot of mismatches.  So how about if we add some other attribute to the merge.  With a bit of \"profiling\", we can see that we should have at least 5 common digits on the zip code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "t = string.maketrans('0123456789', '9999999999')\n",
      "df_fl_a['pzip'] =  df_fl_a.location_zip_code.apply(lambda x: str(x).translate(t) if pd.notnull(x) else x)\n",
      "#df_oge_a_fl.zip_cd.groupby(by=lambda x: str(x).translate(t) if pd.notnull(x) else x).agg(np.min)\n",
      "df_fl_a.groupby('pzip').count()['pzip']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 147,
       "text": [
        "pzip\n",
        "99999         47778\n",
        "99999-9999     1611\n",
        "999999999      4179\n",
        "Name: pzip, dtype: int64"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['nzip'] = df_fl_a.location_zip_code.apply(lambda x:x[0:5] if pd.notnull(x) else x)\n",
      "df_oge_a_fl['nzip'] = df_oge_a_fl.zip_cd.apply(lambda x: str(int(x)))\n",
      "dfmz = df_fl_a.merge(df_oge_a_fl, on=['nname', 'nzip'])\n",
      "dfmz[['nname','nzip', 'location_address', 'street_addr_1_txt']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname</th>\n",
        "      <th>nzip</th>\n",
        "      <th>location_address</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>        jimmy's place</td>\n",
        "      <td> 33161</td>\n",
        "      <td>              510 NE 125 ST</td>\n",
        "      <td>           510 NE 125 St.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>    pascal's on ponce</td>\n",
        "      <td> 33134</td>\n",
        "      <td>    2611 PONCE DE LEON BLVD</td>\n",
        "      <td> 2611 Ponce De Leon Blvd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>      palomilla grill</td>\n",
        "      <td> 33144</td>\n",
        "      <td>          6890 W FLAGLER ST</td>\n",
        "      <td>  6890  W. Flagler Street</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>    meson ria de vigo</td>\n",
        "      <td> 33145</td>\n",
        "      <td>        1363 - 65 CORAL WAY</td>\n",
        "      <td>         1363 SW 22nd St.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> sf catering n events</td>\n",
        "      <td> 33016</td>\n",
        "      <td> 9605 NW 79 AVE BAY 17 &amp; 18</td>\n",
        "      <td>  9605 NW 79 Avenue 17/18</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 148,
       "text": [
        "                  nname   nzip            location_address         street_addr_1_txt\n",
        "0         jimmy's place  33161               510 NE 125 ST            510 NE 125 St.\n",
        "1     pascal's on ponce  33134     2611 PONCE DE LEON BLVD  2611 Ponce De Leon Blvd.\n",
        "2       palomilla grill  33144           6890 W FLAGLER ST   6890  W. Flagler Street\n",
        "3     meson ria de vigo  33145         1363 - 65 CORAL WAY          1363 SW 22nd St.\n",
        "4  sf catering n events  33016  9605 NW 79 AVE BAY 17 & 18   9605 NW 79 Avenue 17/18"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That looks a bit better at first, but it's still easy to find incorrect and duplicate matches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfmz.groupby(['nname','nzip']).filter(lambda x: len(x['cty_nm']) > 1)[['nname',\n",
      "                                                                      'nzip', \n",
      "                                                                      'location_address', \n",
      "                                                                      'street_addr_1_txt']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname</th>\n",
        "      <th>nzip</th>\n",
        "      <th>location_address</th>\n",
        "      <th>street_addr_1_txt</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>525</th>\n",
        "      <td>                bayfront bistro</td>\n",
        "      <td> 33931</td>\n",
        "      <td>            4765 ESTERO BLVD</td>\n",
        "      <td>                  4761 Estero Blvd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>526</th>\n",
        "      <td>                bayfront bistro</td>\n",
        "      <td> 33931</td>\n",
        "      <td>            4765 ESTERO BLVD</td>\n",
        "      <td>                  4761 Estero Blvd.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>420</th>\n",
        "      <td>                     bento cafe</td>\n",
        "      <td> 32246</td>\n",
        "      <td> 4860 BIG ISLAND DR, SUITE 1</td>\n",
        "      <td>                4860 Big Island Dr.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>421</th>\n",
        "      <td>                     bento cafe</td>\n",
        "      <td> 32246</td>\n",
        "      <td> 9734 DEER LAKE COURT STE 11</td>\n",
        "      <td>                4860 Big Island Dr.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>193</th>\n",
        "      <td> buffalo wild wings grill &amp; bar</td>\n",
        "      <td> 33511</td>\n",
        "      <td>            2055 BADLANDS DR</td>\n",
        "      <td>                   2055 Badlands Dr</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>194</th>\n",
        "      <td> buffalo wild wings grill &amp; bar</td>\n",
        "      <td> 33511</td>\n",
        "      <td>            2055 BADLANDS DR</td>\n",
        "      <td>                2055 Badlands Drive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42 </th>\n",
        "      <td>          charcoals latin grill</td>\n",
        "      <td> 33172</td>\n",
        "      <td>        11401 NW 12 ST #E510</td>\n",
        "      <td>            11401 NW 12 Street #E10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43 </th>\n",
        "      <td>          charcoals latin grill</td>\n",
        "      <td> 33172</td>\n",
        "      <td>        11401 NW 12 ST #E510</td>\n",
        "      <td>           11401 NW 12th St. # E510</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>163</th>\n",
        "      <td>                    chart house</td>\n",
        "      <td> 33607</td>\n",
        "      <td>      7616 COURTNEY CAMPBELL</td>\n",
        "      <td>           7616 W Courtney Campbell</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>164</th>\n",
        "      <td>                    chart house</td>\n",
        "      <td> 33607</td>\n",
        "      <td>      7616 COURTNEY CAMPBELL</td>\n",
        "      <td> 7616 W. Courtney Campbell Causeway</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 149,
       "text": [
        "                              nname   nzip             location_address                   street_addr_1_txt\n",
        "525                 bayfront bistro  33931             4765 ESTERO BLVD                   4761 Estero Blvd.\n",
        "526                 bayfront bistro  33931             4765 ESTERO BLVD                   4761 Estero Blvd.\n",
        "420                      bento cafe  32246  4860 BIG ISLAND DR, SUITE 1                 4860 Big Island Dr.\n",
        "421                      bento cafe  32246  9734 DEER LAKE COURT STE 11                 4860 Big Island Dr.\n",
        "193  buffalo wild wings grill & bar  33511             2055 BADLANDS DR                    2055 Badlands Dr\n",
        "194  buffalo wild wings grill & bar  33511             2055 BADLANDS DR                 2055 Badlands Drive\n",
        "42            charcoals latin grill  33172         11401 NW 12 ST #E510             11401 NW 12 Street #E10\n",
        "43            charcoals latin grill  33172         11401 NW 12 ST #E510            11401 NW 12th St. # E510\n",
        "163                     chart house  33607       7616 COURTNEY CAMPBELL            7616 W Courtney Campbell\n",
        "164                     chart house  33607       7616 COURTNEY CAMPBELL  7616 W. Courtney Campbell Causeway"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One potential source of duplicate matches is that the original data sets have duplicates.  Again, that's pretty easy to find with some aggregation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a.groupby(['nname','nzip']).filter(lambda x: len(x['dba']) > 2).head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>dba</th>\n",
        "      <th>location_address</th>\n",
        "      <th>location_city</th>\n",
        "      <th>location_zip_code</th>\n",
        "      <th>nname</th>\n",
        "      <th>pzip</th>\n",
        "      <th>nzip</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>87993</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 3111 S ATLANTIC AVE</td>\n",
        "      <td> DAYTONA BEACH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87994</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2225 S ATLANTIC AVE</td>\n",
        "      <td>   DAYTONA BCH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87996</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2411 S ATLANTIC AVE</td>\n",
        "      <td>   DAYTONA BCH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87997</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 3125 S ATLANTIC AVE</td>\n",
        "      <td>   DAYTONA BCH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87999</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td>  400 N ATLANTIC AVE</td>\n",
        "      <td>          DAYTONA BCH</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88045</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2209 S ATLANTIC AVE</td>\n",
        "      <td>   DAYTONA BCH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88061</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2125 S ATLANTIC AVE</td>\n",
        "      <td>          DAYTONA BCH</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88291</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2209 S ATLANTIC AVE</td>\n",
        "      <td> DAYTONA BEACH SHORES</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88474</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 1615 S ATLANTIC AVE</td>\n",
        "      <td>        DAYTONA BEACH</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>88493</th>\n",
        "      <td> ATLANTIC VENDORS INC</td>\n",
        "      <td> 2500 N ATLANTIC AVE</td>\n",
        "      <td>        DAYTONA BEACH</td>\n",
        "      <td> 32118</td>\n",
        "      <td> atlantic vendors inc</td>\n",
        "      <td> 99999</td>\n",
        "      <td> 32118</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "                        dba     location_address         location_city  \\\n",
        "87993  ATLANTIC VENDORS INC  3111 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
        "87994  ATLANTIC VENDORS INC  2225 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87996  ATLANTIC VENDORS INC  2411 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87997  ATLANTIC VENDORS INC  3125 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "87999  ATLANTIC VENDORS INC   400 N ATLANTIC AVE           DAYTONA BCH   \n",
        "88045  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE    DAYTONA BCH SHORES   \n",
        "88061  ATLANTIC VENDORS INC  2125 S ATLANTIC AVE           DAYTONA BCH   \n",
        "88291  ATLANTIC VENDORS INC  2209 S ATLANTIC AVE  DAYTONA BEACH SHORES   \n",
        "88474  ATLANTIC VENDORS INC  1615 S ATLANTIC AVE         DAYTONA BEACH   \n",
        "88493  ATLANTIC VENDORS INC  2500 N ATLANTIC AVE         DAYTONA BEACH   \n",
        "\n",
        "      location_zip_code                 nname   pzip   nzip  \n",
        "87993             32118  atlantic vendors inc  99999  32118  \n",
        "87994             32118  atlantic vendors inc  99999  32118  \n",
        "87996             32118  atlantic vendors inc  99999  32118  \n",
        "87997             32118  atlantic vendors inc  99999  32118  \n",
        "87999             32118  atlantic vendors inc  99999  32118  \n",
        "88045             32118  atlantic vendors inc  99999  32118  \n",
        "88061             32118  atlantic vendors inc  99999  32118  \n",
        "88291             32118  atlantic vendors inc  99999  32118  \n",
        "88474             32118  atlantic vendors inc  99999  32118  \n",
        "88493             32118  atlantic vendors inc  99999  32118  "
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is an interesting case because there's one pair that's a clear duplicate (at 2209 S Atlantic), and another triple that all look suspiciously close (2225 S, 2125 S, and 3125 S).\n",
      "\n",
      "Aspects of data matching:\n",
      "\n",
      "- Need to preprocess fields (lowerize, normalize, convert types, geocode, split, combine, etc.)\n",
      "- Might need to dedupe before matching.\n",
      "- Different data sources will have variations of name/address, necessitating approximate matching.\n",
      "- Need to assume there will be cases where attributes match but the entities are not a true match.\n",
      "- Need to assume that identifiers (street numbers, zip codes, etc.) will be *wrong* in some percentage of cases (ie, there are true matches where attributes are not equal)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Basic Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, don't forget that you can do many string matching operations with basic Python functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string1 = u'Acme Traffic Signal Company'\n",
      "string2 = u'Acme Traffic Signal Co.'\n",
      "string3 = u'ACME TRAFFIC SIGNAL COMPANY'\n",
      "string4 = u'Acme Rocket Powered Products'\n",
      "strings = [string1, string2, string3, string4]\n",
      "\n",
      "print(string1 == string2)\n",
      "print(string1[:20] == string2[:20])\n",
      "print(string1.lower() == string3.lower())\n",
      "print(string2.find('Signal'))\n",
      "print(string2.index('Signal'))\n",
      "print [s for s in strings if s.startswith('Acme')]\n",
      "print [s for s in strings if 'traffic' in s.lower()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n",
        "True\n",
        "True\n",
        "13\n",
        "13\n",
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.', u'Acme Rocket Powered Products']\n",
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.', u'ACME TRAFFIC SIGNAL COMPANY']\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also do \"wildcard\" matches using the Python regular expression module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "[s for s in strings if re.search('Acme.*Signal', s)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "[u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.']"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "String Similarity and Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another under-appreciated Python module is difflib.  Although its primary use is generating diff-like comparisons of documents, it also has some useful and interesting matching functionality.  The _get_close_matches_ function is a quick way to find near matches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "\n",
      "print(difflib.get_close_matches('Acme Rocket-Powered Inc.', strings))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Acme Rocket Powered Products']\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the difflib SequenceMatcher can generate a score that indicates how well two sequences match (note that the sequences don't necessarily have to be strings).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sm = difflib.SequenceMatcher()\n",
      "sm.set_seq1('Acme Rocket-Powered Inc.')\n",
      "for s in strings:\n",
      "    sm.set_seq2(s)\n",
      "    print(s, sm.ratio())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', 0.27450980392156865)\n",
        "(u'Acme Traffic Signal Co.', 0.2978723404255319)\n",
        "(u'ACME TRAFFIC SIGNAL COMPANY', 0.1568627450980392)\n",
        "(u'Acme Rocket Powered Products', 0.7692307692307693)\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are in fact a huge number of different metrics for measuring the similarity of, or distance between, two text strings.  This [Wikipedia article](http://en.wikipedia.org/wiki/String_metric) lists no fewer than 18.  Probably the best know is the Levenshtein distance, sometimes just called the edit distance, which is simply a count of the 1-character edits you'd have to make to turn one string into another.\n",
      "\n",
      "Finding similarity between strings is a subset of the field of natural language processing, so not surprisingly there are many string metrics in the well-known Python [Natural Language Toolkit (NLTK)](http://www.nltk.org/_modules/nltk/metrics/distance.html).  The jellyfish package in another Python library that has numerous string metrics.  Here we will focus on two metrics, the Levenshtein distance and the Jaro distance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import jellyfish\n",
      "\n",
      "print(string1, string2)\n",
      "print(jellyfish.levenshtein_distance(string1, string2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.')\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Levenshtein Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Levenshtein distance is simply a count of the one-character edits that would have to be made to convert one string into another.  Many theorize that he was inspired to invent this metric by the fact that everybody mispelled his name.  As you can see above this means that the value of the Levenshtein distance is a non-negative integer.  There are variants of this metric that allow simple transpositions to be ignored."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jaro and Jaro-Winkler Similarity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(string1, string2)\n",
      "print jellyfish.jaro_distance(string1, string2)\n",
      "print jellyfish.jaro_winkler(string1, string2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'Acme Traffic Signal Company', u'Acme Traffic Signal Co.')\n",
        "0.923778851315\n",
        "0.954267310789\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Jaro and Jaro-Winkler similarity metrics were developed specifically for record-linking applications.  Both are described well on the [Jaro Distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) Wikipedia page.  The Jaro-Winkler version gives more emphasis to strings that have an initial matching prefix, which is routinely the case when matching human or business names.  Empirically these metrics seem to give the best results for basic string-level matching."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jaccard Similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jaccard similarity is simply defined as the length of the intersection of two sets divided by the length of the union of the two sets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "def jaccard_similarity(a, b):\n",
      "    x = set(a)\n",
      "    y = set(b)\n",
      "    return len(x & y) / len(x | y)\n",
      "\n",
      "jaccard_similarity(string1, string2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "0.8421052631578947"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The definition of the Jaccard similarity gives it interesting properties that aren't necessarily true of the string similarity measures we seen so far.  First, the _order_ of the characters in the two strings does not matter since they are treated as sets, and second extra copies of the same elements in one of the sets have no effect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(jaccard_similarity('abcdef', 'abcdef'), \n",
      "      jaccard_similarity('abcdef', 'cbfaed'), \n",
      "      jaccard_similarity('abcdef', 'cbfaedcbfaed') )\n",
      "print(jellyfish.jaro_distance(u'abcdef', u'abcdef'), \n",
      "      jellyfish.jaro_distance(u'abcdef', u'cbfaed'), \n",
      "      jellyfish.jaro_distance(u'abcdef', u'cbfaedcbfaed') )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1.0, 1.0, 1.0)\n",
        "(1.0, 0.611111111111111, 0.7222222222222222)\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Third, Jaccard similarity can be easily applied to non-text data since it operates on sets.  This will turn out to be handy later:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(jaccard_similarity([2,3,4,5], [3,5]))\n",
      "print(jaccard_similarity([\"bag\",\"of\",\"words\"], [\"words\",\"bag\"]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.5\n",
        "0.666666666667\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The term _distance_ or _metric_ is sometimes used informally for any value that measures the similarity of strings, but there's a formal definition for a distance metric\n",
      "\n",
      "- d(a, a) = 0\n",
      "- d(a, b) \u2265 0\n",
      "- d(a, b) = d(b, a)\n",
      "- d(a, c) \u2264 d(a, b) + d(b, c)  #  The \"triangle inequality\n",
      "\n",
      "The Levenshtein distance does follow these rules.  However, the Jaro distance doesn't, mainly because the Jaro distance as implemented here is actually a similarity metric, that is, two identical strings have the maximum score (here 1.0) rather than 0.0.  However, even if you define a distance metric as 1.0 - Jaro, it's still not a metric.  It's a little difficult to prove that something _is_ a metric, however it's easy to find counter-examples showing that it's not:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "def check_triangle(f):\n",
      "    t0 = u'abc'\n",
      "    p = [''.join(p) for p in itertools.permutations(t0)]\n",
      "\n",
      "    for t1 in p:\n",
      "        for t2 in p:\n",
      "            if(f(t0, t2) > (f(t0, t1) + f(t1,t2))):\n",
      "                print(\"d({t0}-{t2})={d02}  d({t0}-{t1}) + d({t1}-{t2})={d01_12}\".format(t0=t0,t1=t1,t2=t2,d02=f(t0, t2), d01_12=f(t0, t1) + f(t1,t2)))\n",
      "\n",
      "check_triangle(lambda x,y : 1 - jellyfish.jaro_winkler(x, y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "d(abc-bca)=1.0  d(abc-acb) + d(acb-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-acb) + d(acb-cab)=0.888888888889\n",
        "d(abc-bca)=1.0  d(abc-bac) + d(bac-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-bac) + d(bac-cab)=0.888888888889\n",
        "d(abc-bca)=1.0  d(abc-cba) + d(cba-bca)=0.888888888889\n",
        "d(abc-cab)=1.0  d(abc-cba) + d(cba-cab)=0.888888888889\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jaccard similarity can also be turned into a proper distance metric just by subtracting it from 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard_distance(a, b): return 1 - jaccard_similarity(a, b)\n",
      "print(jaccard_distance(string1, string2))\n",
      "print(jaccard_distance('abcdef', 'cbfaed'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.157894736842\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The properties of a distance metric are primarily useful in clustering applications, where we need a consistent idea of how far apart are the items we're trying to cluster.  A true metric creates what's called a _metric space_, the most common instance of which is a Euclidean space like we're used to in geometry."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best metric to use depends on the application.  Some string similarity metrics like the edit distance are based on the idea that small differences are introduced into entity names, probably because of human error.  These metrics are good at finding similar strings with transpositions, misspellings, added or deleted characters, or abbreviations.  The Jaro distance and the Jaro-Winkler variation were developed for record linkage applications.  Something like Jaccard similarity might be better where the sequence is less important than the content or topic.\n",
      "\n",
      "Below is a case where the three metrics are applied to four strings that are in fact names for the same company (a mutual fund company taken from SEC data-- the first name is the current name and the other three are former names)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = [u'FRANKLIN LTD DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON LTD DURATION INCOME TRUST',\n",
      "         u'FRANKLIN TEMPLETON STRATEGIC INCOME TRUST ']\n",
      "\n",
      "similarities = [(n1,n2, jellyfish.levenshtein_distance(n1,n2), jellyfish.jaro_distance(n1,n2), jaccard_similarity(n1,n2))\n",
      "              for n1 in names for n2 in names]\n",
      "        \n",
      "import pandas as pd\n",
      "df = pd.DataFrame(similarities)\n",
      "df.columns = ['n1', 'n2', 'Levenshtein', 'Jaro', 'Jaccard']\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>n1</th>\n",
        "      <th>n2</th>\n",
        "      <th>Levenshtein</th>\n",
        "      <th>Jaro</th>\n",
        "      <th>Jaccard</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>  0</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.794935</td>\n",
        "      <td> 0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0.816399</td>\n",
        "      <td> 0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.765007</td>\n",
        "      <td> 0.833333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.794935</td>\n",
        "      <td> 0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>  0</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0.911616</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.845314</td>\n",
        "      <td> 0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0.816399</td>\n",
        "      <td> 0.941176</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0.911616</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>  0</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td> 10</td>\n",
        "      <td> 0.843852</td>\n",
        "      <td> 0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td>               FRANKLIN LTD DURATION INCOME TRUST</td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.765007</td>\n",
        "      <td> 0.833333</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td> FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST</td>\n",
        "      <td> 14</td>\n",
        "      <td> 0.845314</td>\n",
        "      <td> 0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td>     FRANKLIN TEMPLETON LTD DURATION INCOME TRUST</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0.843852</td>\n",
        "      <td> 0.888889</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td>       FRANKLIN TEMPLETON STRATEGIC INCOME TRUST </td>\n",
        "      <td>  0</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 162,
       "text": [
        "                                                  n1  \\\n",
        "0                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "1                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "2                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "3                 FRANKLIN LTD DURATION INCOME TRUST   \n",
        "4   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "6   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "7   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST   \n",
        "8       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "9       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "11      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST   \n",
        "12        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "13        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "14        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST    \n",
        "\n",
        "                                                  n2  Levenshtein      Jaro  \\\n",
        "0                 FRANKLIN LTD DURATION INCOME TRUST            0  1.000000   \n",
        "1   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.794935   \n",
        "2       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.816399   \n",
        "3         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.765007   \n",
        "4                 FRANKLIN LTD DURATION INCOME TRUST           14  0.794935   \n",
        "5   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            0  1.000000   \n",
        "6       FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            4  0.911616   \n",
        "7         FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            14  0.845314   \n",
        "8                 FRANKLIN LTD DURATION INCOME TRUST           10  0.816399   \n",
        "9   FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST            4  0.911616   \n",
        "10      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST            0  1.000000   \n",
        "11        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST            10  0.843852   \n",
        "12                FRANKLIN LTD DURATION INCOME TRUST           14  0.765007   \n",
        "13  FRANKLIN TEMPLETON LIMITED DURATION INCOME TRUST           14  0.845314   \n",
        "14      FRANKLIN TEMPLETON LTD DURATION INCOME TRUST           10  0.843852   \n",
        "15        FRANKLIN TEMPLETON STRATEGIC INCOME TRUST             0  1.000000   \n",
        "\n",
        "     Jaccard  \n",
        "0   1.000000  \n",
        "1   0.941176  \n",
        "2   0.941176  \n",
        "3   0.833333  \n",
        "4   0.941176  \n",
        "5   1.000000  \n",
        "6   1.000000  \n",
        "7   0.888889  \n",
        "8   0.941176  \n",
        "9   1.000000  \n",
        "10  1.000000  \n",
        "11  0.888889  \n",
        "12  0.833333  \n",
        "13  0.888889  \n",
        "14  0.888889  \n",
        "15  1.000000  "
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this application, the Jaro distance seems like the most selective.  It ranks the similarity in the most intuitive way.  Note that the Jaccard similarity sometimes gives the highest score (1.0) to strings that aren't actually identical."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Token-Based Matching"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A lot of the techniques we've looked at already could be applied to tokens (or really any sequence) but things like Levenshtein distance and Jaro distance are usually applied to strings.  Normally by _token_ we'd mean some delimited piece of a larger strings or expression, but here we're going to use a fairly loose definition of tokens here as basically any subsequence of a string.  That might mean words, but it also might various other things as we'll see."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "N-Grams/Shingling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Languages have structure in the sense that some letters are more likely to follow other letters, and some words are more likely to follow others.  The term n-gram sometimes refers to the overlapping sequences of characters in a string, and sometimes to overlapping sequences of tokens.  These are also called _shingles_ (because they overlap, get it?).  As an example, below are the character bigrams for two of the names above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# N-gram function courtesy of Peter Norvig\n",
      "def ngrams(seq, n):\n",
      "    \"List all the (overlapping) ngrams in a sequence.\"\n",
      "    return [seq[i:i+n] for i in range(1+len(seq)-n)]\n",
      "\n",
      "print(ngrams(names[0], 2))\n",
      "print(ngrams(names[1], 2))\n",
      "print(jaccard_similarity(names[0], names[1]), jaccard_similarity(ngrams(names[0], 2), ngrams(names[1], 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'FR', u'RA', u'AN', u'NK', u'KL', u'LI', u'IN', u'N ', u' L', u'LT', u'TD', u'D ', u' D', u'DU', u'UR', u'RA', u'AT', u'TI', u'IO', u'ON', u'N ', u' I', u'IN', u'NC', u'CO', u'OM', u'ME', u'E ', u' T', u'TR', u'RU', u'US', u'ST']\n",
        "[u'FR', u'RA', u'AN', u'NK', u'KL', u'LI', u'IN', u'N ', u' T', u'TE', u'EM', u'MP', u'PL', u'LE', u'ET', u'TO', u'ON', u'N ', u' L', u'LI', u'IM', u'MI', u'IT', u'TE', u'ED', u'D ', u' D', u'DU', u'UR', u'RA', u'AT', u'TI', u'IO', u'ON', u'N ', u' I', u'IN', u'NC', u'CO', u'OM', u'ME', u'E ', u' T', u'TR', u'RU', u'US', u'ST']\n",
        "(0.9411764705882353, 0.6829268292682927)\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the Jaccard similarity of the original strings is much higher than the Jaccard similarity of the bigram sets.  We can calculate a Jaccard score for the tokens in the strings as well (after we tweak the lists of bigrams slightly since our ngrams function returns unhashable list elements):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(ngrams(names[0].split(), 2))\n",
      "print(ngrams(names[1].split(), 2))\n",
      "print(jaccard_similarity((tuple(x) for x in ngrams(names[0].split(), 2)), \n",
      "                         (tuple(x) for x in ngrams(names[2].split(), 2))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'FRANKLIN', u'LTD'], [u'LTD', u'DURATION'], [u'DURATION', u'INCOME'], [u'INCOME', u'TRUST']]\n",
        "[[u'FRANKLIN', u'TEMPLETON'], [u'TEMPLETON', u'LIMITED'], [u'LIMITED', u'DURATION'], [u'DURATION', u'INCOME'], [u'INCOME', u'TRUST']]\n",
        "0.5\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Term Frequency/Inverse Document Frequency"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One basic similarity metric is _cosine similarity_, which measures how close two vectors are in an n-dimensional space by computing the cosine of the angle between the two vectors.  As you might recall from basic linear algebra, the cosine between to vectors X and Y can be computed as:\n",
      "\n",
      "$$\n",
      "cos(\\theta) = \\frac{ X \\cdot Y }{\\| X |\\ \\| Y \\|}\n",
      "$$\n",
      "\n",
      "That's cool, if you happen to have vectors.  The trick is to make a string of tokens look like a vector.  One way to do this is to analyze a whole set of items to match and calculate properties of each item with respect to the whole set (in information retrieval (IR) a token is a _term_, the item is a _document_ and the set is a _corpus_).  The most common way to do this is to calculate _term frequency_ and _inverse document frequency_, or TdIdf.\n",
      "\n",
      "The term frequency is just the count of a particular token in an item.  The inverse document frequency is a measure of how rare a word is.  Common tokens will occur in most items (documents) so their inverse frequency will be low, and vice verce.  Inverse document frequency is usually defined like this:\n",
      "\n",
      "$$\n",
      "idf(t,D) = log \\frac{|D|}{| d \\in D, t \\in d |}\n",
      "$$\n",
      "\n",
      "The log just magnifies the value of rare terms.\n",
      "\n",
      "In many entity resolution (record linkage, deduplication) cases the documents are small, so term frequencies > 1 are rare, but they do happen.  For example, lets look at the Florida restaurant names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import math\n",
      "import itertools\n",
      "from collections import Counter\n",
      "\n",
      "doc_terms = [(idx, t)for idx, name in df_fl_a.nname.iteritems() for t in re.split('\\W+', name) ]\n",
      "term_counts = Counter(doc_terms)\n",
      "print(term_counts.most_common(10))\n",
      "print(df_fl_a.ix[23315])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[((114117, 'tiki'), 3), ((12952, '03'), 3), ((13056, '03'), 3), ((12943, '01'), 3), ((12973, '05'), 3), ((12957, '07'), 3), ((25078, 'subs'), 3), ((12937, '03'), 3), ((23315, 'village'), 3), ((38177, 'boca'), 2)]\n",
        "dba                  VILLAGE GRILLE VILLAGE PUMP VILLAGE SUSHI\n",
        "location_address                                4404 EL MAR DR\n",
        "location_city                             LAUDERDALE-BY-THE-SE\n",
        "location_zip_code                                        33308\n",
        "nname                village grille village pump village sushi\n",
        "pzip                                                     99999\n",
        "nzip                                                     33308\n",
        "Name: 23315, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inverse document frequency will still be fairly interesting in the matching case, since there are many documents even though they are small"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = Counter(itertools.chain(*(re.split('\\W+',n) for n in df_fl_a.nname)))\n",
      "\n",
      "idf = lambda c,t: math.log(len(c) / c[t])\n",
      "n1 = 'south sea chinese restaurant'\n",
      "name_tokens = re.split('\\W+', n1)\n",
      "n1_idf = {t: idf(counts, t) for t in name_tokens}\n",
      "print(n1_idf)\n",
      "print(counts['sea'], counts['south'], counts['restaurant'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'chinese': 4.019514582094927, 'sea': 5.469524757600925, 'south': 4.893502934278392, 'restaurant': 1.692056224355741}\n",
        "(95, 169, 4152)\n"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now that we can make a TfIdf score for each token, we can make a vector for each name and compare them with cosine similarity.  The vector space that we're dealing with is n-dimensional, where n is the total number of terms in the set of items, so most terms in the vector would be zero (in this case we'd have n=22548).  In practice to calculate similarity you just find the set of tokens that match in each string:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dot = lambda a,b : sum(x*y for x,y in zip(a,b))\n",
      "\n",
      "n2 = 'atlantic chinese restaurant'\n",
      "n3 = 'south beach cafeteria & restaurant'\n",
      "\n",
      "w_12 = [idf(counts, 'chinese'), idf(counts, 'restaurant')]\n",
      "w_11 = [idf(counts, t) for t in name_tokens]\n",
      "w_22 = [idf(counts, t) for t in re.split('\\W+', n2)]\n",
      "\n",
      "w_13 = [idf(counts, 'south'), idf(counts, 'restaurant')]\n",
      "w_33 = [idf(counts, t) for t in re.split('\\W+', n3)]\n",
      "\n",
      "print( dot(w_12, w_12)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_22, w_22))) )\n",
      "print( dot(w_13, w_13)/(math.sqrt(dot(w_11, w_11)) * math.sqrt(dot(w_33, w_33))) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.296089000083\n",
        "0.415114890203\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that even though both strings share two tokens, (n1,n3) scores slightly higher because the token _south_ is more infrequent than _chinese_."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SoftTfIdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One issue with this approach is that only identical tokens will contribute to the numerator of the cosine similarity.  That's less of an issue with larger documents where the objective is to find documents that match some query.  Here though we're trying to find items with the greatest degree of similarity.  One method that works well is called _SoftTfIdf_, where instead of identical tokens, the numerator includes that every pair of tokens that's similar enough with respect to some string similarity metric, like Jaro-Winkler."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gayle-Shapley Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the trickier parts of the SoftTfIdf algorithm is to match up the tokens in the name you want to match to the tokens in a candidate match.  It's possible a token in the first string might be similar to two tokens in the second string.  It's also possible that token _t1_ in the first string has it's best match with token _t2_ in the candidate string, but token _t3_ in the first string _also_ has its best match with _t2_.\n",
      "\n",
      "Fortunately there's a well-known and understood algorithm for making the best set of matches.  The Gayle-Shapley algorithm, also sometimes called the _stable marriage_ algorithm, takes the preferences of each of two sets and finds the match that best meets all of the preferences.  In this case, the preferences are the similarity between tokens."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Matching Non-Text Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-Numbers (L1, L2 norm, relative differences)\n",
      "-Addresses - geocoding, lat./long.\n",
      "-NAICS codes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Matching Sets of Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have numerous tools to match approximately on attributes, so we'll return to the problem of matching items in one set of data to another set.  Remember that in principle we have to check every single pair in the set A x B, which for our test data is about 900M pairs.  Although that's not a huge problem by today's standards, it's still significant and we know that most pairs are not matches, so we seek some way to reduce the number of pairs to test.  Usually this is accomplished by some sort of _indexing_, or in the language of entity resolution _blocking_.  We will discuss blocking later, but usually it involves clustering the candidate items in some way so that each item in set A only needs to be checked against a corresponding cluster in set B.\n",
      "\n",
      "For the sake of argument, let's say that we can use our normalized zip code as an index, and we'll make a dataframe that has all of the remaining potential matches.  Once we have the zip-indexed dataframe we'll compare the names of the remaining pairs using the Jaro-Winkler similarity, after removing the cases where the names already match exactly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a['snum'] = df_fl_a.location_address.str.split(' ',1).apply(lambda x:x[0])\n",
      "df_oge_a_fl['snum'] = df_oge_a_fl.street_addr_1_txt.str.split(' ',1).apply(lambda x:x[0])\n",
      "\n",
      "dfz = df_oge_a_fl.merge(df_fl_a, on='nzip')\n",
      "dfz = dfz[dfz.nname_x != dfz.nname_y]\n",
      "dfnn = dfz[dfz.nname_x == dfz.nname_y]\n",
      "dfz['score'] = dfz[['nname_x','nname_y']].apply(lambda p: jellyfish.jaro_winkler(unicode(p[0], 'unicode-escape'),\n",
      "                                                                                   unicode(p[1], 'unicode-escape')), axis=1)\n",
      "\n",
      "dfz[dfz.score>0.9][['nname_x', \n",
      "                       'nname_y', \n",
      "                       'nzip', \n",
      "                       'snum_x',\n",
      "                       'location_city',\n",
      "                       'snum_y',\n",
      "                       'cty_nm',\n",
      "                       'score']].sort('score',ascending=True).head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>nname_x</th>\n",
        "      <th>nname_y</th>\n",
        "      <th>nzip</th>\n",
        "      <th>snum_x</th>\n",
        "      <th>location_city</th>\n",
        "      <th>snum_y</th>\n",
        "      <th>cty_nm</th>\n",
        "      <th>score</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>190603 </th>\n",
        "      <td>                la pequena columbia restaurant</td>\n",
        "      <td>                    la pequena colombia espress</td>\n",
        "      <td> 33604</td>\n",
        "      <td>    6312</td>\n",
        "      <td>          TAMPA</td>\n",
        "      <td>  6204</td>\n",
        "      <td>          Tampa</td>\n",
        "      <td> 0.900539</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>288526 </th>\n",
        "      <td> fairfield inn &amp; suites by marriott miami airp</td>\n",
        "      <td>             fairfield inn &amp; suites miami south</td>\n",
        "      <td> 33126</td>\n",
        "      <td>    4101</td>\n",
        "      <td>          MIAMI</td>\n",
        "      <td>  4101</td>\n",
        "      <td>          Miami</td>\n",
        "      <td> 0.900776</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>248225 </th>\n",
        "      <td>               ciro's speakeasy and restaurant</td>\n",
        "      <td>               ciro's speakeasy and supper club</td>\n",
        "      <td> 33606</td>\n",
        "      <td>    2109</td>\n",
        "      <td>          TAMPA</td>\n",
        "      <td>  2109</td>\n",
        "      <td>          Tampa</td>\n",
        "      <td> 0.901540</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>711593 </th>\n",
        "      <td>          hooks fish &amp; chicken (miami gardens)</td>\n",
        "      <td>                          hook's fish &amp; chicken</td>\n",
        "      <td> 33054</td>\n",
        "      <td>   15850</td>\n",
        "      <td>      OPA-LOCKA</td>\n",
        "      <td> 15850</td>\n",
        "      <td>  Miami Gardens</td>\n",
        "      <td> 0.901587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>704319 </th>\n",
        "      <td>              katherine's  island  house  cafe</td>\n",
        "      <td>                   katherine's island house inc</td>\n",
        "      <td> 34134</td>\n",
        "      <td>    3801</td>\n",
        "      <td> BONITA SPRINGS</td>\n",
        "      <td>  3801</td>\n",
        "      <td> Bonita Springs</td>\n",
        "      <td> 0.902060</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1503336</th>\n",
        "      <td>                       mcdonalds store # 15535</td>\n",
        "      <td>                    mcdonald's snack bar #15535</td>\n",
        "      <td> 32223</td>\n",
        "      <td> 10991-1</td>\n",
        "      <td>   JACKSONVILLE</td>\n",
        "      <td> 10991</td>\n",
        "      <td>   Jacksonville</td>\n",
        "      <td> 0.902061</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1300441</th>\n",
        "      <td>                                 hunter marine</td>\n",
        "      <td>                                  hunter winter</td>\n",
        "      <td> 32615</td>\n",
        "      <td>   14700</td>\n",
        "      <td>        ALACHUA</td>\n",
        "      <td> 15202</td>\n",
        "      <td>        Alachua</td>\n",
        "      <td> 0.902098</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>197086 </th>\n",
        "      <td>                mikes dockside restaurant, llc</td>\n",
        "      <td>              mike's dockside waterfront grille</td>\n",
        "      <td> 34667</td>\n",
        "      <td>   14333</td>\n",
        "      <td>         HUDSON</td>\n",
        "      <td> 14333</td>\n",
        "      <td>         Hudson</td>\n",
        "      <td> 0.902182</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>313182 </th>\n",
        "      <td>                                      subway v</td>\n",
        "      <td>                                    subway 3174</td>\n",
        "      <td> 32514</td>\n",
        "      <td>    1325</td>\n",
        "      <td>      PENSACOLA</td>\n",
        "      <td>  8102</td>\n",
        "      <td>      Pensacola</td>\n",
        "      <td> 0.902273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>755271 </th>\n",
        "      <td>               innisbrook resort and golf club</td>\n",
        "      <td> innisbrook resort &amp; golf club osprey clubhouse</td>\n",
        "      <td> 34684</td>\n",
        "      <td>   36750</td>\n",
        "      <td>    PALM HARBOR</td>\n",
        "      <td> 36750</td>\n",
        "      <td>    Palm Harbor</td>\n",
        "      <td> 0.902384</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 212,
       "text": [
        "                                               nname_x  \\\n",
        "190603                  la pequena columbia restaurant   \n",
        "288526   fairfield inn & suites by marriott miami airp   \n",
        "248225                 ciro's speakeasy and restaurant   \n",
        "711593            hooks fish & chicken (miami gardens)   \n",
        "704319                katherine's  island  house  cafe   \n",
        "1503336                        mcdonalds store # 15535   \n",
        "1300441                                  hunter marine   \n",
        "197086                  mikes dockside restaurant, llc   \n",
        "313182                                        subway v   \n",
        "755271                 innisbrook resort and golf club   \n",
        "\n",
        "                                                nname_y   nzip   snum_x  \\\n",
        "190603                      la pequena colombia espress  33604     6312   \n",
        "288526               fairfield inn & suites miami south  33126     4101   \n",
        "248225                 ciro's speakeasy and supper club  33606     2109   \n",
        "711593                            hook's fish & chicken  33054    15850   \n",
        "704319                     katherine's island house inc  34134     3801   \n",
        "1503336                     mcdonald's snack bar #15535  32223  10991-1   \n",
        "1300441                                   hunter winter  32615    14700   \n",
        "197086                mike's dockside waterfront grille  34667    14333   \n",
        "313182                                      subway 3174  32514     1325   \n",
        "755271   innisbrook resort & golf club osprey clubhouse  34684    36750   \n",
        "\n",
        "          location_city snum_y          cty_nm     score  \n",
        "190603            TAMPA   6204           Tampa  0.900539  \n",
        "288526            MIAMI   4101           Miami  0.900776  \n",
        "248225            TAMPA   2109           Tampa  0.901540  \n",
        "711593        OPA-LOCKA  15850   Miami Gardens  0.901587  \n",
        "704319   BONITA SPRINGS   3801  Bonita Springs  0.902060  \n",
        "1503336    JACKSONVILLE  10991    Jacksonville  0.902061  \n",
        "1300441         ALACHUA  15202         Alachua  0.902098  \n",
        "197086           HUDSON  14333          Hudson  0.902182  \n",
        "313182        PENSACOLA   8102       Pensacola  0.902273  \n",
        "755271      PALM HARBOR  36750     Palm Harbor  0.902384  "
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that there are many businesses that match but don't have identical names (7 out of 10 in this group of the first 10 with scores > 0.9).   There are two things we can learn from this: one is that we will miss some matches if we rely on exact matches, and the other is that you will rarely find a threshold for similarity such that everything is a match.  One of the key problems in matching is finding the most true matches while introducing the least false positives.\n",
      "\n",
      "The first method that's usually tried is to devise some heuristic score and then treat anything above some value of that score as a match, and anything below some value as a miss.  So for example, we could have a score for this data set that is [(Jaro-Winkler similarity of name) + (1 if zips match) + (1 if street number matches)].  We could say that anything > 2.9 is a match, anything < 2.0 is a non-match, and everything between 2.9 and 2.0 is uncertain (ie, might need a human to look at it).\n",
      "\n",
      "That heuristic might work adequately in this case (remember, we're ignoring the fact that we could geocode the addresses), but it's difficult to assess the quality of the results since the threshold numbers are essentially guesses.  In other words we know that we'll have false positive matches and false negative misses, but we can't estimate beforehand how likely those errors will be."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Fellegi-Sunter"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first rigorous method for finding an optimal set of matches is the Fellegi-Sunter method, which goes back to the 1960s, although it's still in use today.  It was motivated by a need to link records from various data sources collected by the Census Bureau.  Like the heuristic method above, the goals here is to find two threshold levels, T_u and T_l, such that anything above T_u is a positive link, anything below T_l is a positive non_link, and anything between is uncertain.  However, the goal with this method is to be able to set the threshold values such that they meet known (and presumably acceptable) error rates.\n",
      "\n",
      "In general any two data sets will have some subset of attributes, say &alpha;1 to &alpha;n, on which they could match.  With n attributes there are 2^n patterns of matching.  For example, suppose that in our test data we have only business name, zip code, and street number on which to match.  The patterns are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(data=list(itertools.product(['match','unmatch'], repeat=3)),\n",
      "             columns=['name', 'zip', 'street num'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>name</th>\n",
        "      <th>zip</th>\n",
        "      <th>street num</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>   match</td>\n",
        "      <td>   match</td>\n",
        "      <td>   match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>   match</td>\n",
        "      <td>   match</td>\n",
        "      <td> unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>   match</td>\n",
        "      <td> unmatch</td>\n",
        "      <td>   match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>   match</td>\n",
        "      <td> unmatch</td>\n",
        "      <td> unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> unmatch</td>\n",
        "      <td>   match</td>\n",
        "      <td>   match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> unmatch</td>\n",
        "      <td>   match</td>\n",
        "      <td> unmatch</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> unmatch</td>\n",
        "      <td> unmatch</td>\n",
        "      <td>   match</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> unmatch</td>\n",
        "      <td> unmatch</td>\n",
        "      <td> unmatch</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 213,
       "text": [
        "      name      zip street num\n",
        "0    match    match      match\n",
        "1    match    match    unmatch\n",
        "2    match  unmatch      match\n",
        "3    match  unmatch    unmatch\n",
        "4  unmatch    match      match\n",
        "5  unmatch    match    unmatch\n",
        "6  unmatch  unmatch      match\n",
        "7  unmatch  unmatch    unmatch"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can think of each pattern as a separate comparison on a record from A and a record from B.  So the vector of 8 comparison makes a function on A X B:\n",
      "\n",
      "$$\n",
      "\\gamma(r_{A}, r_{B}) = [ \\gamma_{mmm}(r_{A}, r_{B}), \\gamma_{mmu}(r_{A}, r_{B}), \\dots, \\gamma_{uuu}(r_{A}, r_{B})]\n",
      "$$\n",
      "\n",
      "If you pick records at random from A and B, you'll end up with a bunch of vectors like those below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gamma(ra, rb):\n",
      "    gg = lambda x: 1 if x else 0\n",
      "    return [gg(((ra[0] == rb[0]) == i) and ((ra[1] == rb[1]) == j) and ((ra[2] == rb[2]) == k)) \n",
      "     for i,j,k in list(itertools.product([True,False], repeat=3))]\n",
      "        \n",
      "print(gamma(('tropicana restaurant & lounge', '33012', '1950'), ('pizza hut', '33566', '2316')))\n",
      "print(gamma(('granger and sons bar-b-que', '33809', '8121'), ('body parts of america', '32534', '8121')))\n",
      "print(gamma((\"antonio's pizzeria restaurant\", '33023', '6890'), ('horizon dental care', '33023', '6890')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 0, 0, 0, 0, 0, 1]\n",
        "[0, 0, 0, 0, 0, 0, 1, 0]\n",
        "[0, 0, 0, 0, 1, 0, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 260
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most of the vector patterns will be more common for records that don't match, but some will be more common for records that do match.  You can think of &gamma; as a random variable and the following conditional probabilities can be defined:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "m(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in M] \\\\\n",
      "= \\sum_{a,b \\in M} P[\\gamma(r_{A},r_{B})]P[(r_{A}, r_{B}) \\in M]\n",
      "\\end{align}\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\begin{align}\n",
      "u(\\gamma) = P[\\gamma(r_{A}, r_{B})| (r_{A}, r_{B}) \\in U] \\\\\n",
      "= \\sum_{a,b \\in U} P[\\gamma(r_{A},r_{B}]P[(r_{A}, r_{B}) \\in U]\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "The best patterns for matching will be those that have the largest ratio of m/u.  Suppose that we have two files with 100 records, giving us 100x100=10000 pairs.  Say that all 100 records in each file match.  In 20 cases, the name, zip, and street number are identical.  In 75 cases the zip and street number are the same.  However, in only 2 cases do all attributes match where the records are not a match, but there are 50 cases where the zip and street number match but the record do not match: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_111 = (20/100)\n",
      "u_111 = (2/10000)\n",
      "m_011 = (75/100)\n",
      "u_011 = (50/10000)\n",
      "print(m_111, u_111, m_111/u_111)\n",
      "print(m_011, u_011, m_011/u_011)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.2, 0.0002, 1000.0)\n",
        "(0.75, 0.005, 150.0)\n"
       ]
      }
     ],
     "prompt_number": 269
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In practice, rather than using probabilities like Pr(name matches, zip matches, street matches) _conditional independence_ will be assumed between the attributes so that we can write Pr(name matches) * Pr(zip matches) * Pr(street matches).  The m and u probabilities can then be written in terms of the individual properties, eg:\n",
      "\n",
      "$$\n",
      "m(\\texttt{name matches}) = \\frac{Pr(\\texttt{name matches}|M)}{Pr(\\texttt{name matches}|U)}\n",
      "$$\n",
      "\n",
      "$$\n",
      "w_{i} = \n",
      "\\left\\{\n",
      "\t\\begin{array}{ll}\n",
      "\t\tlog_{2}(\\frac{m_{i}}{w_{i}})  & \\mbox{if } x \\geq 0 \\\\\n",
      "\t\tlog_{2}(\\frac{1-m}{1-u}) & \\mbox{if } x < 0\n",
      "\t\\end{array}\n",
      "\\right.\n",
      "$$\n",
      "\n",
      "\n",
      "So the m_i here means, for example, the probability that the name attribute is equal for a pair of records in A x B, given that the pair is in M.  To calculate m_i exactly we'd need to know the number of true matches, and the number of matches among the true matches where the name is equal.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supervised Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unsupervised and Active Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [String Metric - Wikipedia](https://en.wikipedia.org/wiki/String_metric)\n",
      "- [\"On the Resemblance and Containment of Documents\" - Andrei Broder](http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf)\n",
      "- [_Entity Resolution and Information Quality_ - John R. Talburt](http://www.amazon.com/Entity-Resolution-Information-Quality-Talburt/dp/0123819725)\n",
      "- Christen, P., & SpringerLink (Online service). (2012). Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection. Berlin ; New York: Springer.\n",
      "- [Data Quality and Record Linkage Techniques](http://www.amazon.com/gp/product/B0016PZT7M)\n",
      "Thomas N. Herzog, Fritz J. Scheuren, William E. Winkler\n",
      "-  Fellegi, Ivan; Sunter, Alan (December 1969). \"A Theory for Record Linkage\" (PDF). Journal of the American Statistical Association 64 (328): pp. 1183\u20131210. doi:10.2307/2286061. JSTOR 2286061.\n",
      "- Gregg, Forest and Derek Eder. 2015. Dedupe. https://github.com/datamade/dedupe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_fl_a[df_fl_a.snum=='6890'][['nname', 'nzip', 'snum']].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 258,
       "text": [
        "array([['palomilla grill', '33144', '6890'],\n",
        "       ['subway', '33014', '6890'],\n",
        "       [\"antonio's pizzeria restaurant\", '33023', '6890'],\n",
        "       [\"mcdonald's restaurant\", '33884', '6890'],\n",
        "       [\"yogi's restaurant & liquor bar\", '32920', '6890'],\n",
        "       ['burger king 6480', '32505', '6890'],\n",
        "       ['pinchers beach bar & grill', '33931', '6890']], dtype=object)"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_oge_a_fl[df_oge_a_fl.snum=='6890'][['nname', 'nzip', 'snum']].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 259,
       "text": [
        "array([['escot bus lines llc', '33771', '6890'],\n",
        "       ['palomilla grill', '33144', '6890'],\n",
        "       ['horizon dental care', '33023', '6890']], dtype=object)"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}