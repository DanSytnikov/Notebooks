{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality-Sensitive Hashing\n",
    "\n",
    "I first heard about _locality-sensitive hashing_ (LSH) while taking the Coursera class __Mining Massive Datasets__.  I found it confusing and counter-intuitive, and therefore fascinating.  The basic idea behind LSH is that the hash value of two data items can be compared to give an estimate of the similarity between the original items, but there's not much about the typical notion of hashing that suggests this is possible.\n",
    "\n",
    "We know that a hash function is a function that takes a variable length input, like maybe a text document, and turns it into a fixed-length output.  As a programmer the idea of a hash brings to mind two things: the hash functions used for hash tables, and cryptographic hashes; neither of which make you think of similarity.\n",
    "\n",
    "For example you'd expect that the hash function in Python would give you a good uniform distribution of keys, because that's want you want for a hash table.  But you don't expect that similar inputs will necessarily give you similar hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5785379458708478365, 4601426126437732335, 4519506853885264743]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list(map(lambda x: hash(x), ('john smith', 'john smyth', 'john smythe')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course with a cryptographic hash it's _essential_ that similar data not give similar hash values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f5c8c961556a008b2393f09bba35f9b3', '182da9669648b14bdbef9abb5d076076')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "(hashlib.md5('when in the course of human events'.encode('utf-8')).hexdigest(),\n",
    " hashlib.md5('when in the course of human events it'.encode('utf-8')).hexdigest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _idea_ of LSH seems to have originated in Andrei Broder's 1997 paper _On the Resemblance and Containment of Documents_, although he never used the term.  Broder was concerned with the problem of finding similar web pages.  He used a metric for document similarity defined like:\n",
    "\n",
    "$$\n",
    "r(A,B) = \\frac{|S(A,k) \\cap S(B,k)|}{|S(A,k) \\cup S(B,k)|}\n",
    "$$\n",
    "\n",
    "where S(A,k) is the set of length k _shingles_ of document A.  A _shingle_ here is a set of contigous tokens, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Happy', 'families', 'are'],\n",
       " ['families', 'are', 'all'],\n",
       " ['are', 'all', 'alike;'],\n",
       " ['all', 'alike;', 'every'],\n",
       " ['alike;', 'every', 'unhappy'],\n",
       " ['every', 'unhappy', 'family'],\n",
       " ['unhappy', 'family', 'is'],\n",
       " ['family', 'is', 'unhappy'],\n",
       " ['is', 'unhappy', 'in'],\n",
       " ['unhappy', 'in', 'its'],\n",
       " ['in', 'its', 'own'],\n",
       " ['its', 'own', 'way']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram function courtesy of Peter Norvig\n",
    "def ngrams(seq, n):\n",
    "    \"List all the (overlapping) ngrams in a sequence.\"\n",
    "    return [seq[i:i+n] for i in range(1+len(seq)-n)]\n",
    "\n",
    "doc1 = \"Happy families are all alike; every unhappy family is unhappy in its own way\"\n",
    "ngrams(doc1.split(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shingles are important in the application of comparing documents because they reduce the likelihood that two documents will seem similar just because they share many individual words.  However, in what follows you can just think of the shingles as the _things_ that make up a set, in which case the measure above is just _set similarity_ or _Jaccard similarity_.  The arguments below apply as well to individual tokens or even numbers.\n",
    "\n",
    "Let &Omega; be the set of all things (eg, shingles) in all documents.\n",
    "\n",
    "$$\n",
    "W \\subset \\Omega\n",
    "$$\n",
    "\n",
    "$$\n",
    "MIN_{s}(W) = \\mbox{the set of smallest s elements in W}\n",
    "$$\n",
    "\n",
    "Of course _smallest_ implies that &Omega; is ordered.  For numbers or tokens there's a natural way to do that, but we can generally assume that &Omega; is ordered and that this ordering determines what is smallest in W.\n",
    "\n",
    "Now suppose that we have a function &pi; that defines a permutation on &Omega;. For example, say that &Omega; is the set of lower-case letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz\n",
      "ugrcewtamqksoxndzphifybjlv\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "p = list(range(len(string.ascii_lowercase)))\n",
    "random.shuffle(p)\n",
    "\n",
    "def permute(w, p):\n",
    "    return ''.join(string.ascii_lowercase[p[ord(c) - ord('a')]] for c in w)\n",
    "\n",
    "print(string.ascii_lowercase)\n",
    "print(permute(string.ascii_lowercase, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in what follows, when i refer to a _permutation_ i mean applying the permute function (ie &pi;) to a set of items.  This might be perfectly obvious but when i first looked at LSH i was confused by this terminology.\n",
    "\n",
    "Next we will create some \"documents\" built from the items in &Omega; (in other words some characters strings).  We also make a function that measures the similarity between sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srkapmhfilye skopadmbhifrel ckojatbhrew\n",
      "hpkudoawmsle hknducogamwpes rknquigapeb\n",
      "0.7333333333333333 0.7333333333333333 0.7333333333333333\n",
      "0.2777777777777778 0.2777777777777778 0.2777777777777778\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    x = set(a)\n",
    "    y = set(b)\n",
    "    return len(x & y) / len(x | y)\n",
    "\n",
    "d1 = 'allhappyfamiliesarealike'\n",
    "d2 = 'rarehippofamiliesridebikes'\n",
    "d3 = 'bewarethejabberwock'\n",
    "s1 = list(set(d1))\n",
    "s2 = list(set(d2))\n",
    "s3 = list(set(d3))\n",
    "print(''.join(s1),''.join(s2),''.join(s3))\n",
    "print (permute(s1, p), permute(s2, p), permute(s3,p))\n",
    "print (jaccard_similarity(d1, d2), jaccard_similarity(s1, s2) ,jaccard_similarity(permute(s1, p), permute(s2, p)))\n",
    "print (jaccard_similarity(d1, d3), jaccard_similarity(s1, s3) ,jaccard_similarity(permute(s1, p), permute(s3, p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have three documents (d1,d2,d2), and we first reduce these to sets of unique elements (s1,s2,s2) for simplicity (Broder's technique also works with multisets).  Notice that the Jaccard similarity of the original strings, the reduced sets, and permutations of the reduced sets are all the same.\n",
    "\n",
    "The paper next defines a function M for each document:\n",
    "\n",
    "$$\n",
    "M(A) = MIN_{s}(\\pi(S(A,k)))\n",
    "$$\n",
    "\n",
    "but since we're ignoring the shingling, we'll just use\n",
    "\n",
    "$$\n",
    "M(A) = MIN_{s}(\\pi(A))\n",
    "$$\n",
    "\n",
    "Note that the _M_ function takes a document and reduces it to a fixed-length (=s) representation, so this is a hash function.  More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'd', 'e', 'h', 'k', 'l'],\n",
       " ['a', 'c', 'd', 'e', 'g', 'h'],\n",
       " ['a', 'b', 'e', 'g', 'i', 'k'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def m(w, s, p):\n",
    "    return sorted(permute(w, p))[:s]\n",
    "    \n",
    "(m(s1, 6, p),\n",
    "m(s2, 6, p),\n",
    "m(s3, 6, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight in the paper is that\n",
    "\n",
    "$$\n",
    "\\frac{|MIN_{s}(M(A) \\cup M(B)) \\cap M(A) \\cap M(B)|}{|MIN_{s}(M(A) \\cup M(B))|}\n",
    "$$\n",
    "\n",
    "is an _unbiased estimator_ of the resemblance of A and B.  This just means that the value is a statistic for which the expected value is the true value of the resemblance.\n",
    "\n",
    "If you're like me, it's probably not immediately apparent why this is.  To clarify, we'll look at both the proof offered by Broder and some specific cases.  First, we'll consider the estimator in two different parts:\n",
    "\n",
    "$$\n",
    "MIN_{s}(M(A) \\cup M(B))\n",
    "$$\n",
    "\n",
    "which is in both the numerator and denominator, and\n",
    "\n",
    "$$\n",
    "M(A) \\cap M(B)\n",
    "$$\n",
    "\n",
    "The paper shows that the first term is equal to\n",
    "\n",
    "$$\n",
    "MIN_{s}(\\pi(A \\cup B))\n",
    "$$\n",
    "\n",
    "Now suppose that the minimum element of the latter expression is &alpha;.  Since it's the minimum element of the permutation of the union, it's also going to be the minimum element of either M(A) or M(B).  If the character that &alpha; permuted from (&pi;<sup>-1</sup>(&alpha;)) is in the _intersection_ of A and B then &alpha; will be the minimum value in both M(A) and M(B) and so will be in the second term.  So if you put that all together you get that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr(\\alpha \\in M(a) \\cap M(B)) &= Pr(\\pi^{-1}(\\alpha) \\in A \\cap B) \\\\\n",
    "&= \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, the probability that &alpha; is in M(A) intersection M(B) is equal to the set resemblance.  Note that if s=1, this means that Pr(M(A) = M(B)) is equal to the set resemblance, which will be useful later. \n",
    "\n",
    "Imagine a simple case where A is {a,b,c,d,e} and B is {a,b,c,d,f}, so that the union is {a,b,c,d,e,f} and the intersection is {a,b,c,d}.  if you choose a random element from the union, there's a 2/3 probability that the element is also in the intersection, which is also the value of the set resemblance.  Applying a permutation to the union is sort of like applying a new sort order to the original elements, and we assume that any element now has an equal probability of being the minimum element.  As a result we'd expect the size of the set in the numerator to be 2/3 the size of the set in the denominator given a large enough sample or enough permutations.\n",
    "\n",
    "As you can see below, this doesn't mean that one value for this calculation will give us the correct answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.3333333333333333)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimator(d1, d2, s, p):\n",
    "    ma = set(m(d1, s, p))\n",
    "    mb = set(m(d2, s, p))\n",
    "\n",
    "    mab = set(sorted(list(ma | mb))[:s])\n",
    "    return len(mab & ma & mb)/len(mab)\n",
    "\n",
    "estimator(s1, s2, 6, p), estimator(s1, s3, 6, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does mean that if we have larger samples (larger value for s) or more permutations, we should get closer to the right answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73 0.27500000000000013\n",
      "0.7350000000000004 0.2866666666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAEACAYAAAC52hD7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGNZJREFUeJzt3X+sZGd93/H3h7WJA8TrWKDFAhcvKA5gKGtcIC0lmQ3Q\nbikYEyoKFNskFkJRSiwU0joIl3GoUkKFk6KoVhUg3aYVcYSDa1cmsQM7FFowoV6bNdhxADvFBa8p\nv8E2sfG3f8zs5vZm751z554zP/a8X9Joz5w5Z873mTP3uZ979pl5UlVIkiRJffOIRRcgSZIkLYJB\nWJIkSb1kEJYkSVIvGYQlSZLUSwZhSZIk9ZJBWJIkSb3UKAgn2ZHkYJJrJ/eHSe6erDuYZF+3ZUqS\nJEntOqHhdhcDnwd+bHK/gMur6vJOqpIkSZI6NvWKcJInAi8B3gvkyOo1y5IkSdLKaTI04reAXwUe\nXrOugDcluSXJ+5Kc0kl1kiRJUkc2DcJJXgrcW1UH+f+vAF8B7Ab2AF8F3t1ZhZIkSVIHUlUbP5j8\nBnA+8BBwEnAycFVVXbBmmzOAa6vqmcfYf+Mnl6QlV1W9GgJmny1plc3SZ296Rbiq3lpVp1fVbuDV\nwEer6oIkp63Z7BXAoU2eY+Vvb3/72xdeg22xLatwO17aUdXfPLjo1/14fa+tUr2rVOuq1btKta5a\nvbNq+q0RMB4aceRI70ryrMn9O4E3zlyBJEmStACNg3BVjYDRZPn8juqRJEmS5sKZ5RoYDAaLLqE1\ntmU5HS9tOV7aoeW3au+1Vap3lWqF1ap3lWqF1at3Fpt+WG7bT55Ul8+v5ZVMH6/ue0PLLAnVww/L\n+XMpaRXN2mdvZYywtEWb/ULtVb6QJElLyKERkiRJ6iWDsCRJknrJICxJkqReMghLkiSplwzCkiRJ\n6iWDsCRJknrJICxJkqReMghLkiSplwzCkiRJ6iWDsCRJknrJICxJkqReMghLkiSpl05YdAGSJGl1\nJJl536pqsRJp+xoF4SQ7gM8Ad1fVy5KcClwJPAm4C3hVVX2rsyolSdLSOHBg6/vs3dt+HdJ2NR0a\ncTHweeDIn3KXADdU1ZnARyb3JUmSpJUxNQgneSLwEuC9wJH/DzkX2D9Z3g+c10l1kiRJUkeaXBH+\nLeBXgYfXrNtVVYcny4eBXW0XJkmSJHVp0zHCSV4K3FtVB5MMjrVNVVWSDUe/D4fDo8uDwYDB4JhP\nI7b3AYSt8MMK0t80Go0YjUaLLmPh7LMlrYK2+uxsFoqS/AZwPvAQcBJwMvBHwHOAQVXdk+Q04EBV\nPfUY+5ehq7kkdP1qhfkE4XGo3+w4MZBrqSWhqubz1+mSsM9WE0lm/rCc7y91ZdY+e9OhEVX11qo6\nvap2A68GPlpV5wPXABdONrsQuHqrB5YkSZIWaasTahz5U+6dwIuT3AH87OS+JEmStDIaT6hRVR8D\nPjZZ/gbwoq6KkiRJkrrmzHKSpF6Z1weT13N8rLR8DMKSpP4ZHufHk9TIVscIS5IkSccFg7AkSZJ6\nySAsSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKkXvJ7hFfE94FDDbZ7GrCz41okSZKOBwbh\nFXEH8AJO4NE8ZcNt7ucuruMHvHB+ZUmSJK0sg/AKeRSn821u3/DxnZwD3DS/giRJklaYY4QlSZLU\nSwZhSZIk9ZJBWJIkSb1kEJYkSVIvTQ3CSU5KcmOSm5PcmmQ4WT9McneSg5Pbvs6rlSRJkloy9Vsj\nquqBJHur6r4kJwCfSPJhoIDLq+ryzquUJEmSWtZoaERV3TdZfCRwIuMQDJAuipIkSZK61igIJ3lE\nkpuBw8D1VfXpyUNvSnJLkvclOaWzKiVJkqSWNZpQo6oeBvYk2Ql8KMlZwBXAr082eQfwbuCi9fsO\nh8Ojy4PBgMFgsL2KJakDo9GI0Wi06DIWzj5b0ipoq89OVU3fau0OyaXAfVX17jXrzgCurapnrtu2\ntvr8fZaEjV6tg8CA3XyHL224/07O4Spu2nSK5QDzOCdJYMPWjCvxvaFlloSq6tXwr7702UlgOOeD\nDufT985DEg4c2Pp+e/ceP6+Bls+sfXaTb4147JFhD0l+FHgxcFuSx6/Z7BXAoa0eXJIkSVqUJkMj\nTgP2J9nBODhfWVXXJflPSfYwvux3J/DGDuuUJEmSWtXk69MOAc8+xvoLOqlIkiRJmgNnlpMkSVIv\nGYQlSZLUSwZhSZIk9ZJBWJIkSb1kEJYkSVIvGYQlSZLUSwZhSZIk9ZJBWJIkSb1kEJYkSVIvGYQl\nSZLUS1OnWJYkScspycz7VlWLlTSznXo3s4i26PhgEJYkaYUdOLD1ffbubb+ORoYr8pzqDYdGSJIk\nqZcMwpIkSeolg7AkSZJ6ySAsSZKkXto0CCc5KcmNSW5OcmuS4WT9qUluSHJHkuuTnDKXaiVJkqSW\nbBqEq+oBYG9V7QH2APuSPA+4BLihqs4EPjK5L0mSJK2MqUMjquq+yeIjgROBAs4F9k/W7wfO66Q6\nSZIkqSNTg3CSRyS5GTgMXF9VnwZ2VdXhySaHgV0d1ihJkiS1buqEGlX1MLAnyU7gQ0mese7xSrLh\nlC7D4fDo8mAwYDAYzFysJHVlNBoxGo0WXcbC2WdLWgVt9dnZyrSESS4F7gPeAAyq6p4kpwEHquqp\nx9i+nPawuSRs9GodBAbs5jt8acP9d3IOV3ETL9zsGMxnKsrxNJqbHSdOiamlloSq6mY+2CXVlz47\nyfxnIxt20/cmmXlmuVnr2c4xu5pZrg/vW21u1j572rdGPPbIN0Ik+VHgxcBtwDXAhZPNLgSu3uqB\nJUmSpEWaNjTiNGB/kh2MQ/OVVXVdkk8Bf5jkIuAu4FXdlilJkiS1a9MgXFWHgGcfY/03gBd1VZQk\nSZLUNWeWkyRJUi8ZhCVJktRLBmFJkiT1kkFYkiRJvWQQliRJUi9NnVlOWlbjSTu65xe1S5J0fDII\na6V1HVF7Na2YJEk949AISZIk9ZJBWJIkSb1kEJYkSVIvGYQlSZLUSwZhSZIk9ZJBWJIkSb1kEJYk\nSVIvGYQlSZLUS1ODcJLTkxxI8rkktyb55cn6YZK7kxyc3PZ1X64kSZLUjiYzyz0IvLmqbk7yGOB/\nJbmB8aRel1fV5Z1WKEmSJHVgahCuqnuAeybL30tyG/CEycPOQCtJkqSVtKUxwknOAM4GPjVZ9aYk\ntyR5X5JTWq5NkiRJ6kzjIDwZFvFB4OKq+h5wBbAb2AN8FXh3JxVKkiRJHWgyRpgkJwJXAf+5qq4G\nqKp71zz+XuDaY+07HA6PLg8GAwaDwezVSlJHRqMRo9Fo0WUsnH22pFXQVp+dqtp8gyTAfuDrVfXm\nNetPq6qvTpbfDDynql67bt+a9vz6a0nY6NU6CAzYzXf40ob77+QcruImXrjZMYB5nJPx22az42Tb\ndWz2erVlXq+Xlk8SqqpXn4PoS5+dBIZzPuiwm74kCQcObH2/vXtnr2c7x+zkdR/aT2v2PrvJFeHn\nA68DPpvk4GTdW4HXJNnDOO3cCbxxqweXJEmSFqXJt0Z8gmOPJf5w++VIkiRJ8+HMcpIkSeolg7Ak\nSZJ6ySAsSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6\nySAsSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKkXjIIS5IkqZemBuEkpyc5kORzSW5N8suT\n9acmuSHJHUmuT3JK9+VKkiRJ7WhyRfhB4M1VdRbwU8AvJXkacAlwQ1WdCXxkcl+SJElaCVODcFXd\nU1U3T5a/B9wGPAE4F9g/2Ww/cF5XRUqSJElt29IY4SRnAGcDNwK7qurw5KHDwK5WK5MkSZI61DgI\nJ3kMcBVwcVV9d+1jVVVAtVybJEmS1JkTmmyU5ETGIfj3q+rqyerDSR5fVfckOQ2491j7DofDo8uD\nwYDBYLCtgiWpC6PRiNFotOgyFs4+W9IqaKvPzvhi7iYbJGE8BvjrVfXmNevfNVn3m0kuAU6pqkvW\n7VvTnl9/LcmGl9UPAgN28x2+tOH+OzmHq7iJF252DGAe52T8ttnsONl2HZu9Xm2Z1+ul5ZOEqsqi\n65invvTZSWA454MOu+lLknDgwNb327t39nq2c8xOXveh/bRm77ObXBF+PvA64LNJDk7W/RrwTuAP\nk1wE3AW8aqsHlyRJkhZlahCuqk+w8VjiF7VbjiRJkjQfziwnSZKkXmr0YTlpFTUdKOTIMkmS+skg\nrOPctJjbq89CSZKkNRwaIUmSpF4yCEuSJKmXOg/CJ+7Y0entovPP77oJkiRJOg51Pkb4vocf7uy5\n9wOfuP/+zp5fkiRpvfGkUfPnxCHt6zwIn9jhc/tJP0mStBDD4/x4PeEYYUmSJPWSQViSJEm9ZBCW\nJElSLxmEJUmS1EsGYUmSJPWSQViSJEm9ZBCWJElSLxmEJUmS1EtTg3CS9yc5nOTQmnXDJHcnOTi5\n7eu2TEmSJKldTa4I/x6wPugWcHlVnT25/XH7pUmSJEndmRqEq+rjwDeP8dBiJtqWJEmSWrCdMcJv\nSnJLkvclOaW1iiRJkqQ5OGHG/a4Afn2y/A7g3cBFx9pwuGZ5MLlJ0rIZjUaMRqNFl7Fww+Hw6PJg\nMGAwGCysFknaSFt99kxBuKruPbKc5L3AtRttO5zlAJI0Z+tD32WXXba4YhZobRCWpGXVVp8909CI\nJKetufsK4NBG20qSJEnLaOoV4SQfAH4GeGySLwNvBwZJ9jD+9og7gTd2WqUkSZLUsqlBuKpec4zV\n7++gFkmSJGluZv2wnKQVk8znGw+rai7HkSRpuwzCUo90HVH9cnFJ0irZzvcIS5IkSSvLICxJkqRe\nMghLkiSplxwjLEnSEcMF7y9prgzCkiStceDAbPvt3dtuHZK659AISZIk9ZJBWJIkSb1kEJYkSVIv\nGYQlSZLUSwZhSZIk9ZJBWJIkSb1kEJYkSVIvGYQlSZLUS06oIUmSlttw84eTbPhYVbVbi44rU4Nw\nkvcD/xi4t6qeOVl3KnAl8CTgLuBVVfWtDuuUJEk9NsuMf872p2maDI34PWDfunWXADdU1ZnARyb3\nJUmSpJUxNQhX1ceBb65bfS6wf7K8Hziv5bokSZKkTs36YbldVXV4snwY2NVSPZIkSdJcbPvDclVV\nSTYciT5cszyY3CRp2YxGI0aj0aLLWLjhcHh0eTAYMBgMFlaLJG2krT571iB8OMnjq+qeJKcB9260\n4XDGA0jSPK0PfZdddtniilmgtUFYkpZVW332rEMjrgEunCxfCFw94/NIkiRJCzE1CCf5APA/gZ9M\n8uUkPw+8E3hxkjuAn53clyRJklbG1KERVfWaDR56Ucu1SJIW7Morr5zr8Z7xjGdw1llnzfWYknSE\nM8tJko56w7veMLdj/dVX/oq3/dLbDMKSFsYgLEk66rvnfndux8pHN54WV5LmYdYPy0mSJEkrzSAs\nSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKkXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKk\nXjIIS5IkqZcMwpIkSeolg7AkSZJ6ySAsSZKkXjphOzsnuQv4DvBD4MGqem4bRUmSJEld21YQBgoY\nVNU32ihGkiRJmpc2hkakheeQJEmS5mq7QbiAP03ymSRvaKMgSZIkaR62OzTi+VX11SSPA25IcntV\nfXztBsM1y4PJTZKWzWg0YjQaLbqMxTuwZvkMYPeC6pCkTbTVZ28rCFfVVyf/fi3Jh4DnAhsGYUla\nVoPBgMFgcPT+ZZddtrhiFmnvoguQpOna6rNnHhqR5FFJfmyy/GjgHwCHZn0+SZIkaZ62c0V4F/Ch\nJEee579U1fWtVCVJkiR1bOYgXFV3AntarEWSJEmaG2eWkyRJUi8ZhCVJktRLBmFJkiT1kkFYkiRJ\nvbTdCTUkSWrd7bffznveczlVP9zyvskO3vKWS3jyk5/cQWWSjicGYUnS0rn77ru57rrf5+Uvf2DL\n+37wgyfxute93iAsaSqDsCRpKe3a9SO84hVbD8Kj0Y90UI2k45FjhCVJktRLBmFJkiT1kkFYkiRJ\nveQYYUmSJP0NSRZy3Kqa27EMwpIkSTq24fF9PIdGSJIkqZcMwpIkSeolg7AkSZJ6aVtBOMm+JLcn\n+Ysk/7KtoiRJkqSuzRyEk+wAfgfYBzwdeE2Sp7VV2DIZjUaLLkHHOd9j0tbcfPOiKzh+rVp/tFLv\nhTsXXYDW284V4ecCX6iqu6rqQeAPgJe3U9ZyWbVOQavH95i0NSsVflbMqvVHK/VeuGvRBWi97QTh\nJwBfXnP/7sk6SZIkaelt53uEG33b8ctOPnkbh9jclx98kD07dnT2/MvmAb7Cyfz0Jo/fMcdqJB2P\nTr6quz57vR8c/gG8eOPH77jjAS699Nj1/OVfPsAXv3jSMR+7664H2ihPUg9k1tk7kvwUMKyqfZP7\nvwY8XFW/uWab+U0NIkktq6rFTKu0IPbZklbZLH32doLwCcCfAy8EvgJ8GnhNVd020xNKkiRJczTz\n0IiqeijJPwf+BNgBvM8QLEmSpFUx8xVhSZIkaZW1MrPctIk1kgySfDvJwcntbW0ct21NJgiZtOVg\nkluTjOZcYmMNzslb1pyPQ0keSnLKImqdpkFbdia5NsnNk/Py+gWUOVWDdvx4kg8luSXJjUnOWkSd\n0yR5f5LDSQ5tss17Ju28JcnZ86xvK6a1JclTk3wyyQNJfmXe9XWlYV+3FOewwc/NUp2jBvX+s8lr\n+tkk/yPJ315EnZNaptX68kmtB5P8WZLnL6LONfU0msQryXMmv9N+bp71rathpXLRKuWf1vNNVW3r\nxnhYxBeAM4ATgZuBp63bZgBcs91jdXlr2I5TgM8BT5zcf+yi6561Leu2fynwp4uuexvn5a3Avzly\nToCvAycsuvYZ2vFvgUsnyz+5xOfkBcDZwKENHn8JcN1k+XnApxZd8zba8jjg7wD/GviVRdfbUpub\nvBeX4hw2rHVpzlHDev8usHOyvG/JX9tHr1l+JnDbMr+2a7b7KPDfgFcua60sUS5qWO9S5J+m74M1\n20/NN21cEW46scayf/q6STteC1xVVXcDVNX/nXONTW11spPXAh+YS2Vb16QtDwNHvmPpZODrVfXQ\nHGtsokk7ngYcAKiqPwfOSPK4+ZY5XVV9HPjmJpucC+yfbHsjcEqSXfOobaumtaWqvlZVnwEenF9V\nnWvyXlyWczi11iU7R03q/WRVfXty90bgiXOu8YgmtX5/zd3HMO5rF6Xp77U3AR8EvjbP4tZZtVy0\nSvmn9XzTRhBuMrFGAX9v8l8s1yV5egvHbVuTdvwEcGqSA0k+k+T8uVW3NY0nO0nyKOAfAlfNoa5Z\nNGnL7wBPT/IV4Bbg4jnVthVN2nEL8HMASZ4LPInF/ZLcjmO1dRXbcbxq8l5clnO4ahM3bbXei4Dr\nOq1oY41qTXJektsYX2H9hTnVdixT603yBMah6IrJqkV9CGrVctEq5Z/W8812JtQ4oskb7Sbg9Kq6\nL8k/Aq4Gzmzh2G1q0o4TgWcz/sq4RwGfTPKpqvqLTivbuq388L8M+ERVfaurYrapSVv2ATdV1d4k\nTwFuSPKsqvpux7VtRZN2vBP4d0kOAoeAg8APO62qO+uvdPip3OXR9FwswzlctfdN43qT7GUcLBc1\n7rZRrVV1NXB1khcwHn6yyRQonWpS728Dl1RVJQmLu+K6arlolfJP6/mmjSvC/wc4fc390xkn9KOq\n6rtVdd9k+cPAiUlObeHYbZraDsZ/hVxfVfdX1deB/w48a071bUWTthzxapZ3WAQ0a8vrgT8CqKov\nAncyHmO7TJr+nPxCVZ1dVRcwHvv4pTnW2Jb1bX3iZJ2WQ5OfqWU5h1vpy5ZBo3onH5D7XeDcqtps\nmFGXtvTaToYRPXmBv7ub1HsO8AdJ7gReCfz7JOfOqb61Vi0XrVL+aT3ftBGEPwP8RJIzkjwS+KfA\nNWs3SLJr8tfZkf/yTVV9o4Vjt2lqO4D/Cvz9JDsml9yfB3x+znU20aQtJNkJ/DTjdi2rJm3538CL\nYPxeYxyCly1ANvk52Tl5jCRvAD5WVd+bf6nbdg1wARydgfJbVXV4sSVt27KM5WtDk5+pZTmHjfqy\niWU4R01+zv8W4z/cX1dVX1hAjUc0qfUpa353Pxt45AJ/d0+tt6qeXFW7q2o343HCv1hVG71fFlrr\nkuWiVco/reebbQ+NqA0m1kjyxsnj/wH4J8AvJnkIuI9xSl8qTdpRVbcn+WPgs4w/NPC7VbV0Qbjh\nOQE4D/iTqrp/QaVO1bAt7wD+Y5LPMv5l+C+W7Q+thu14OuN2FHAr4/GDSyfJB4CfAR6b5MvA2xn/\nt9mRn5PrkrwkyReA7wM/v7hqNzetLUkeD/wZ4w9hPpzkYuDpK/oHCtC4r1uKc9ik1mU6Rw1/zv8V\n8OPAFZMc9GBVPXdJa30lcEGSB4H7GYeOhdjC77WFW7VctEr5p4t844QakiRJ6qVWJtSQJEmSVo1B\nWJIkSb1kEJYkSVIvGYQlSZLUSwZhSZIk9ZJBWJIkSb1kEJYkSVIvGYQlSZLUS/8PPyPl6VfxFe4A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10722fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_PERMUTATIONS = 100\n",
    "rps = []\n",
    "for i in range(NUM_PERMUTATIONS):\n",
    "    random.shuffle(p)\n",
    "    rps.append(p.copy())\n",
    "    \n",
    "est1 = [estimator(s1, s2, 6, rp) for rp in rps]\n",
    "est2 = [estimator(s1, s3, 6, rp) for rp in rps]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12,4))\n",
    "ax1.hist(est1, color='r')\n",
    "ax2.hist(est2, color='g')\n",
    "print(sum(est1)/NUM_PERMUTATIONS, sum(est2)/NUM_PERMUTATIONS)\n",
    "\n",
    "est1 = [estimator(s1, s2, 12, rp) for rp in rps]\n",
    "est2 = [estimator(s1, s3, 12, rp) for rp in rps]\n",
    "\n",
    "_=ax1.hist(est1, color='b')\n",
    "_=ax2.hist(est2, color='y')\n",
    "print(sum(est1)/NUM_PERMUTATIONS, sum(est2)/NUM_PERMUTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These histograms show the value for the estimator over 100 permutations, with s=6 and s=12.  The left graph is the estimate of similarity for s1 and s2 while the right graph is the for s1 and s3.  You can see that that values get both closer to the actual value and less dispersed with larger samples; however the average value of the estimator is close in both cases.\n",
    "\n",
    "Note that we can even do this with s=1.  In this case the proportion of times that the minimum character is the same after the permutation is the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77 0.32\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAEACAYAAAC52hD7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1WLRKadEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106fed198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est1 = [estimator(s1, s2, 1, rp) for rp in rps]\n",
    "est2 = [estimator(s1, s3, 1, rp) for rp in rps]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12,4))\n",
    "_=ax1.hist(est1, color='b')\n",
    "_=ax2.hist(est2, color='y')\n",
    "print(sum(est1)/NUM_PERMUTATIONS, sum(est2)/NUM_PERMUTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea here is that we can calculate M(x) for all of the documents in our set, and then we can estimate the similarity of any two documents A and B using M(A) and M(B). This function M(x) has become known as _MinHash_, although Broder never used that name.  In fact, he called M(x) a _sketch_. (Note: I have yet to find the first use of the term MinHash).  In our example, this isn't particularly useful because the value of M(x) is almost as large as the original string.  But the MinHash of the shingles in web page will be much more compact that the original document.\n",
    "\n",
    "Minhash is also not usually implemented in this way.  First, MinHash now usually means doing multiple permutations and taking the minimum item from each, rather than doing one permutation and keeping _s_ values.  Second, in real situations it can be difficult to generate independent permutations of large sets, and even if you could the sketch itself can be fairly large for sufficiently large values of _s_.  Broder's solution was to reduce the set of shingles to a binary string, and then calculate [_Rabin's fingerprint_](https://en.wikipedia.org/wiki/Rabin_fingerprint) for each bit string.\n",
    "\n",
    "A common technique is to use another hash function to give the effect of a permutation.  In our simple example, the set &Omega; is just the lowercase letters, but it could be a set of tokens or shingles as in Broder's paper.  A given hash function takes items from that set and maps them to a number.  If you have several hash functions, you can apply them all to an item and you end up with a sketch or _signature_ of that item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.71, 0.26)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123456)\n",
    "\n",
    "def rand_hash(p):\n",
    "    a = random.randint(1,p-1)\n",
    "    b = random.randint(1,p-1)\n",
    "    def h(x):\n",
    "        return ((a * x + b) % p) \n",
    "    return h\n",
    "\n",
    "def min_hash_sig(s):\n",
    "    return [min(h(ord(c) - ord('a')) for c in s) for h in hash_fam]\n",
    "\n",
    "NUM_HASH = 100\n",
    "hash_fam = [rand_hash(97) for i in range(NUM_HASH)]\n",
    "\n",
    "(sum(1 if a==b else 0 for a,b in zip(min_hash_sig(s1), min_hash_sig(s2)))/NUM_HASH,\n",
    " sum(1 if a==b else 0 for a,b in zip(min_hash_sig(s1), min_hash_sig(s3)))/NUM_HASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first use of the actual term _locality-sensitive hashing_ was apparently in a paper by Indyk and Motwani from 1998.  That paper was primarily concerned with the problem of finding approximate nearest-neighbors in high-dimensional data sets; but it relied on LSH for its algorithm.\n",
    "\n",
    "Suppose that you have a function _Sim_ that measures the similarity between any two items _d<sub>1</sub>_ and _d<sub>2</sub>_; and a family of hash functions _H_.  What we want in an LSH is that if d1 and d2 are similar according to _Sim_ then there should be a higher probability that any randomly chosen _h_ in H will give the same value for both items, and if d1 and d2 are dissimilar then there should be a lower probability that the value of of _h_ is the same.  Indyk and Motwani formalized that like this:\n",
    "\n",
    "$$\n",
    "\\mbox{if  } Sim(d_{1}, d_{2}) > s_{1} \\mbox{  then  } Pr(h(d_{1}) = h(d_{2})) > p_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{if  } Sim(d_{1}, d_{2}) < s_{2} \\mbox{  then  } Pr(h(d_{1}) = h(d_{2})) < p_{2}\n",
    "$$\n",
    "\n",
    "where s1 > s2 and p1 > p2.  If H meets these conditions, then they call it an _(s1, s2, p1, p2)-sensitive_ hash family.  They identify MinHash as an instance of an LSH (though they, too, do not use that term), but it's probably fairly clear that it does meet this definition since we've already seen that the probability that MinHash(d1) = MinHash(d2) is the same as the set resemblance (Jaccard similarity).  Of course, there are other LSH families for other measures, including Hamming distance and l<sub>p</sub> norm.\n",
    "\n",
    "Generally speaking the applications of LSH that we've seen are for searching large high-dimensional data sets for similar items, for which it's often the case that there's no better solution that just comparing every item; or more precisely the best algorithm is O(N), or linear in N,  where N is the number of items in the set.  The Indyk-Motwani paper showed that with LSH you can implement a sub-linear search, or specifically you can search with\n",
    "\n",
    "$$\n",
    "O(N^{-\\frac{ln p_{1}}{ln p_{1}/p_{2}}})\n",
    "$$\n",
    "\n",
    "evaluations of the hash function, provided that you do some preprocessing.\n",
    "\n",
    "To illustrate the idea i'm going to use a deduplication example.  The data sets are restaurants from the Fodors and Zagats guides, which i got from [RIDDLE](http://www.cs.utexas.edu/users/ml/riddle/).  In principle to do deduplication you need to compare every element in the set with every other element, so it's an O(N<sup>2</sup>) process.  For large data sets you need some way to reduce the number of comparisons.  Often that involves some sort of _pre-clustering_ that divides the whole set into clusters so that you only have to compare elements within the clusters.  LSH is one good way to do this.\n",
    "\n",
    "First we'll make a set of all of the character bigrams in the data, and then make a dictionary that maps each bigram to it's position in the sorted order.  We'll use the latter to establish the order in our permuted sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('fodors.txt') as f:\n",
    "    r1 = f.readlines()\n",
    "\n",
    "with open('zagats.txt') as f:\n",
    "    r2 = f.readlines()\n",
    "    \n",
    "NGRAM_SIZE = 2\n",
    "\n",
    "r = [(item_id, s.rstrip('\\n \\x02')) for item_id, s in enumerate(r1 + r2)]\n",
    "all_ngrams = set()\n",
    "for iid, l in r:\n",
    "    if l:\n",
    "        all_ngrams.update(ngrams(l.strip().lower(),NGRAM_SIZE))\n",
    "        \n",
    "ngram_order = {g: i for i,g in enumerate(sorted(list(all_ngrams)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is to make a key for each element of the set from a number of MinHash-like functions.  We'll use that for the key into a dictionary of lists.  We expect that if there are duplicate elements, they will be in these sublists.  However, there is a small chance that a given set of MinHash permutations won't catch the duplicates, so we'll also make multiple dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "NUM_HASH = 2\n",
    "NUM_GROUPS = 5\n",
    "\n",
    "def group_key(s, omega, lsh_fam):\n",
    "    return frozenset([min([h(x) for x in [omega[ng] for ng in ngrams(s.lower(),NGRAM_SIZE)]]) for h in lsh_fam])\n",
    "\n",
    "\n",
    "def make_group(items, omega):\n",
    "    grp = defaultdict(list)\n",
    "\n",
    "    hash_fn = [rand_hash(5099) for i in range(NUM_HASH)]\n",
    "\n",
    "    for item_id, s in items:\n",
    "        if s:\n",
    "            k = group_key(s, omega, hash_fn)\n",
    "\n",
    "        grp[k].append((item_id,s))\n",
    "    return grp, hash_fn\n",
    "\n",
    "grps = [make_group(r, ngram_order) for _ in range(NUM_GROUPS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One case of a duplicate in this set is the following restaurant, which has a bigram set similarity of 0.9375."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397,\n",
       " 'Veni Vidi Vici 41 14th St. Atlanta 404/875-8424 Italian',\n",
       " 812,\n",
       " 'Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian',\n",
       " 0.9375)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupes1 = [item for item in r if 'Veni Vidi' in item[1]]\n",
    "dd1_id, dd1_name = dupes1[0]\n",
    "dd2_id, dd2_name = dupes1[1]\n",
    "\n",
    "(dd1_id, dd1_name,\n",
    " dd2_id, dd2_name,\n",
    " jaccard_similarity(ngrams(dd1_name.lower(), NGRAM_SIZE), ngrams(dd2_name.lower(), NGRAM_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made our groups, we can look for duplicates by searching within the smaller groups rather than the whole set.  For this example, the two items are so similar that we'd expect them to be under the same key in most if not all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Size=15\n",
      "Group Size=64\n",
      "Group Size=31\n",
      "Group Size=2\n",
      "Group Size=15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian', 0.9375, 812),\n",
       " ('Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian', 0.9375, 812),\n",
       " ('Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian', 0.9375, 812),\n",
       " ('Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian', 0.9375, 812),\n",
       " ('Veni Vidi Vici 41 14th St. Atlanta 404-875-8424 Italian', 0.9375, 812)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(ts_id, ts, ngram_order, grps, top=5):\n",
    "    sims = []\n",
    "    for grp,hf in grps:\n",
    "        gk = group_key(ts, ngram_order, hf)\n",
    "        print('Group Size={0}'.format(len(grp[gk])))\n",
    "        for item_id, s in grp[gk]:\n",
    "            if ts_id == item_id: continue\n",
    "            sims += [(s, jaccard_similarity(ngrams(ts.lower(), NGRAM_SIZE), ngrams(s.lower(), NGRAM_SIZE)), item_id)]\n",
    "    return sorted(sims, key=lambda x: x[1], reverse=True)[0:top]\n",
    "                \n",
    "most_similar(397, dd1_name, ngram_order, grps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the pair below seems to be a duplicate, but the similarity is much lower, so it should appear together in fewer of the groups under the same key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335,\n",
       " 'Abruzzi 2355 Peachtree Rd.  Peachtree Battle Shopping Center Atlanta 404/261-8186 Italian',\n",
       " 750,\n",
       " 'Abruzzi 2355 Peachtree Rd. NE Atlanta 404-261-8186 Italian',\n",
       " 0.6578947368421053)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupes1 = [item for item in r if 'Abruzzi' in item[1]]\n",
    "dd1_id, dd1_name = dupes1[0]\n",
    "dd2_id, dd2_name = dupes1[1]\n",
    "\n",
    "(dd1_id, dd1_name,\n",
    " dd2_id, dd2_name,\n",
    " jaccard_similarity(ngrams(dd1_name.lower(), NGRAM_SIZE), ngrams(dd2_name.lower(), NGRAM_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Size=33\n",
      "Group Size=64\n",
      "Group Size=31\n",
      "Group Size=2\n",
      "Group Size=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Abruzzi 2355 Peachtree Rd. NE Atlanta 404-261-8186 Italian',\n",
       "  0.6578947368421053,\n",
       "  750),\n",
       " ('Abruzzi 2355 Peachtree Rd. NE Atlanta 404-261-8186 Italian',\n",
       "  0.6578947368421053,\n",
       "  750),\n",
       " ('La Grotta 2637 Peachtree Rd.  Peachtree House Condominium Atlanta 404/231-1368 Italian',\n",
       "  0.41414141414141414,\n",
       "  370),\n",
       " ('La Grotta 2637 Peachtree Rd.  Peachtree House Condominium Atlanta 404/231-1368 Italian',\n",
       "  0.41414141414141414,\n",
       "  370),\n",
       " (\"Bertolini's 3500 Peachtree Rd.  Phipps Plaza Atlanta 404/233-2333 Italian\",\n",
       "  0.4,\n",
       "  342)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(335, dd1_name, ngram_order, grps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the similarity of the duplicates is lower, it only shows up as the most similar item in 2 of 5 groups, however it _does_ show up.  This is essentially the trick of LSH-- you can guarantee with some constant probability that the actual nearest neighbor will have the same hash value as the item of interest.\n",
    "\n",
    "As you can see though LSH sometimes requires a bit of parameter tweaking to apply optimally.  This has naturally lead to techniques like the LSH-Forest, which is one way of attempting to learn the best parameters for a clustering application.\n",
    "\n",
    "### References\n",
    "\n",
    "- Broder, a. Z. (1997). On the resemblance and containment of documents. Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), 1–9. doi:10.1109/SEQUEN.1997.666900\n",
    "\n",
    "- Indyk, P., & Motwd, R. (n.d.). Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality, 604–613. doi:10.4086/toc.2012.v008a014\n",
    "\n",
    "- Bawa, M., Condie, T., & Ganesan, P. (2005). LSH forest: self-tuning indexes for similarity search. Proceedings of the 14th International Conference on World Wide Web - WWW ’05, 651. doi:10.1145/1060745.1060840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
